================================================================
RepopackPy Output File
================================================================

This file was generated by RepopackPy on: 2024-11-21T18:44:05.115668

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This header section
2. Repository structure
3. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
1. This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
2. When processing this file, use the separators and "File:" markers to
  distinguish between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and RepopackPy's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

For more information about RepopackPy, visit: https://github.com/abinthomasonline/repopack-py

================================================================
Repository Structure
================================================================
dataLoader/
  oxford/
    test_yaw.npy
    train_yaw.npy
    val_yaw.npy
    xx
  Ford_dataset.py
  KITTI_dataset.py
  Oxford_dataset.py
  Vigor_dataset.py
  utils.py
LICENSE
Oxford_dataset.py
README.md
RNNs.py
SLR.py
VGG.py
cfgnode.py
cross_attention.py
dino.py
jacobian.py
model_oxford.py
model_vigor.py
models_ford.py
models_kitti.py
sam.py
swin_transformer.py
swin_transformer_cross.py
train_ford_2DoF.py
train_ford_3DoF.py
train_kitti_2DoF.py
train_kitti_3DoF.py
train_oxford_2DoF.py
train_vigor.py
train_vigor_2DoF.py
utils.py

================================================================
Repository Files
================================================================

================
File: RNNs.py
================
import torch
import torch.nn as nn
import torch.nn.functional as F


class ConvGRU(nn.Module):
    def __init__(self, hidden_dim=128, input_dim=128*2, kernel_size=(3, 3), padding=(1, 1)):
        super(ConvGRU, self).__init__()
        self.convz = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=padding)
        self.convr = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=padding)
        self.convq = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=padding)

    def forward(self, h, x):
        hx = torch.cat([h, x], dim=1)

        z = torch.sigmoid(self.convz(hx))
        r = torch.sigmoid(self.convr(hx))
        q = torch.tanh(self.convq(torch.cat([r*h, x], dim=1)))

        h = (1-z) * h + z * q
        return h


class CoordEncoder(nn.Module):
    def __init__(self, cfg, hidden_dim=128, num_layers=4):
        super(CoordEncoder, self).__init__()
        self.convs = nn.ModuleList([
            nn.Conv2d(2, hidden_dim, kernel_size=(1, 1), padding=0),
        ])
        # (2 * cfg.models.num_encoding_fn_xyz + 1)*2
        for idx in range(num_layers):
            self.convs.extend([
                nn.ReLU(),
                nn.Conv2d(hidden_dim, hidden_dim, kernel_size=(1, 1), padding=0),
            ])

    def forward(self, x):
        for layer in self.convs:
            x = layer(x)

        return x


class PoseFeature(nn.Module):
    def __init__(self, cfg, input_dim, hidden_dim=128, num_layers=4):
        super(PoseFeature, self).__init__()
        self.CoordEncoder = CoordEncoder(cfg, hidden_dim, num_layers)
        # self.CoordEncoder = self.construct_layers(hidden_dim, num_layers)

        self.conv1 = nn.Sequential(
            nn.Conv2d(input_dim, hidden_dim, kernel_size=(3, 3), padding=(1, 1)),
            nn.ReLU(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(hidden_dim*2, hidden_dim, kernel_size=(3, 3), padding=(1, 1)),
            nn.ReLU(),
        )

    def forward(self, feat, coord):
        x = self.conv1(feat)
        y = self.CoordEncoder(coord)
        z = torch.cat([x, y], dim=1)
        z = self.conv2(z)
        return z


class GRUPoseRefine(nn.Module):
    def __init__(self, cfg, input_dim, hidden_dim=128, num_layers=4):
        super(GRUPoseRefine, self).__init__()

        self.PoseFeature = PoseFeature(cfg, input_dim, hidden_dim, num_layers)
        self.PoseHeader = nn.Sequential(
            nn.Linear(hidden_dim, int(hidden_dim//2)),
            nn.ReLU(),
            nn.Linear(int(hidden_dim//2), 4),
            nn.Tanh()
        )
        self.convGRU = ConvGRU(hidden_dim=hidden_dim, input_dim=input_dim+hidden_dim)

    def forward(self, query_feat, pred_feat, pred_grids, h):
        pred_grids = F.interpolate(pred_grids.permute(0, 3, 1, 2), size=pred_feat.shape[2:], mode='bilinear')

        poseFeat = self.PoseFeature(pred_feat, pred_grids)
        x = torch.cat([query_feat, poseFeat], dim=1)
        h = self.convGRU(h, x)

        h_ = torch.mean(h, dim=[-1, -2])
        delta_pose = self.PoseHeader(h_)

        return h, delta_pose


class NNrefine(nn.Module):
    def __init__(self):
        super(NNrefine, self).__init__()
        self.linear0 = nn.Sequential(nn.ReLU(inplace=True),
                                     nn.Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))
        self.linear1 = nn.Sequential(nn.ReLU(inplace=True),
                                     nn.Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))
        self.linear2 = nn.Sequential(nn.ReLU(inplace=True),
                                     nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))
        self.linear3 = nn.Sequential(nn.ReLU(inplace=True),
                                     nn.Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))

        self.mapping = nn.Sequential(nn.ReLU(inplace=True),
                                     nn.Linear(64, 16),
                                     nn.ReLU(inplace=True),
                                     nn.Linear(16, 3),
                                     nn.Tanh())

    def forward(self, pred_feat, ref_feat):
        r = pred_feat - ref_feat  # [B, C, H, W]
        B, C, _, _ = r.shape
        if C == 256:
            x = self.linear0(r)
        elif C == 128:
            x = self.linear1(r)
        elif C == 64:
            x = self.linear2(r)
        elif C == 16:
            x = self.linear3(r)

        x = torch.mean(x, dim=[2, 3])

        y = self.mapping(x)  # [B, 3]
        return y




class Uncertainty(nn.Module):
    def __init__(self):
        super(Uncertainty, self).__init__()
        self.conv1 = nn.Sequential(nn.ReLU(inplace=True),
                                     nn.Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
                                     nn.ReLU(inplace=True),
                                     nn.Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
                                     nn.ReLU(inplace=True),
                                     nn.Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
                                     nn.Sigmoid())
        self.conv2 = nn.Sequential(nn.ReLU(inplace=True),
                                     nn.Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
                                     nn.ReLU(inplace=True),
                                     nn.Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
                                     nn.ReLU(inplace=True),
                                     nn.Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
                                   nn.Sigmoid())
        self.conv3 = nn.Sequential(nn.ReLU(inplace=True),
                                     nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
                                     nn.ReLU(inplace=True),
                                     nn.Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
                                     nn.ReLU(inplace=True),
                                     nn.Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
                                   nn.Sigmoid())

    def forward(self, x):

        return [self.conv1(x[0]), self.conv2(x[1]), self.conv3(x[2])]

================
File: cfgnode.py
================
"""
Define a class to hold configurations.

Borrows and merges stuff from YACS, fvcore, and detectron2
https://github.com/rbgirshick/yacs
https://github.com/facebookresearch/fvcore/
https://github.com/facebookresearch/detectron2/

"""

import copy
import importlib.util
import io
import logging
import os
from ast import literal_eval
from typing import Optional

import yaml

# File exts for yaml
_YAML_EXTS = {"", ".yml", ".yaml"}
# File exts for python
_PY_EXTS = {".py"}

# CfgNodes can only contain a limited set of valid types
_VALID_TYPES = {tuple, list, str, int, float, bool}

# Valid file object types
_FILE_TYPES = (io.IOBase,)

# Logger
logger = logging.getLogger(__name__)


class CfgNode(dict):
    r"""CfgNode is a `node` in the configuration `tree`. It's a simple wrapper around a `dict` and supports access to
    `attributes` via `keys`.
    """

    IMMUTABLE = "__immutable__"
    DEPRECATED_KEYS = "__deprecated_keys__"
    RENAMED_KEYS = "__renamed_keys__"
    NEW_ALLOWED = "__new_allowed__"

    def __init__(
        self,
        init_dict: Optional[dict] = None,
        key_list: Optional[list] = None,
        new_allowed: Optional[bool] = False,
    ):
        r"""
        Args:
            init_dict (dict): A dictionary to initialize the `CfgNode`.
            key_list (list[str]): A list of names that index this `CfgNode` from the root. Currently, only used for
                logging.
            new_allowed (bool): Whether adding a new key is allowed when merging with other `CfgNode` objects.

        """

        # Recursively convert nested dictionaries in `init_dict` to config tree.
        init_dict = {} if init_dict is None else init_dict
        key_list = [] if key_list is None else key_list
        init_dict = self._create_config_tree_from_dict(init_dict, key_list)
        super(CfgNode, self).__init__(init_dict)

        # Control the immutability of the `CfgNode`.
        self.__dict__[CfgNode.IMMUTABLE] = False
        # Support for deprecated options.
        # If you choose to remove support for an option in code, but don't want to change all of the config files
        # (to allow for deprecated config files to run), you can add the full config key as a string to this set.
        self.__dict__[CfgNode.DEPRECATED_KEYS] = set()
        # Support for renamed options.
        # If you rename an option, record the mapping from the old name to the new name in this dictionary. Optionally,
        # if the type also changed, you can make this value a tuple that specifies two things: the renamed key, and the
        # instructions to edit the config file.
        self.__dict__[CfgNode.RENAMED_KEYS] = {
            # 'EXAMPLE.OLD.KEY': 'EXAMPLE.NEW.KEY',  # Dummy example
            # 'EXAMPLE.OLD.KEY': (                   # A more complex example
            #     'EXAMPLE.NEW.KEY',
            #     "Also convert to a tuple, eg. 'foo' -> ('foo', ) or "
            #     + "'foo.bar' -> ('foo', 'bar')"
            # ),
        }

        # Allow new attributes after initialization.
        self.__dict__[CfgNode.NEW_ALLOWED] = new_allowed

    @classmethod
    def _create_config_tree_from_dict(cls, init_dict: dict, key_list: list):
        r"""Create a configuration tree using the input dict. Any dict-like objects inside `init_dict` will be treated
        as new `CfgNode` objects.

        Args:
            init_dict (dict): Input dictionary, to create config tree from.
            key_list (list): A list of names that index this `CfgNode` from the root. Currently only used for logging.

        """

        d = copy.deepcopy(init_dict)
        for k, v in d.items():
            if isinstance(v, dict):
                # Convert dictionary to CfgNode
                d[k] = cls(v, key_list=key_list + [k])
            else:
                # Check for valid leaf type or nested CfgNode
                _assert_with_logging(
                    _valid_type(v, allow_cfg_node=False),
                    "Key {} with value {} is not a valid type; valid types: {}".format(
                        ".".join(key_list + [k]), type(v), _VALID_TYPES
                    ),
                )
        return d

    def __getattr__(self, name: str):
        if name in self:
            return self[name]
        else:
            raise AttributeError(name)

    def __setattr__(self, name: str, value):
        if self.is_frozen():
            raise AttributeError(
                "Attempted to set {} to {}, but CfgNode is immutable".format(
                    name, value
                )
            )

        _assert_with_logging(
            name not in self.__dict__,
            "Invalid attempt to modify internal CfgNode state: {}".format(name),
        )

        _assert_with_logging(
            _valid_type(value, allow_cfg_node=True),
            "Invalid type {} for key {}; valid types = {}".format(
                type(value), name, _VALID_TYPES
            ),
        )

        self[name] = value

    def __str__(self):
        def _indent(s_, num_spaces):
            s = s_.split("\n")
            if len(s) == 1:
                return s_
            first = s.pop(0)
            s = [(num_spaces * " ") + line for line in s]
            s = "\n".join(s)
            s = first + "\n" + s
            return s

        r = ""
        s = []
        for k, v in sorted(self.items()):
            separator = "\n" if isinstance(v, CfgNode) else " "
            attr_str = "{}:{}{}".format(str(k), separator, str(v))
            attr_str = _indent(attr_str, 2)
            s.append(attr_str)
        r += "\n".join(s)
        return r

    def __repr__(self):
        return "{}({})".format(self.__class__.__name__, super(CfgNode, self).__repr__())

    def dump(self, **kwargs):
        r"""Dump CfgNode to a string.
        """

        def _convert_to_dict(cfg_node, key_list):
            if not isinstance(cfg_node, CfgNode):
                _assert_with_logging(
                    _valid_type(cfg_node),
                    "Key {} with value {} is not a valid type; valid types: {}".format(
                        ".".join(key_list), type(cfg_node), _VALID_TYPES
                    ),
                )
                return cfg_node
            else:
                cfg_dict = dict(cfg_node)
                for k, v in cfg_dict.items():
                    cfg_dict[k] = _convert_to_dict(v, key_list + [k])
                return cfg_dict

        self_as_dict = _convert_to_dict(self, [])
        return yaml.safe_dump(self_as_dict, **kwargs)

    def merge_from_file(self, cfg_filename: str):
        r"""Load a yaml config file and merge it with this CfgNode.

        Args:
            cfg_filename (str): Config file path.

        """
        with open(cfg_filename, "r") as f:
            cfg = self.load_cfg(f)
        self.merge_from_other_cfg(cfg)

    def merge_from_other_cfg(self, cfg_other):
        r"""Merge `cfg_other` into the current `CfgNode`.

        Args:
            cfg_other
        """
        _merge_a_into_b(cfg_other, self, self, [])

    def merge_from_list(self, cfg_list: list):
        r"""Merge config (keys, values) in a list (eg. from commandline) into this `CfgNode`.

        Eg. `cfg_list = ['FOO.BAR', 0.5]`.
        """
        _assert_with_logging(
            len(cfg_list) % 2 == 0,
            "Override list has odd lengths: {}; it must be a list of pairs".format(
                cfg_list
            ),
        )
        root = self
        for full_key, v in zip(cfg_list[0::2], cfg_list[1::2]):
            if root.key_is_deprecated(full_key):
                continue
            if root.key_is_renamed(full_key):
                root.raise_key_rename_error(full_key)
            key_list = full_key.split(".")
            d = self
            for subkey in key_list[:-1]:
                _assert_with_logging(
                    subkey in d, "Non-existent key: {}".format(full_key)
                )
                d = d[subkey]
            subkey = key_list[-1]
            _assert_with_logging(subkey in d, "Non-existent key: {}".format(full_key))
            value = self._decode_cfg_value(v)
            value = _check_and_coerce_cfg_value_type(value, d[subkey], subkey, full_key)
            d[subkey] = value

    def freeze(self):
        r"""Make this `CfgNode` and all of its children immutable. """
        self._immutable(True)

    def defrost(self):
        r"""Make this `CfgNode` and all of its children mutable. """
        self._immutable(False)

    def is_frozen(self):
        r"""Return mutability. """
        return self.__dict__[CfgNode.IMMUTABLE]

    def _immutable(self, is_immutable: bool):
        r"""Set mutability and recursively apply to all nested `CfgNode` objects.

        Args:
            is_immutable (bool): Whether or not the `CfgNode` and its children are immutable.

        """
        self.__dict__[CfgNode.IMMUTABLE] = is_immutable
        # Recursively propagate state to all children.
        for v in self.__dict__.values():
            if isinstance(v, CfgNode):
                v._immutable(is_immutable)
        for v in self.values():
            if isinstance(v, CfgNode):
                v._immutable(is_immutable)

    def clone(self):
        r"""Recursively copy this `CfgNode`. """
        return copy.deepcopy(self)

    def register_deprecated_key(self, key: str):
        r"""Register key (eg. `FOO.BAR`) a deprecated option. When merging deprecated keys, a warning is generated and
        the key is ignored.
        """

        _assert_with_logging(
            key not in self.__dict__[CfgNode.DEPRECATED_KEYS],
            "key {} is already registered as a deprecated key".format(key),
        )
        self.__dict__[CfgNode.DEPRECATED_KEYS].add(key)

    def register_renamed_key(
        self, old_name: str, new_name: str, message: Optional[str] = None
    ):
        r"""Register a key as having been renamed from `old_name` to `new_name`. When merging a renamed key, an
        exception is thrown alerting the user to the fact that the key has been renamed.
        """

        _assert_with_logging(
            old_name not in self.__dict__[CfgNode.RENAMED_KEYS],
            "key {} is already registered as a renamed cfg key".format(old_name),
        )
        value = new_name
        if message:
            value = (new_name, message)
        self.__dict__[CfgNode.RENAMED_KEYS][old_name] = value

    def key_is_deprecated(self, full_key: str):
        r"""Test if a key is deprecated. """
        if full_key in self.__dict__[CfgNode.DEPRECATED_KEYS]:
            logger.warning("deprecated config key (ignoring): {}".format(full_key))
            return True
        return False

    def key_is_renamed(self, full_key: str):
        r"""Test if a key is renamed. """
        return full_key in self.__dict__[CfgNode.RENAMED_KEYS]

    def raise_key_rename_error(self, full_key: str):
        new_key = self.__dict__[CfgNode.RENAMED_KEYS][full_key]
        if isinstance(new_key, tuple):
            msg = " Note: " + new_key[1]
            new_key = new_key[0]
        else:
            msg = ""
        raise KeyError(
            "Key {} was renamed to {}; please update your config.{}".format(
                full_key, new_key, msg
            )
        )

    def is_new_allowed(self):
        return self.__dict__[CfgNode.NEW_ALLOWED]

    @classmethod
    def load_cfg(cls, cfg_file_obj_or_str):
        r"""Load a configuration into the `CfgNode`.

        Args:
            cfg_file_obj_or_str (str or cfg compatible object): Supports loading from:
                - A file object backed by a YAML file.
                - A file object backed by a Python source file that exports an sttribute "cfg" (dict or `CfgNode`).
                - A string that can be parsed as valid YAML.

        """
        _assert_with_logging(
            isinstance(cfg_file_obj_or_str, _FILE_TYPES + (str,)),
            "Expected first argument to be of type {} or {}, but got {}".format(
                _FILE_TYPES, str, type(cfg_file_obj_or_str)
            ),
        )
        if isinstance(cfg_file_obj_or_str, str):
            return cls._load_cfg_from_yaml_str(cfg_file_obj_or_str)
        elif isinstance(cfg_file_obj_or_str, _FILE_TYPES):
            return cls._load_cfg_from_file(cfg_file_obj_or_str)
        else:
            raise NotImplementedError("Impossible to reach here (unless there's a bug)")

    @classmethod
    def _load_cfg_from_file(cls, file_obj):
        r"""Load a config from a YAML file or a Python source file. """
        _, file_ext = os.path.splitext(file_obj.name)
        if file_ext in _YAML_EXTS:
            return cls._load_cfg_from_yaml_str(file_obj.read())
        elif file_ext in _PY_EXTS:
            return cls._load_cfg_py_source(file_obj.name)
        else:
            raise Exception(
                "Attempt to load from an unsupported filetype {}; only {} supported".format(
                    _YAML_EXTS.union(_PY_EXTS)
                )
            )

    @classmethod
    def _load_cfg_from_yaml_str(cls, str_obj):
        r"""Load a config from a YAML string encoding. """
        cfg_as_dict = yaml.safe_load(str_obj)
        return cls(cfg_as_dict)

    @classmethod
    def _load_cfg_py_source(cls, filename):
        r"""Load a config from a Python source file. """
        module = _load_module_from_file("yacs.config.override", filename)
        _assert_with_logging(
            hasattr(module, "cfg"),
            "Python module from file {} must export a 'cfg' attribute".format(filename),
        )
        VALID_ATTR_TYPES = {dict, CfgNode}
        _assert_with_logging(
            type(module.cfg) in VALID_ATTR_TYPES,
            "Import module 'cfg' attribute must be in {} but is {}".format(
                VALID_ATTR_TYPES, type(module.cfg)
            ),
        )
        return cls(module.cfg)

    @classmethod
    def _decode_cfg_value(cls, value):
        r"""Decodes a raw config value (eg. from a yaml config file or commandline argument) into a Python object.

        If `value` is a dict, it will be interpreted as a new `CfgNode`.
        If `value` is a str, it will be evaluated as a literal.
        Otherwise, it is returned as is.

        """
        # Configs parsed from raw yaml will contain dictionary keys that need to be converted to `CfgNode` objects.
        if isinstance(value, dict):
            return cls(value)
        # All remaining processing is only applied to strings.
        if not isinstance(value, str):
            return value
        # Try to interpret `value` as a: string, number, tuple, list, dict, bool, or None
        try:
            value = literal_eval(value)
        # The following two excepts allow `value` to pass through it when it represents a string.
        # The type of `value` is always a string (before calling `literal_eval`), but sometimes it *represents* a
        # string and other times a data structure, like a list. In the case that `value` represents a str, what we
        # got back from the yaml parser is `foo` *without quotes* (so, not `"foo"`). `literal_eval` is ok with `"foo"`,
        # but will raise a `ValueError` if given `foo`. In other cases, like paths (`val = 'foo/bar'`) `literal_eval`
        # will raise a `SyntaxError`.
        except ValueError:
            pass
        except SyntaxError:
            pass
        return value


# Keep this function in global scope, for backward compataibility.
load_cfg = CfgNode.load_cfg


def _valid_type(value, allow_cfg_node: Optional[bool] = False):
    return (type(value) in _VALID_TYPES) or (
        allow_cfg_node and isinstance(value, CfgNode)
    )


def _merge_a_into_b(a: CfgNode, b: CfgNode, root: CfgNode, key_list: list):
    r"""Merge `CfgNode` `a` into `CfgNode` `b`, clobbering the options in `b` wherever they are also specified in `a`.
    """
    _assert_with_logging(
        isinstance(a, CfgNode),
        "`a` (cur type {}) must be an instance of {}".format(type(a), CfgNode),
    )
    _assert_with_logging(
        isinstance(b, CfgNode),
        "`b` (cur type {}) must be an instance of {}".format(type(b), CfgNode),
    )

    for k, v_ in a.items():
        full_key = ".".join(key_list + [k])
        v = copy.deepcopy(v_)
        v = b._decode_cfg_value(v)

        if k in b:
            v = _check_and_coerce_cfg_value_type(v, b[k], k, full_key)
            # Recursively merge dicts.
            if isinstance(v, CfgNode):
                try:
                    _merge_a_into_b(v, b[k], root, key_list + [k])
                except BaseException:
                    raise
            else:
                b[k] = v
        elif b.is_new_allowed():
            b[k] = v
        else:
            if root.key_is_deprecated(full_key):
                continue
            elif root.key_is_renamed(full_key):
                root.raise_key_rename_error(full_key)
            else:
                raise KeyError("Non-existent config key: {}".format(full_key))


def _check_and_coerce_cfg_value_type(replacement, original, key, full_key):
    r"""Checks that `replacement`, which is intended to replace `original` is of the right type. The type is correct if
    it matches exactly or is one of a few cases in which the type can easily be coerced.
    """

    original_type = type(original)
    replacement_type = type(replacement)
    if replacement_type == original_type:
        return replacement

    # If replacement and original types match, cast replacement from `from_type` to `to_type`.
    def _conditional_cast(from_type, to_type):
        if replacement_type == from_type and original_type == to_type:
            return True, to_type(replacement)
        else:
            return False, None

    # Conditional casts.
    # list <-> tuple
    casts = [(tuple, list), (list, tuple)]
    for (from_type, to_type) in casts:
        converted, converted_value = _conditional_cast(from_type, to_type)
        if converted:
            return converted_value

    raise ValueError(
        "Type mismatch ({} vs. {} with values ({} vs. {}) for config key: {}".format(
            original_type, replacement_type, original, replacement, full_key
        )
    )


def _assert_with_logging(cond, msg):
    if not cond:
        logger.debug(msg)
    assert cond, msg


def _load_module_from_file(name, filename):
    spec = importlib.util.spec_from_file_location(name, filename)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module

================
File: jacobian.py
================
import torch.nn as nn
import torch
import torch.nn.functional as F
import torchvision


class Conv2d_cir(nn.Module):

    def __init__(self, in_dim, out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(0, 0), n=1):
        super(Conv2d_cir, self).__init__()
        self.n = n
        self.conv = nn.Conv2d(in_dim, out_dim, kernel_size=kernel_size, stride=stride, padding=padding)
    def forward(self, x):
        out = torch.cat([x[:, :, :, -self.n:], x, x[:, :, :, :self.n]], dim=-1)
        out = F.pad(out, (0, 0, 1, 1), mode='constant') # pad last dim by (0, 0) and 2nd to last by (1, 1)
        out = self.conv(out)
        return out


def conv2d_jac(conv2d, x, jac):
    '''
    x: (N, C_in, H_in, W_in)
    jac: (q_dim, N, C_in, H_in, W_in)
    returns:
    y: (N, C_out, H_out, W_out)
    jac: (q_dim, N, C_out, H_out, W_out)
    '''
    q_dim, N, C_in, H_in, W_in = jac.shape
    jac = jac.reshape(-1, C_in, H_in, W_in)
    if conv2d.padding_mode != 'zeros':
        jac= F.conv2d(F.pad(jac, tuple(x for x in reversed(x.padding) for _ in range(2)), mode=conv2d.padding_mode),
                        conv2d.weight, None, conv2d.stride, (0,0), conv2d.dilation, conv2d.groups)
    else:
        jac = F.conv2d(jac, conv2d.weight, None, conv2d.stride, conv2d.padding, conv2d.dilation, conv2d.groups)
    _, C_out, H_out, W_out = jac.shape
    return conv2d(x), jac.reshape(q_dim, N, -1, H_out, W_out)


def maxpool_jac(maxpool, x, jac):
    # specifically designed for maxpool_2d, with kernel_size = 2, stride = 2
    y, indices = maxpool(x)

    q_dim, N, C_in, H_in, W_in = jac.shape
    indices = indices.unsqueeze(dim=0).repeat(q_dim, 1, 1, 1, 1).reshape(q_dim*N, C_in, H_in//2, W_in//2)

    jac = jac.reshape(-1, C_in, H_in, W_in)
    flattened_tensor = jac.flatten(start_dim=2)
    output = flattened_tensor.gather(dim=2, index=indices.flatten(start_dim=2)).view_as(indices)

    return y, output.reshape(q_dim, N, C_in, H_in//2, W_in//2)


def activation_jac(activation, x, jac):
    '''
    actication function must be element-wsie
    x: (...)
    jac: (q_dim, ...)
    returns:
    y: (...)
    jac: (q_dim, ...)
    '''
    y = activation(x)
    # jac = jac * torch.autograd.grad(y.sum(), x, create_graph=True)[0]
    # if relu:
    jac = jac * (x>0).float()
    return y, jac


def interpolate_jac(image, new_shape, jac):
    # jac.shape = [#pose, N, C, IH, IW]
    B, C, IH, IW = image.shape
    H, W = new_shape

    u0 = torch.arange(W, dtype=torch.float32, device=image.device) / (W - 1) * (IW - 1)
    v0 = torch.arange(H, dtype=torch.float32, device=image.device) / (H - 1) * (IH - 1)

    iy, ix = torch.meshgrid(v0, u0)

    with torch.no_grad():
        ix_nw = torch.floor(ix)  # north-west  upper-left-x
        iy_nw = torch.floor(iy)  # north-west  upper-left-y
        ix_ne = ix_nw + 1        # north-east  upper-right-x
        iy_ne = iy_nw            # north-east  upper-right-y
        ix_sw = ix_nw            # south-west  lower-left-x
        iy_sw = iy_nw + 1        # south-west  lower-left-y
        ix_se = ix_nw + 1        # south-east  lower-right-x
        iy_se = iy_nw + 1        # south-east  lower-right-y

        torch.clamp(ix_nw, 0, IW -1, out=ix_nw)
        torch.clamp(iy_nw, 0, IH -1, out=iy_nw)

        torch.clamp(ix_ne, 0, IW -1, out=ix_ne)
        torch.clamp(iy_ne, 0, IH -1, out=iy_ne)

        torch.clamp(ix_sw, 0, IW -1, out=ix_sw)
        torch.clamp(iy_sw, 0, IH -1, out=iy_sw)

        torch.clamp(ix_se, 0, IW -1, out=ix_se)
        torch.clamp(iy_se, 0, IH -1, out=iy_se)

    nw = (ix_se - ix) * (iy_se - iy)  #[H, W]
    ne = (ix - ix_sw) * (iy_sw - iy)
    sw = (ix_ne - ix) * (iy - iy_ne)
    se = (ix - ix_nw) * (iy - iy_nw)

    nw = nw.unsqueeze(dim=0).unsqueeze(dim=0) # [1, 1, H, W]
    ne = ne.unsqueeze(dim=0).unsqueeze(dim=0)
    sw = sw.unsqueeze(dim=0).unsqueeze(dim=0)
    se = se.unsqueeze(dim=0).unsqueeze(dim=0)

    image = image.view(B, C, IH * IW)

    nw_val = torch.gather(image, 2, (iy_nw * IW + ix_nw).long().view(1, 1, H * W).repeat(B, C, 1)).view(B, C, H, W)
    ne_val = torch.gather(image, 2, (iy_ne * IW + ix_ne).long().view(1, 1, H * W).repeat(B, C, 1)).view(B, C, H, W)
    sw_val = torch.gather(image, 2, (iy_sw * IW + ix_sw).long().view(1, 1, H * W).repeat(B, C, 1)).view(B, C, H, W)
    se_val = torch.gather(image, 2, (iy_se * IW + ix_se).long().view(1, 1, H * W).repeat(B, C, 1)).view(B, C, H, W)

    out_val = (nw_val * nw + ne_val * ne + sw_val * sw + se_val * se)

    if jac is not None:
        M = jac.shape[0]
        jac1 = jac.permute(1, 0, 2, 3, 4).reshape(B, M*C, IH, IW)

        nw_jac = torch.gather(jac1, 2, (iy_nw * IW + ix_nw).long().view(1, 1, H * W).repeat(B, M*C, 1)).view(B, M*C, H, W)
        ne_jac = torch.gather(jac1, 2, (iy_ne * IW + ix_ne).long().view(1, 1, H * W).repeat(B, M*C, 1)).view(B, M*C, H, W)
        sw_jac = torch.gather(jac1, 2, (iy_sw * IW + ix_sw).long().view(1, 1, H * W).repeat(B, M*C, 1)).view(B, M*C, H, W)
        se_jac = torch.gather(jac1, 2, (iy_se * IW + ix_se).long().view(1, 1, H * W).repeat(B, M*C, 1)).view(B, M*C, H, W)

        jac_new = (nw_jac * nw + ne_jac * ne + sw_jac * sw + se_jac * se)

        jac_new = jac_new.reshape(B, M, C, IH, IW).permute(1, 0, 2, 3, 4)  # [M, B, C, IH, IW]

        return out_val, jac_new
    else:
        return out_val


def grid_sample(image, optical, jac=None):
    # values in optical within range of [0, H], and [0, W]
    N, C, IH, IW = image.shape
    _, H, W, _ = optical.shape

    ix = optical[..., 0].view(N, 1, H, W)
    iy = optical[..., 1].view(N, 1, H, W)

    with torch.no_grad():
        ix_nw = torch.floor(ix)  # north-west  upper-left-x
        iy_nw = torch.floor(iy)  # north-west  upper-left-y
        ix_ne = ix_nw + 1        # north-east  upper-right-x
        iy_ne = iy_nw            # north-east  upper-right-y
        ix_sw = ix_nw            # south-west  lower-left-x
        iy_sw = iy_nw + 1        # south-west  lower-left-y
        ix_se = ix_nw + 1        # south-east  lower-right-x
        iy_se = iy_nw + 1        # south-east  lower-right-y

        torch.clamp(ix_nw, 0, IW -1, out=ix_nw)
        torch.clamp(iy_nw, 0, IH -1, out=iy_nw)

        torch.clamp(ix_ne, 0, IW -1, out=ix_ne)
        torch.clamp(iy_ne, 0, IH -1, out=iy_ne)

        torch.clamp(ix_sw, 0, IW -1, out=ix_sw)
        torch.clamp(iy_sw, 0, IH -1, out=iy_sw)

        torch.clamp(ix_se, 0, IW -1, out=ix_se)
        torch.clamp(iy_se, 0, IH -1, out=iy_se)

    mask_x = (ix >= 0) & (ix <= IW - 1)
    mask_y = (iy >= 0) & (iy <= IH - 1)
    mask = mask_x * mask_y

    assert torch.sum(mask) > 0

    nw = (ix_se - ix) * (iy_se - iy) * mask
    ne = (ix - ix_sw) * (iy_sw - iy) * mask
    sw = (ix_ne - ix) * (iy - iy_ne) * mask
    se = (ix - ix_nw) * (iy - iy_nw) * mask

    image = image.view(N, C, IH * IW)

    nw_val = torch.gather(image, 2, (iy_nw * IW + ix_nw).long().view(N, 1, H * W).repeat(1, C, 1)).view(N, C, H, W)
    ne_val = torch.gather(image, 2, (iy_ne * IW + ix_ne).long().view(N, 1, H * W).repeat(1, C, 1)).view(N, C, H, W)
    sw_val = torch.gather(image, 2, (iy_sw * IW + ix_sw).long().view(N, 1, H * W).repeat(1, C, 1)).view(N, C, H, W)
    se_val = torch.gather(image, 2, (iy_se * IW + ix_se).long().view(N, 1, H * W).repeat(1, C, 1)).view(N, C, H, W)

    out_val = (nw_val * nw + ne_val * ne + sw_val * sw + se_val * se)

    if jac is not None:

        dout_dpx = (nw_val * (-(iy_se - iy) * mask) + ne_val * (iy_sw - iy) * mask +
                    sw_val * (-(iy - iy_ne) * mask) + se_val * (iy - iy_nw) * mask)
        dout_dpy = (nw_val * (-(ix_se - ix) * mask) + ne_val * (-(ix - ix_sw) * mask) +
                    sw_val * (ix_ne - ix) * mask + se_val * (ix - ix_nw) * mask)
        dout_dpxy = torch.stack([dout_dpx, dout_dpy], dim=-1)  # [N, C, H, W, 2]

        # assert jac.shape[1:] == [N, H, W, 2]
        jac_new = dout_dpxy[None, :, :, :, :, :] * jac[:, :, None, :, :, :]
        jac_new1 = torch.sum(jac_new, dim=-1)

        if torch.any(torch.isnan(jac)) or torch.any(torch.isnan(dout_dpxy)):
            print('Nan occurs')

        return out_val, jac_new1 #jac_new1 #jac_new.permute(4, 0, 1, 2, 3)
    else:
        return out_val, None


    # out_val = (nw_val.view(N, C, H, W) * nw.view(N, 1, H, W) +
    #            ne_val.view(N, C, H, W) * ne.view(N, 1, H, W) +
    #            sw_val.view(N, C, H, W) * sw.view(N, 1, H, W) +
    #            se_val.view(N, C, H, W) * se.view(N, 1, H, W))
    #
    # return out_val


# import numpy as np
# import PIL.Image as Image
# x = torch.from_numpy(np.random.rand(1, 3, 32, 32).astype(np.float32)).cuda()
# x.requires_grad = True
#
# grids = torch.from_numpy(np.random.uniform(-1, 1, size=[1, 32, 32, 2]).astype(np.float32)).cuda()
#
# img0 = F.grid_sample(x, grids, align_corners=True)
# img1 = grid_sample(x, (grids + 1)/2 * 31)
# print(torch.sum(torch.abs(img0 - img1)))



# y, dy_dgrids = grid_sample(x, grids)
#
# jac = torch.autograd.functional.jacobian(grid_sample, (x, grids))
#
# torch.sum(jac[0][1])
# torch.sum(dy_dgrids)
# temp = jac[0][1][0, :, :,:, 0, :, :, : ].reshape([3, 32*32, 32*32, 2])
# temp_diag = torch.diagonal(temp, dim1=1, dim2=2)
# torch.sum(temp_diag) - torch.sum(temp)
#
# a = 1

================
File: SLR.py
================
import re

import torch


class ScaledLowRankConvAdapter(torch.nn.Module):
    """SLR adapter for conv layers."""

    def __init__(self, conv2d: torch.nn.Conv2d, hidden_dim: int = 16):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.proj = conv2d
        self.kernel_size = conv2d.kernel_size
        self.scaler = torch.nn.Parameter(torch.ones(self.proj.out_channels))

        assert conv2d.kernel_size == (16, 16)
        kernel_size = (4, 4)
        self.down = torch.nn.Conv2d(
            self.proj.in_channels,
            self.hidden_dim,
            kernel_size=kernel_size,
            stride=kernel_size,
        )
        self.up = torch.nn.Conv2d(
            self.hidden_dim,
            self.proj.out_channels,
            kernel_size=kernel_size,
            stride=kernel_size,
        )

        for p in self.proj.parameters():
            p.requires_grad = False

    def forward(self, x: torch.tensor) -> torch.tensor:
        x_lr = self.up(self.down(x))
        x = self.proj(x)

        x += x_lr

        return torch.einsum("bdhw,d->bdhw", x, self.scaler)


class ScaledLowRankAdapter(torch.nn.Module):
    """SLR adapter for linear layers.

    Adds a low rank adapter and scaling parameters to a linear layer"
    """

    def __init__(self, linear: torch.nn.Linear, hidden_dim: int = 16):
        super().__init__()

        self.hidden_dim = hidden_dim
        self.linear = linear
        self.out_dim, self.in_dim = self.linear.weight.shape

        # freeze original parameters
        for p in self.linear.parameters():
            p.requires_grad = False

        # initialize scaling vectors as ones
        self.in_scaler = torch.nn.Parameter(torch.ones(self.in_dim))
        self.out_scaler = torch.nn.Parameter(torch.ones(self.out_dim))

        self.down = torch.nn.Linear(self.in_dim, self.hidden_dim)
        self.up = torch.nn.Linear(self.hidden_dim, self.out_dim)

        # init low-rank matrices as normal/zeros
        self.up.weight.data.fill_(0)
        self.up.bias.data.fill_(0)
        torch.nn.init.normal_(self.down.weight.data)

    def forward(self, x: torch.tensor) -> torch.tensor:
        # x *= self.in_scaler
        # x_lr = self.up(self.down(x))
        # x = self.linear(x)
        # x += x_lr
        # x *= self.out_scaler

        # without in-place operations (chaned due to torch error, version above used for most experiments)
        x_scaled = x * self.in_scaler
        x_lr = self.up(self.down(x_scaled))
        x = self.linear(x_scaled)
        x_new = x + x_lr
        x = x_new * self.out_scaler

        # return x + x_lr
        return x

class ScaledLowRankConfigTimmViT:
    """Config to add SLR adapters to a timm ViT."""

    def __init__(
        self, hidden_dim: int = 8, patch_embed: bool = False, norm: bool = True
    ):
        self.lora_rank = hidden_dim
        self.adapter_modules = ".*attn|.*mlp|decoder_embed|decoder_pred"
        # # nn.leaner named fc in DINOv2, lin in SAM
        self.adapter_layers = "qkv|fc1|fc2|lin1|lin2|proj|decoder_embed|decoder_pred"
        if patch_embed:
            self.adapter_modules += "|patch_embed"
            self.adapter_layers += "|proj"
        self.model_modifier = "adapter"
        self.extra_trainable_param_names = "fcn_high.pred.7|fcn_low.pred.7"
        if norm:
            self.extra_trainable_param_names += "|.*norm.*"
        # self.extra_trainable_param_names = ".*norm.*|.*decoder_embed.*|.*decoder_pred.*"
        # self.extra_trainable_param_names = ".*norm.*|.*decoder_embed.*"

"""
usage:
model = add_extra_weights(
    model,
    config,
    ScaledLowRankAdapter,
    ScaledLowRankConvAdapter,
    adapter_trainable,
    only_scaler_trainable,
)
"""
def add_extra_weights(
    model,
    config,
    adapter,
    conv_adapter=None,
    trainable=True,
    only_scaler_trainable=False,
):
    # together with config of type ScaledLowRankConfigTimmViT
    for m_name, module in dict(model.named_modules()).items():
        if re.fullmatch(config.adapter_modules, m_name):
            children = dict(module.named_children())
            set_as_module = False
            if not children:
                set_as_module = True
                # if module is a layer
                children = {m_name: module}
            for c_name, layer in children.items():
                if re.fullmatch(config.adapter_layers, c_name):
                    if isinstance(layer, torch.nn.Linear):
                        adp = adapter
                    elif isinstance(layer, torch.nn.Conv2d):
                        adp = conv_adapter
                    else:
                        raise ValueError()
                    adapter_instance = adp(layer, hidden_dim=config.lora_rank)
                    if not trainable:
                        for p in adapter_instance.parameters():
                            p.requires_grad = False
                    if only_scaler_trainable:
                        for n, p in adapter_instance.named_parameters():
                            if "scaler" in n:
                                p.requires_grad = True
                            else:
                                p.requires_grad = False
                    if set_as_module:
                        setattr(model, c_name, adapter_instance)
                    else:
                        setattr(module, c_name, adapter_instance)

        # make extra params trainable (e.g., layer norm layers)
        if re.fullmatch(config.extra_trainable_param_names, m_name):
            for p in module.parameters():
                p.requires_grad = True

    return model

================
File: train_vigor_2DoF.py
================
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# from logging import _Level
import os

import torchvision.utils

os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1'

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms
from dataLoader.Vigor_dataset import load_vigor_data
from torch.utils.tensorboard import SummaryWriter
import torch.nn.functional as F
import scipy.io as scio

import ssl

ssl._create_default_https_context = ssl._create_unverified_context  # for downloading pretrained VGG weights

from model_vigor import ModelVigor

import numpy as np
import os
import argparse

from utils import gps2distance
import time


def test(net_test, args, save_path, epoch):
    ### net evaluation state
    net_test.eval()

    dataloader = load_vigor_data(args.batch_size, area=args.area, rotation_range=args.rotation_range,
                                 train=False)

    pred_us = []
    pred_vs = []

    gt_us = []
    gt_vs = []

    start_time = time.time()
    with torch.no_grad():
        for i, Data in enumerate(dataloader, 0):

            grd, sat, gt_shift_u, gt_shift_v, gt_rot, meter_per_pixel = [item.to(device) for item in Data]

            pred_u, pred_v = net_test(sat, grd, meter_per_pixel, gt_rot, mode='test')

            pred_u = pred_u * meter_per_pixel
            pred_v = pred_v * meter_per_pixel

            pred_us.append(pred_u.data.cpu().numpy())
            pred_vs.append(pred_v.data.cpu().numpy())

            gt_shift_u = gt_shift_u * meter_per_pixel * 512 / 4
            gt_shift_v = gt_shift_v * meter_per_pixel * 512 / 4

            gt_us.append(gt_shift_u.data.cpu().numpy())
            gt_vs.append(gt_shift_v.data.cpu().numpy())

            if i % 20 == 0:
                print(i)
    end_time = time.time()
    duration = (end_time - start_time) / len(dataloader) / args.batch_size

    pred_us = np.concatenate(pred_us, axis=0)
    pred_vs = np.concatenate(pred_vs, axis=0)

    gt_us = np.concatenate(gt_us, axis=0)
    gt_vs = np.concatenate(gt_vs, axis=0)

    scio.savemat(os.path.join(save_path, 'result.mat'), {'gt_us': gt_us, 'gt_vs': gt_vs,
                                                         'pred_us': pred_us, 'pred_vs': pred_vs,
                                                         })

    distance = np.sqrt((pred_us - gt_us) ** 2 + (pred_vs - gt_vs) ** 2)  # [N]
    init_dis = np.sqrt(gt_us ** 2 + gt_vs ** 2)


    metrics = [1, 3, 5]

    f = open(os.path.join(save_path, 'results.txt'), 'a')
    f.write('====================================\n')
    f.write('       EPOCH: ' + str(epoch) + '\n')
    print('====================================')
    print('       EPOCH: ' + str(epoch))
    line = 'Time per image (second): ' + str(duration) + '\n'
    print(line)
    f.write(line)

    line = 'Distance average: (init, pred)' + str(np.mean(init_dis)) + ' ' + str(np.mean(distance))
    print(line)
    f.write(line + '\n')
    line = 'Distance median: (init, pred)' + str(np.median(init_dis)) + ' ' + str(np.median(distance))
    print(line)
    f.write(line + '\n')

    for idx in range(len(metrics)):
        pred = np.sum(distance < metrics[idx]) / distance.shape[0] * 100
        init = np.sum(init_dis < metrics[idx]) / init_dis.shape[0] * 100

        line = 'distance within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    print('====================================')
    f.write('====================================\n')
    f.close()
    result = np.mean(distance)

    net_test.train()

    return


def val(dataloader, net_test, args, save_path, epoch, best=0.0):
    ### net evaluation state
    net_test.eval()

    pred_us = []
    pred_vs = []

    gt_us = []
    gt_vs = []

    start_time = time.time()
    with torch.no_grad():
        for i, Data in enumerate(dataloader, 0):

            grd, sat, gt_shift_u, gt_shift_v, gt_rot, meter_per_pixel = [item.to(device) for item in Data]

            pred_u, pred_v = net_test(sat, grd, meter_per_pixel, gt_rot, mode='test')

            pred_u = pred_u * meter_per_pixel
            pred_v = pred_v * meter_per_pixel

            pred_us.append(pred_u.data.cpu().numpy())
            pred_vs.append(pred_v.data.cpu().numpy())

            gt_shift_u = gt_shift_u * meter_per_pixel * 512 / 4
            gt_shift_v = gt_shift_v * meter_per_pixel * 512 / 4

            gt_us.append(gt_shift_u.data.cpu().numpy())
            gt_vs.append(gt_shift_v.data.cpu().numpy())

            if i % 20 == 0:
                print(i)
    end_time = time.time()
    duration = (end_time - start_time) / len(dataloader) / args.batch_size

    pred_us = np.concatenate(pred_us, axis=0)
    pred_vs = np.concatenate(pred_vs, axis=0)

    gt_us = np.concatenate(gt_us, axis=0)
    gt_vs = np.concatenate(gt_vs, axis=0)

    scio.savemat(os.path.join(save_path, 'result.mat'), {'gt_us': gt_us, 'gt_vs': gt_vs,
                                                         'pred_us': pred_us, 'pred_vs': pred_vs,
                                                         })

    distance = np.sqrt((pred_us - gt_us) ** 2 + (pred_vs - gt_vs) ** 2)  # [N]
    init_dis = np.sqrt(gt_us ** 2 + gt_vs ** 2)

    metrics = [1, 3, 5]

    f = open(os.path.join(save_path, 'val_results.txt'), 'a')
    f.write('====================================\n')
    f.write('       EPOCH: ' + str(epoch) + '\n')
    print('====================================')
    print('       EPOCH: ' + str(epoch))
    line = 'Time per image (second): ' + str(duration) + '\n'
    print(line)
    f.write(line)

    line = 'Distance average: (init, pred)' + str(np.mean(init_dis)) + ' ' + str(np.mean(distance))
    print(line)
    f.write(line + '\n')
    line = 'Distance median: (init, pred)' + str(np.median(init_dis)) + ' ' + str(np.median(distance))
    print(line)
    f.write(line + '\n')

    for idx in range(len(metrics)):
        pred = np.sum(distance < metrics[idx]) / distance.shape[0] * 100
        init = np.sum(init_dis < metrics[idx]) / init_dis.shape[0] * 100

        line = 'distance within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    print('====================================')
    f.write('====================================\n')
    f.close()
    result = np.mean(distance)

    net_test.train()

    if (result < best):
        if not os.path.exists(save_path):
            os.makedirs(save_path)
        torch.save(net.state_dict(), os.path.join(save_path, 'Model_best.pth'))

    print('Finished Val')
    return result


def triplet_loss(corr_maps, gt_shift_u, gt_shift_v):
    
   
    losses = []
    for level in range(len(corr_maps)):

        corr = corr_maps[level]
        B, corr_H, corr_W = corr.shape

        w = torch.round(corr_W / 2 - 0.5 + gt_shift_u * 512 / np.power(2, 3 - level) / 4).reshape(-1)
        h = torch.round(corr_H / 2 - 0.5 + gt_shift_v * 512 / np.power(2, 3 - level) / 4).reshape(-1)

        pos = corr[range(B), h.long(), w.long()]  # [B]
        # print(pos.shape)
        pos_neg = pos.reshape(-1, 1, 1) - corr  # [B, H, W]
        loss = torch.sum(torch.log(1 + torch.exp(pos_neg * 10))) / (B * (corr_H * corr_W - 1))
        losses.append(loss)

    return torch.sum(torch.stack(losses, dim=0))


def train(net, args, save_path):
    bestResult = 0.0

    time_start = time.time()
    for epoch in range(args.resume, args.epochs):
        net.train()

        base_lr = 1e-4

        optimizer = optim.Adam(net.parameters(), lr=base_lr)
        optimizer.zero_grad()

        trainloader, valloader = load_vigor_data(args.batch_size, area=args.area, rotation_range=args.rotation_range,
                                                 train=True)

        print('batch_size:', args.batch_size, '\n num of batches:', len(trainloader))

        for Loop, Data in enumerate(trainloader, 0):
            grd, sat, gt_shift_u, gt_shift_v, gt_rot, meter_per_pixel = [item.to(device) for item in Data]

            corr_maps0, corr_maps1, corr_maps2 = net(sat, grd, meter_per_pixel, gt_rot, mode='train')
            loss = triplet_loss([corr_maps0, corr_maps1, corr_maps2], gt_shift_u, gt_shift_v)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()  # This step is responsible for updating weights

            if Loop % 10 == 9:
                time_end = time.time()
                print('Epoch: ' + str(epoch) + ' Loop: ' + str(Loop) +
                      ' triplet loss: ' + str(np.round(loss.item(), decimals=4)) +
                      ' Time: ' + str(time_end - time_start)
                      )
                time_start = time_end

        print('Save Model ...')
        if not os.path.exists(save_path):
            os.makedirs(save_path)

        torch.save(net.state_dict(), os.path.join(save_path, 'model_' + str(epoch) + '.pth'))

        bestResult = val(valloader, net, args, save_path, epoch, best=bestResult)

    print('Finished Training')


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--resume', type=int, default=0, help='resume the trained model')
    parser.add_argument('--test', type=int, default=0, help='test with trained model')
    parser.add_argument('--debug', type=int, default=0, help='debug to dump middle processing images')

    parser.add_argument('--epochs', type=int, default=20, help='number of training epochs')

    parser.add_argument('--rotation_range', type=float, default=0., help='degree')

    parser.add_argument('--batch_size', type=int, default=8, help='batch size')

    parser.add_argument('--proj', type=str, default='CrossAttn', help='geo, polar, nn, CrossAttn')

    parser.add_argument('--use_uncertainty', type=int, default=1, help='0 or 1')
    
    parser.add_argument('--area', type=str, default='cross', help='same or cross')
    parser.add_argument('--multi_gpu', type=int, default=1, help='0 or 1')

    args = parser.parse_args()

    return args


def getSavePath(args):
    save_path = './ModelsVigor/' \
                + str(args.proj) + '_' + args.area

    if args.use_uncertainty:
        save_path = save_path + '_Uncertainty'

    print('save_path:', save_path)

    return save_path


if __name__ == '__main__':

    if torch.cuda.is_available():
        device = torch.device("cuda:0")
    else:
        device = torch.device("cpu")

    np.random.seed(2022)

    args = parse_args()

    mini_batch = args.batch_size

    save_path = getSavePath(args)

    net = ModelVigor(args)
    if args.multi_gpu:
        # net = MultiGPU(net, dim=0)
        net = nn.DataParallel(net, dim=0)

    ### cudaargs.epochs, args.debug)
    net.to(device)
    ###########################

    if args.test:
        net.load_state_dict(torch.load(os.path.join(save_path, 'model_9.pth')))
        current = test(net, args, save_path, epoch=0)

    else:

        if args.resume:
            net.load_state_dict(torch.load(os.path.join(save_path, 'model_' + str(args.resume - 1) + '.pth')))
            print("resume from " + 'model_' + str(args.resume - 1) + '.pth')

        train(net, args, save_path)

================
File: model_vigor.py
================
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import torch
from torchvision import transforms
import utils
import os
import torchvision.transforms.functional as TF

# from GRU1 import ElevationEsitimate,VisibilityEsitimate,VisibilityEsitimate2,GRUFuse
# from VGG import VGGUnet, VGGUnet_G2S
from VGG import VGGUnet, VGGUnet_G2S, Encoder, Decoder, Decoder2, Decoder4, VGGUnetTwoDec
from jacobian import grid_sample

from models_ford import loss_func
from RNNs import NNrefine, Uncertainty
from swin_transformer import TransOptimizerS2GP_V1, TransOptimizerG2SP_V1
from swin_transformer_cross import TransOptimizerG2SP, TransOptimizerG2SPV2, SwinTransformerSelf
from cross_attention import CrossViewAttention

EPS = utils.EPS


class ModelVigor(nn.Module):
    def __init__(self, args):  # device='cuda:0',
        super(ModelVigor, self).__init__()

        self.args = args

        self.level = 3

        self.rotation_range = args.rotation_range

        self.SatFeatureNet = VGGUnet(self.level)
        if self.args.proj == 'CrossAttn':
            self.GrdEnc = Encoder()
            self.GrdDec = Decoder()
            self.Dec4 = Decoder4()
            self.Dec2 = Decoder2()
            self.CVattn = CrossViewAttention(blocks=2, dim=256, heads=4, dim_head=16, qkv_bias=False)
        else:
            self.GrdFeatureNet = VGGUnet(self.level)

        self.grd_height = -2

        if self.args.use_uncertainty:
            self.uncertain_net = Uncertainty()

        torch.autograd.set_detect_anomaly(True)

    def sat2grd_uv(self, rot, shift_u, shift_v, level, H, W, meter_per_pixel):
        '''
        rot.shape = [B]
        shift_u.shape = [B]
        shift_v.shape = [B]
        H: scalar  height of grd feature map, from which projection is conducted
        W: scalar  width of grd feature map, from which projection is conducted
        '''

        B = shift_u.shape[0]

        S = 512 / np.power(2, 3 - level)
        shift_u = shift_u * S / 4
        shift_v = shift_v * S / 4

        # ii,jj
        ii, jj = torch.meshgrid(torch.arange(0, S, dtype=torch.float32, device=shift_u.device),
                                torch.arange(0, S, dtype=torch.float32, device=shift_u.device))
        ii = ii.unsqueeze(dim=0).repeat(B, 1, 1)  # [B, S, S] v dimension
        jj = jj.unsqueeze(dim=0).repeat(B, 1, 1)  # [B, S, S] u dimension

        # python
        radius = torch.sqrt((ii - (S / 2 - 0.5 + shift_v.reshape(-1, 1, 1))) ** 2 + (
                    jj - (S / 2 - 0.5 + shift_u.reshape(-1, 1, 1))) ** 2)

        theta = torch.atan2(ii - (S / 2 - 0.5 + shift_v.reshape(-1, 1, 1)),
                            jj - (S / 2 - 0.5 + shift_u.reshape(-1, 1, 1)))
        theta = (-np.pi / 2 + (theta) % (2 * np.pi)) % (2 * np.pi)
        theta = (theta + rot[:, None, None] * self.args.rotation_range / 180 * np.pi) % (2 * np.pi)

        theta = theta / 2 / np.pi * W

        # meter_per_pixel = self.meter_per_pixel_dict[city] * 512 / S
        meter_per_pixel = meter_per_pixel * np.power(2, 3 - level)
        phimin = torch.atan2(radius * meter_per_pixel[:, None, None], torch.tensor(self.grd_height))
        phimin = phimin / np.pi * H

        uv = torch.stack([theta, phimin], dim=-1)

        return uv

    def project_grd_to_map(self, grd_f, grd_c, rot, shift_u, shift_v, level, meter_per_pixel):
        '''
        grd_f.shape = [B, C, H, W]
        shift_u.shape = [B]
        shift_v.shape = [B]
        '''
        B, C, H, W = grd_f.size()
        uv = self.sat2grd_uv(rot, shift_u, shift_v, level, H, W, meter_per_pixel)  # [B, S, S, 2]
        grd_f_trans, _ = grid_sample(grd_f, uv)
        if grd_c is not None:
            grd_c_trans, _ = grid_sample(grd_c, uv)
        else:
            grd_c_trans = None
        return grd_f_trans, grd_c_trans, uv[..., 0]

    def triplet_loss(self, corr_maps, gt_shift_u, gt_shift_v): 
        losses = []
        for level in range(len(corr_maps)):

            corr = corr_maps[level]
            B, corr_H, corr_W = corr.shape

            w = torch.round(corr_W / 2 - 0.5 + gt_shift_u / np.power(2, 3 - level)).reshape(-1)
            h = torch.round(corr_H / 2 - 0.5 + gt_shift_v / np.power(2, 3 - level)).reshape(-1)

            pos = corr[range(B), h.long(), w.long()]  # [B]
            # print(pos.shape)
            pos_neg = pos.reshape(-1, 1, 1) - corr  # [B, H, W]
            loss = torch.sum(torch.log(1 + torch.exp(pos_neg * 10))) / (B * (corr_H * corr_W - 1))
            # import pdb; pdb.set_trace()
            losses.append(loss)

        return torch.sum(torch.stack(losses, dim=0))

    def forward(self, sat_map, grd_img_left, meter_per_pixel, gt_rot=None, mode='train'):
        '''
        Args:
            sat_map: [B, C, A, A] A--> sidelength
            left_camera_k: [B, 3, 3]
            grd_img_left: [B, C, H, W]
            gt_shift_u: [B, 1] u->longitudinal
            gt_shift_v: [B, 1] v->lateral
            gt_heading: [B, 1] east as 0-degree
            mode:
            file_name:

        Returns:

        '''
        B, _, ori_grdH, ori_grdW = grd_img_left.shape

        sat_feat_list, sat_conf_list = self.SatFeatureNet(sat_map)
        if self.args.use_uncertainty:
            sat_uncer_list = self.uncertain_net(sat_feat_list)

        grd8, grd4, grd2 = self.GrdEnc(grd_img_left)
        # [H/8, W/8] [H/4, W/4] [H/2, W/2]
        grd_feat_list = self.GrdDec(grd8, grd4, grd2)


        B, _, ori_grdH, ori_grdW = grd_img_left.shape

        shift_u = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        shift_v = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)

        grd2sat8, _, u = self.project_grd_to_map(
            grd_feat_list[0], None, gt_rot, shift_u, shift_v, level=0, meter_per_pixel=meter_per_pixel
        )
        grd2sat4, _, _ = self.project_grd_to_map(
            grd_feat_list[1], None, gt_rot, shift_u, shift_v, level=1, meter_per_pixel=meter_per_pixel
        )
        grd2sat2, _, _ = self.project_grd_to_map(
            grd_feat_list[2], None, gt_rot, shift_u, shift_v, level=2, meter_per_pixel=meter_per_pixel
        )

        # 4cross-view transformer
        grd2sat8_attn = self.CVattn(grd2sat8, grd8, u, geo_mask=None)
        grd2sat4_attn = grd2sat4 + self.Dec4(grd2sat8_attn, grd2sat4)
        grd2sat2_attn = grd2sat2 + self.Dec2(grd2sat4_attn, grd2sat2)

        grd_feat_list = [grd2sat8_attn, grd2sat4_attn, grd2sat2_attn]

        corr_maps = []

        if mode == 'train':
            for level in range(len(sat_feat_list)):
                # meter_per_pixel = self.meters_per_pixel[level]

                sat_feat = sat_feat_list[level]
                grd_feat = grd_feat_list[level]

                A = sat_feat.shape[-1]

                crop_size = int(A * 0.4)

                g2s_feat = TF.center_crop(grd_feat, [crop_size, crop_size])
                g2s_feat = F.normalize(g2s_feat.reshape(B, -1)).reshape(B, -1, crop_size, crop_size)

                s_feat = sat_feat.reshape(1, -1, A, A)  # [B, C, H, W]->[1, B*C, H, W]
                corr = F.conv2d(s_feat, g2s_feat, groups=B)[0]  # [B, H, W]

                denominator = F.avg_pool2d(sat_feat.pow(2), (crop_size, crop_size), stride=1, divisor_override=1)  # [B, 4W]
                if self.args.use_uncertainty:
                    denominator = torch.sum(denominator, dim=1) * TF.center_crop(sat_uncer_list[level], [corr.shape[1], corr.shape[2]])[:, 0]
                else:
                    denominator = torch.sum(denominator, dim=1)  # [B, H, W]
                denominator = torch.maximum(torch.sqrt(denominator), torch.ones_like(denominator) * 1e-6)
                corr = 2 - 2 * corr / denominator
                corr_maps.append(corr)

            return corr_maps[0], corr_maps[1], corr_maps[2]

        
        else:
            level = 2

            sat_feat = sat_feat_list[level]
            grd_feat = grd_feat_list[level]

            A = sat_feat.shape[-1]

            crop_size = int(A * 0.4)

            g2s_feat = TF.center_crop(grd_feat, [crop_size, crop_size])
            g2s_feat = F.normalize(g2s_feat.reshape(B, -1)).reshape(B, -1, crop_size, crop_size)

            s_feat = sat_feat.reshape(1, -1, A, A)  # [B, C, H, W]->[1, B*C, H, W]
            corr = F.conv2d(s_feat, g2s_feat, groups=B)[0]  # [B, H, W]

            denominator = F.avg_pool2d(sat_feat.pow(2), (crop_size, crop_size), stride=1, divisor_override=1)  # [B, 4W]
            if self.args.use_uncertainty:
                denominator = torch.sum(denominator, dim=1) * TF.center_crop(sat_uncer_list[level], [corr.shape[1], corr.shape[2]])[:, 0]
            else:
                denominator = torch.sum(denominator, dim=1)  # [B, H, W]
            denominator = torch.maximum(torch.sqrt(denominator), torch.ones_like(denominator) * 1e-6)
            corr = 2 - 2 * corr / denominator

            B, corr_H, corr_W = corr.shape

            corr_maps.append(corr)

            max_index = torch.argmin(corr.reshape(B, -1), dim=1)
            pred_u = (max_index % corr_W - (corr_W / 2 + 0.5)) #* meter_per_pixel  # / self.args.shift_range_lon
            pred_v = (max_index // corr_W - (corr_H / 2 + 0.5)) #* meter_per_pixel  # / self.args.shift_range_lat
                
            return pred_u * 2, pred_v * 2  # [B], [B]

================
File: models_kitti.py
================
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import torch
import utils
import torchvision.transforms.functional as TF

from VGG import VGGUnet, Encoder, Decoder, Decoder2, Decoder4
from jacobian import grid_sample

from models_ford import loss_func
from RNNs import Uncertainty
from swin_transformer import TransOptimizerS2GP_V1, TransOptimizerG2SP_V1
from cross_attention import CrossViewAttention

EPS = utils.EPS

class Model(nn.Module):
    def __init__(self, args):  # device='cuda:0',
        super(Model, self).__init__()

        self.args = args

        self.level = args.level
        self.N_iters = args.N_iters

        self.SatFeatureNet = VGGUnet(self.level)

        if self.args.proj == 'CrossAttn':
            self.GrdEnc = Encoder()
            self.GrdDec = Decoder()
            self.Dec4 = Decoder4()
            self.Dec2 = Decoder2()
            self.CVattn = CrossViewAttention(blocks=2, dim=256, heads=4, dim_head=16, qkv_bias=False)
        else:
            self.GrdFeatureNet = VGGUnet(self.level)

        self.meters_per_pixel = []
        meter_per_pixel = utils.get_meter_per_pixel()
        for level in range(4):
            self.meters_per_pixel.append(meter_per_pixel * (2 ** (3 - level)))

        if self.args.Optimizer == 'TransV1G2SP':
            self.TransRefine = TransOptimizerG2SP_V1()
        elif self.args.Optimizer == 'TransV1S2GP':
            self.TransRefine = TransOptimizerS2GP_V1()
            ori_grdH, ori_grdW = 256, 1024
            xyz_grds = []
            for level in range(4):
                grd_H, grd_W = ori_grdH / (2 ** (3 - level)), ori_grdW / (2 ** (3 - level))

                xyz_grd, mask, xyz_w = self.grd_img2cam(grd_H, grd_W, ori_grdH,
                                                        ori_grdW)  # [1, grd_H, grd_W, 3] under the grd camera coordinates
                xyz_grds.append((xyz_grd, mask, xyz_w))

            self.xyz_grds = xyz_grds

        if self.args.rotation_range > 0:
            self.coe_R = nn.Parameter(torch.tensor(-5., dtype=torch.float32), requires_grad=True)
            self.coe_T = nn.Parameter(torch.tensor(-3., dtype=torch.float32), requires_grad=True)

        if self.args.use_uncertainty:
            self.uncertain_net = Uncertainty()

        torch.autograd.set_detect_anomaly(True)
        # Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function.
        # Any backward computation that generate nan value will raise an error.

    def grd_img2cam(self, grd_H, grd_W, ori_grdH, ori_grdW):

        ori_camera_k = torch.tensor([[[582.9802, 0.0000, 496.2420],
                                      [0.0000, 482.7076, 125.0034],
                                      [0.0000, 0.0000, 1.0000]]],
                                    dtype=torch.float32, requires_grad=True)  # [1, 3, 3]

        camera_height = utils.get_camera_height()

        camera_k = ori_camera_k.clone()
        camera_k[:, :1, :] = ori_camera_k[:, :1,
                             :] * grd_W / ori_grdW  # original size input into feature get network/ output of feature get network
        camera_k[:, 1:2, :] = ori_camera_k[:, 1:2, :] * grd_H / ori_grdH
        camera_k_inv = torch.inverse(camera_k)  # [B, 3, 3]

        v, u = torch.meshgrid(torch.arange(0, grd_H, dtype=torch.float32),
                              torch.arange(0, grd_W, dtype=torch.float32))
        uv1 = torch.stack([u, v, torch.ones_like(u)], dim=-1).unsqueeze(dim=0)  # [1, grd_H, grd_W, 3]
        xyz_w = torch.sum(camera_k_inv[:, None, None, :, :] * uv1[:, :, :, None, :], dim=-1)  # [1, grd_H, grd_W, 3]

        w = camera_height / torch.where(torch.abs(xyz_w[..., 1:2]) > utils.EPS, xyz_w[..., 1:2],
                                        utils.EPS * torch.ones_like(xyz_w[..., 1:2]))  # [BN, grd_H, grd_W, 1]
        xyz_grd = xyz_w * w  # [1, grd_H, grd_W, 3] under the grd camera coordinates

        mask = (xyz_grd[..., -1] > 0).float()  # # [1, grd_H, grd_W]

        return xyz_grd, mask, xyz_w

    def grd2cam2world2sat(self, ori_shift_u, ori_shift_v, ori_heading, level, satmap_sidelength,):
        '''
        realword: X: south, Y:down, Z: east
        camera: u:south, v: down from center (when heading east, need to rotate heading angle)
        Args:
            ori_shift_u: [B, 1]
            ori_shift_v: [B, 1]
            heading: [B, 1]
            XYZ_1: [H,W,4]
            ori_camera_k: [B,3,3]
            grd_H:
            grd_W:
            ori_grdH:
            ori_grdW:

        Returns:
        '''
        B, _ = ori_heading.shape
        heading = ori_heading * self.args.rotation_range / 180 * np.pi
        shift_u = ori_shift_u * self.args.shift_range_lon
        shift_v = ori_shift_v * self.args.shift_range_lat

        cos = torch.cos(heading)
        sin = torch.sin(heading)
        zeros = torch.zeros_like(cos)
        ones = torch.ones_like(cos)
        R = torch.cat([cos, zeros, -sin, zeros, ones, zeros, sin, zeros, cos], dim=-1)  # shape = [B, 9]
        R = R.view(B, 3, 3)  # shape = [B, N, 3, 3]
        # this R is the inverse of the R in G2SP

        camera_height = utils.get_camera_height()
        # camera offset, shift[0]:east,Z, shift[1]:north,X
        height = camera_height * torch.ones_like(shift_u[:, :1])
        T0 = torch.cat([shift_v, height, -shift_u], dim=-1)  # shape = [B, 3]
        T = torch.sum(-R * T0[:, None, :], dim=-1)  # [B, 3]
        # The above R, T define transformation from camera to world

        xyz_grd = self.xyz_grds[level][0].detach().to(ori_shift_u.device).repeat(B, 1, 1, 1)
        mask = self.xyz_grds[level][1].detach().to(ori_shift_u.device).repeat(B, 1, 1)  # [B, grd_H, grd_W]
        grd_H, grd_W = xyz_grd.shape[1:3]

        xyz = torch.sum(R[:, None, None, :, :] * xyz_grd[:, :, :, None, :], dim=-1) + T[:, None, None, :]
        # [B, grd_H, grd_W, 3]

        R_sat = torch.tensor([0, 0, 1, 1, 0, 0], dtype=torch.float32, device=ori_shift_u.device, requires_grad=True) \
            .reshape(2, 3)
        zx = torch.sum(R_sat[None, None, None, :, :] * xyz[:, :, :, None, :], dim=-1)
        # [B, grd_H, grd_W, 2]

        meter_per_pixel = utils.get_meter_per_pixel()
        meter_per_pixel *= utils.get_process_satmap_sidelength() / satmap_sidelength
        sat_uv = zx / meter_per_pixel + satmap_sidelength / 2  # [B, grd_H, grd_W, 2] sat map uv

        return sat_uv, mask

    def project_map_to_grd(self, sat_f, sat_c, shift_u, shift_v, heading, level):
        '''
        Args:
            sat_f: [B, C, H, W]
            sat_c: [B, 1, H, W]
            shift_u: [B, 2]
            shift_v: [B, 2]
            heading: [B, 1]
            camera_k: [B, 3, 3]

            ori_grdH:
            ori_grdW:

        Returns:

        '''
        B, C, satmap_sidelength, _ = sat_f.size()

        uv, mask = self.grd2cam2world2sat(shift_u, shift_v, heading, level, satmap_sidelength)
        # [B, H, W, 2], [B, H, W], [B, H, W, 2], [B, H, W, 2], [B,H, W, 2]

        B, grd_H, grd_W, _ = uv.shape

        sat_f_trans, _ = grid_sample(sat_f, uv, jac=None)
        sat_f_trans = sat_f_trans * mask[:, None, :, :]

        if sat_c is not None:
            sat_c_trans, _ = grid_sample(sat_c, uv)
            sat_c_trans = sat_c_trans * mask[:, None, :, :]
        else:
            sat_c_trans = None

        return sat_f_trans, sat_c_trans, uv * mask[:, :, :, None], mask

    def sat2world(self, satmap_sidelength):
        # satellite: u:east , v:south from bottomleft and u_center: east; v_center: north from center
        # realword: X: south, Y:down, Z: east   origin is set to the ground plane

        # meshgrid the sat pannel
        i = j = torch.arange(0, satmap_sidelength).cuda()  # to(self.device)
        ii, jj = torch.meshgrid(i, j)  # i:h,j:w

        # uv is coordinate from top/left, v: south, u:east
        uv = torch.stack([jj, ii], dim=-1).float()  # shape = [satmap_sidelength, satmap_sidelength, 2]

        # sat map from top/left to center coordinate
        u0 = v0 = satmap_sidelength // 2
        uv_center = uv - torch.tensor(
            [u0, v0]).cuda()  # .to(self.device) # shape = [satmap_sidelength, satmap_sidelength, 2]

        # affine matrix: scale*R
        meter_per_pixel = utils.get_meter_per_pixel()
        meter_per_pixel *= utils.get_process_satmap_sidelength() / satmap_sidelength
        R = torch.tensor([[0, 1], [1, 0]]).float().cuda()  # to(self.device) # u_center->z, v_center->x
        Aff_sat2real = meter_per_pixel * R  # shape = [2,2]

        # Trans matrix from sat to realword
        XZ = torch.einsum('ij, hwj -> hwi', Aff_sat2real,
                          uv_center)  # shape = [satmap_sidelength, satmap_sidelength, 2]

        Y = torch.zeros_like(XZ[..., 0:1])
        ones = torch.ones_like(Y)
        sat2realwap = torch.cat([XZ[:, :, :1], Y, XZ[:, :, 1:], ones], dim=-1)  # [sidelength,sidelength,4]

        return sat2realwap

    def World2GrdImgPixCoordinates(self, ori_shift_u, ori_shift_v, ori_heading, XYZ_1, ori_camera_k, grd_H, grd_W,
                                   ori_grdH, ori_grdW):
        # realword: X: south, Y:down, Z: east
        # camera: u:south, v: down from center (when heading east, need to rotate heading angle)
        # XYZ_1:[H,W,4], heading:[B,1], camera_k:[B,3,3], shift:[B,2]
        B = ori_heading.shape[0]
        shift_u_meters = self.args.shift_range_lon * ori_shift_u
        shift_v_meters = self.args.shift_range_lat * ori_shift_v
        heading = ori_heading * self.args.rotation_range / 180 * np.pi

        cos = torch.cos(-heading)
        sin = torch.sin(-heading)
        zeros = torch.zeros_like(cos)
        ones = torch.ones_like(cos)
        R = torch.cat([cos, zeros, -sin, zeros, ones, zeros, sin, zeros, cos], dim=-1)  # shape = [B,9]
        R = R.view(B, 3, 3)  # shape = [B,3,3]

        camera_height = utils.get_camera_height()
        # camera offset, shift[0]:east,Z, shift[1]:north,X
        height = camera_height * torch.ones_like(shift_u_meters)
        T = torch.cat([shift_v_meters, height, -shift_u_meters], dim=-1)  # shape = [B, 3]
        T = torch.unsqueeze(T, dim=-1)  # shape = [B,3,1]

        # P = K[R|T]
        camera_k = ori_camera_k.clone()
        camera_k[:, :1, :] = ori_camera_k[:, :1,
                             :] * grd_W / ori_grdW  # original size input into feature get network/ output of feature get network
        camera_k[:, 1:2, :] = ori_camera_k[:, 1:2, :] * grd_H / ori_grdH
        P = camera_k @ torch.cat([R, T], dim=-1)

        uv1 = torch.sum(P[:, None, None, :, :] * XYZ_1[None, :, :, None, :], dim=-1)
        # only need view in front of camera ,Epsilon = 1e-6
        uv1_last = torch.maximum(uv1[:, :, :, 2:], torch.ones_like(uv1[:, :, :, 2:]) * 1e-6)
        uv = uv1[:, :, :, :2] / uv1_last  # shape = [B, H, W, 2]

        mask = torch.greater(uv1_last, torch.ones_like(uv1[:, :, :, 2:]) * 1e-6)

        return uv, mask

    def project_grd_to_map(self, grd_f, grd_c, shift_u, shift_v, heading, camera_k, satmap_sidelength, ori_grdH,
                           ori_grdW):
        '''
        grd_f: [B, C, H, W]
        grd_c: [B, 1, H, W]
        shift_u: [B, 1]
        shift_v: [B, 1]
        heading: [B, 1]
        camera_k: [B, 3, 3]
        satmap_sidelength: scalar
        ori_grdH: scalar
        ori_grdW: scalar
        '''

        B, C, H, W = grd_f.size()

        XYZ_1 = self.sat2world(satmap_sidelength)  # [ sidelength,sidelength,4]
        uv, mask = self.World2GrdImgPixCoordinates(shift_u, shift_v, heading, XYZ_1, camera_k,
                                                   H, W, ori_grdH, ori_grdW)  # [B, S, E, H, W,2]
        # [B, H, W, 2], [2, B, H, W, 2], [1, B, H, W, 2]

        grd_f_trans, _ = grid_sample(grd_f, uv, jac=None)
        # [B,C,sidelength,sidelength], [3, B, C, sidelength, sidelength]
        if grd_c is not None:
            grd_c_trans, _ = grid_sample(grd_c, uv)
        else:
            grd_c_trans = None

        return grd_f_trans, grd_c_trans, uv[..., 0], mask

    def Trans_update(self, shift_u, shift_v, heading, grd_feat_proj, sat_feat):
        B = shift_u.shape[0]
        grd_feat_norm = torch.norm(grd_feat_proj.reshape(B, -1), p=2, dim=-1)
        grd_feat_norm = torch.maximum(grd_feat_norm, 1e-6 * torch.ones_like(grd_feat_norm))
        grd_feat_proj = grd_feat_proj / grd_feat_norm[:, None, None, None]

        delta = self.TransRefine(grd_feat_proj, sat_feat)  # [B, 3]

        shift_u_new = shift_u + delta[:, 0:1]
        shift_v_new = shift_v + delta[:, 1:2]
        heading_new = heading + delta[:, 2:3]

        B = shift_u.shape[0]

        rand_u = torch.distributions.uniform.Uniform(-1, 1).sample([B, 1]).to(shift_u.device)
        rand_v = torch.distributions.uniform.Uniform(-1, 1).sample([B, 1]).to(shift_u.device)
        rand_u.requires_grad = True
        rand_v.requires_grad = True
        shift_u_new = torch.where((shift_u_new > -2) & (shift_u_new < 2), shift_u_new, rand_u)
        shift_v_new = torch.where((shift_v_new > -2) & (shift_v_new < 2), shift_v_new, rand_v)

        return shift_u_new, shift_v_new, heading_new

    def corr(self, sat_map, grd_img_left, left_camera_k, gt_shift_u=None, gt_shift_v=None, gt_heading=None,
             mode='train'):
        '''
        Args:
            sat_map: [B, C, A, A] A--> sidelength
            left_camera_k: [B, 3, 3]
            grd_img_left: [B, C, H, W]
            gt_shift_u: [B, 1] u->longitudinal
            gt_shift_v: [B, 1] v->lateral
            gt_heading: [B, 1] east as 0-degree
            mode:
            file_name:

        Returns:

        '''

        B, _, ori_grdH, ori_grdW = grd_img_left.shape

        sat_feat_list, sat_conf_list = self.SatFeatureNet(sat_map)

        grd_feat_list, grd_conf_list = self.GrdFeatureNet(grd_img_left)

        shift_u = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        shift_v = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        # heading = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)

        corr_maps = []

        for level in range(len(sat_feat_list)):
            meter_per_pixel = self.meters_per_pixel[level]

            sat_feat = sat_feat_list[level]
            grd_feat = grd_feat_list[level]

            A = sat_feat.shape[-1]
            heading = gt_heading + np.random.uniform(- self.args.coe_heading_aug, self.args.coe_heading_aug)
            grd_feat_proj, _, u, mask = self.project_grd_to_map(
                grd_feat, None, shift_u, shift_v, heading, left_camera_k, A, ori_grdH, ori_grdW)

            crop_H = int(A - self.args.shift_range_lat * 3 / meter_per_pixel)
            crop_W = int(A - self.args.shift_range_lon * 3 / meter_per_pixel)
            g2s_feat = TF.center_crop(grd_feat_proj, [crop_H, crop_W])
            g2s_feat = F.normalize(g2s_feat.reshape(B, -1)).reshape(B, -1, crop_H, crop_W)

            s_feat = sat_feat.reshape(1, -1, A, A)  # [B, C, H, W]->[1, B*C, H, W]
            corr = F.conv2d(s_feat, g2s_feat, groups=B)[0]  # [B, H, W]

            denominator = F.avg_pool2d(sat_feat.pow(2), (crop_H, crop_W), stride=1, divisor_override=1)  # [B, 4W]
            denominator = torch.sum(denominator, dim=1)  # [B, H, W]
            denominator = torch.maximum(torch.sqrt(denominator), torch.ones_like(denominator) * 1e-6)
            corr = 2 - 2 * corr / denominator

            B, corr_H, corr_W = corr.shape

            corr_maps.append(corr)

            max_index = torch.argmin(corr.reshape(B, -1), dim=1)
            pred_u = (max_index % corr_W - corr_W / 2) * meter_per_pixel  # / self.args.shift_range_lon
            pred_v = -(max_index // corr_W - corr_H / 2) * meter_per_pixel  # / self.args.shift_range_lat

            cos = torch.cos(gt_heading[:, 0] * self.args.rotation_range / 180 * np.pi)
            sin = torch.sin(gt_heading[:, 0] * self.args.rotation_range / 180 * np.pi)

            pred_u1 = pred_u * cos + pred_v * sin
            pred_v1 = - pred_u * sin + pred_v * cos


        if mode == 'train':
            return self.triplet_loss(corr_maps, gt_shift_u, gt_shift_v, gt_heading)
        else:
            return pred_u1, pred_v1  # [B], [B]

    def triplet_loss(self, corr_maps, gt_shift_u, gt_shift_v, gt_heading):
        cos = torch.cos(gt_heading[:, 0] * self.args.rotation_range / 180 * np.pi)
        sin = torch.sin(gt_heading[:, 0] * self.args.rotation_range / 180 * np.pi)

        gt_delta_x = - gt_shift_u[:, 0] * self.args.shift_range_lon
        gt_delta_y = - gt_shift_v[:, 0] * self.args.shift_range_lat

        gt_delta_x_rot = - gt_delta_x * cos + gt_delta_y * sin
        gt_delta_y_rot = gt_delta_x * sin + gt_delta_y * cos

        losses = []
        for level in range(len(corr_maps)):
            meter_per_pixel = self.meters_per_pixel[level]

            corr = corr_maps[level]
            B, corr_H, corr_W = corr.shape

            w = torch.round(corr_W / 2 - 0.5 + gt_delta_x_rot / meter_per_pixel)
            h = torch.round(corr_H / 2 - 0.5 + gt_delta_y_rot / meter_per_pixel)

            pos = corr[range(B), h.long(), w.long()]  # [B]
            pos_neg = pos.reshape(-1, 1, 1) - corr  # [B, H, W]
            loss = torch.sum(torch.log(1 + torch.exp(pos_neg * 10))) / (B * (corr_H * corr_W - 1))
            losses.append(loss)

        return torch.sum(torch.stack(losses, dim=0))

    def CVattn_corr(self, sat_map, grd_img_left, left_camera_k, gt_shift_u=None, gt_shift_v=None, gt_heading=None,
                    mode='train'):
        '''
        Args:
            sat_map: [B, C, A, A] A--> sidelength
            left_camera_k: [B, 3, 3]
            grd_img_left: [B, C, H, W]
            gt_shift_u: [B, 1] u->longitudinal
            gt_shift_v: [B, 1] v->lateral
            gt_heading: [B, 1] east as 0-degree
            mode:
            file_name:

        Returns:

        '''

        B, _, ori_grdH, ori_grdW = grd_img_left.shape

        sat_feat_list, sat_conf_list = self.SatFeatureNet(sat_map)
        if self.args.use_uncertainty:
            sat_uncer_list = self.uncertain_net(sat_feat_list)
        sat8, sat4, sat2 = sat_feat_list

        grd8, grd4, grd2 = self.GrdEnc(grd_img_left)
        # [H/8, W/8] [H/4, W/4] [H/2, W/2]
        grd_feat_list = self.GrdDec(grd8, grd4, grd2)

        shift_u = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        shift_v = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        heading = gt_heading

        grd2sat8, _, u, mask = self.project_grd_to_map(
            grd_feat_list[0], None, shift_u, shift_v, heading, left_camera_k, sat8.shape[-1], ori_grdH, ori_grdW)
        grd2sat4, _, _, _ = self.project_grd_to_map(
            grd_feat_list[1], None, shift_u, shift_v, heading, left_camera_k, sat4.shape[-1], ori_grdH, ori_grdW)
        grd2sat2, _, _, _ = self.project_grd_to_map(
            grd_feat_list[2], None, shift_u, shift_v, heading, left_camera_k, sat2.shape[-1], ori_grdH, ori_grdW)

        grd2sat8_attn = self.CVattn(grd2sat8, grd8, u, mask)
        grd2sat4_attn = grd2sat4 + self.Dec4(grd2sat8_attn, grd2sat4)
        grd2sat2_attn = grd2sat2 + self.Dec2(grd2sat4_attn, grd2sat2)

        grd_feat_list = [grd2sat8_attn, grd2sat4_attn, grd2sat2_attn]

        corr_maps = []

        for level in range(len(sat_feat_list)):
            meter_per_pixel = self.meters_per_pixel[level]

            sat_feat = sat_feat_list[level]
            grd_feat = grd_feat_list[level]

            A = sat_feat.shape[-1]

            crop_H = int(A - self.args.shift_range_lat * 3 / meter_per_pixel)
            crop_W = int(A - self.args.shift_range_lon * 3 / meter_per_pixel)
            g2s_feat = TF.center_crop(grd_feat, [crop_H, crop_W])
            g2s_feat = F.normalize(g2s_feat.reshape(B, -1)).reshape(B, -1, crop_H, crop_W)

            s_feat = sat_feat.reshape(1, -1, A, A)  # [B, C, H, W]->[1, B*C, H, W]
            corr = F.conv2d(s_feat, g2s_feat, groups=B)[0]  # [B, H, W]

            denominator = F.avg_pool2d(sat_feat.pow(2), (crop_H, crop_W), stride=1, divisor_override=1)  # [B, 4W]
            # denominator = torch.sum(denominator, dim=1)  # [B, H, W]
            if self.args.use_uncertainty:
                denominator = torch.sum(denominator, dim=1) * TF.center_crop(sat_uncer_list[level],
                                                                             [corr.shape[1], corr.shape[2]])[:, 0]
            else:
                denominator = torch.sum(denominator, dim=1)  # [B, H, W]
            denominator = torch.maximum(torch.sqrt(denominator), torch.ones_like(denominator) * 1e-6)
            corr = 2 - 2 * corr / denominator

            B, corr_H, corr_W = corr.shape

            corr_maps.append(corr)

            max_index = torch.argmin(corr.reshape(B, -1), dim=1)
            pred_u = (max_index % corr_W - corr_W / 2) * meter_per_pixel  # / self.args.shift_range_lon
            pred_v = -(max_index // corr_W - corr_H / 2) * meter_per_pixel  # / self.args.shift_range_lat

            cos = torch.cos(gt_heading[:, 0] * self.args.rotation_range / 180 * np.pi)
            sin = torch.sin(gt_heading[:, 0] * self.args.rotation_range / 180 * np.pi)

            pred_u1 = pred_u * cos + pred_v * sin
            pred_v1 = - pred_u * sin + pred_v * cos

        if mode == 'train':
            return self.triplet_loss(corr_maps, gt_shift_u, gt_shift_v, gt_heading)
        else:
            return pred_u1, pred_v1  # [B], [B]

    def rot_corr(self, sat_map, grd_img_left, left_camera_k, gt_shift_u=None, gt_shift_v=None, gt_heading=None,
                 mode='train'):
        '''
        Args:
            sat_map: [B, C, A, A] A--> sidelength
            left_camera_k: [B, 3, 3]
            grd_img_left: [B, C, H, W]
            gt_shift_u: [B, 1] u->longitudinal
            gt_shift_v: [B, 1] v->lateral
            gt_heading: [B, 1] east as 0-degree
            mode:
            file_name:

        Returns:

        '''
        B, _, ori_grdH, ori_grdW = grd_img_left.shape

        sat_feat_list, sat_conf_list = self.SatFeatureNet(sat_map)
        if self.args.use_uncertainty:
            sat_uncer_list = self.uncertain_net(sat_feat_list)

        grd_feat_list, grd_conf_list = self.GrdFeatureNet(grd_img_left)

        shift_u = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        shift_v = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        heading = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)

        pred_feat_dict = {}
        shift_us_all = []
        shift_vs_all = []
        headings_all = []
        for iter in range(self.N_iters):
            shift_us = []
            shift_vs = []
            headings = []
            for level in range(len(sat_feat_list)):
                sat_feat = sat_feat_list[level]
                sat_conf = sat_conf_list[level]
                grd_feat = grd_feat_list[level]
                grd_conf = grd_conf_list[level]

                A = sat_feat.shape[-1]
                grd_feat_proj, _, u, mask = self.project_grd_to_map(
                    grd_feat, None, shift_u, shift_v, heading, left_camera_k, A, ori_grdH, ori_grdW)

                shift_u_new, shift_v_new, heading_new = self.Trans_update(
                    shift_u, shift_v, heading, grd_feat_proj, sat_feat)

                shift_us.append(shift_u_new[:, 0])  # [B]
                shift_vs.append(shift_v_new[:, 0])  # [B]
                headings.append(heading_new[:, 0])

                shift_u = shift_u_new.clone()
                shift_v = shift_v_new.clone()
                heading = heading_new.clone()

                if level not in pred_feat_dict.keys():
                    pred_feat_dict[level] = [grd_feat_proj]
                else:
                    pred_feat_dict[level].append(grd_feat_proj)

            shift_us_all.append(torch.stack(shift_us, dim=1))  # [B, Level]
            shift_vs_all.append(torch.stack(shift_vs, dim=1))  # [B, Level]
            headings_all.append(torch.stack(headings, dim=1))  # [B, Level]

        shift_lats = torch.stack(shift_vs_all, dim=1)  # [B, N_iters, Level]
        shift_lons = torch.stack(shift_us_all, dim=1)  # [B, N_iters, Level]
        thetas = torch.stack(headings_all, dim=1)  # [B, N_iters, Level]

        def corr(sat_feat_list, grd_feat_list, left_camera_k, gt_shift_u=None, gt_shift_v=None, gt_heading=None,
                 pred_heading=None, mode='train'):
            '''
        Args:
            sat_map: [B, C, A, A] A--> sidelength
            left_camera_k: [B, 3, 3]
            grd_img_left: [B, C, H, W]
            gt_shift_u: [B, 1] u->longitudinal
            gt_shift_v: [B, 1] v->lateral
            gt_heading: [B, 1] east as 0-degree
            mode:
            file_name:

        Returns:

        '''

            B, _, ori_grdH, ori_grdW = grd_img_left.shape

            shift_u = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
            shift_v = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
            # heading = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)

            corr_maps = []

            for level in range(len(sat_feat_list)):
                meter_per_pixel = self.meters_per_pixel[level]

                sat_feat = sat_feat_list[level]
                grd_feat = grd_feat_list[level]

                A = sat_feat.shape[-1]
                if mode == 'train':
                    # if epoch == 0:
                    #     heading = gt_heading + np.random.uniform(-0.1, 0.1)
                    # else:
                    #     heading = gt_heading + np.random.uniform(-0.05, 0.05)

                    heading = gt_heading
                else:
                    heading = pred_heading
                grd_feat_proj, _, _, mask = self.project_grd_to_map(
                    grd_feat, None, shift_u, shift_v, heading, left_camera_k, A, ori_grdH, ori_grdW)

                crop_H = int(A - self.args.shift_range_lat * 3 / meter_per_pixel)
                crop_W = int(A - self.args.shift_range_lon * 3 / meter_per_pixel)
                g2s_feat = TF.center_crop(grd_feat_proj, [crop_H, crop_W])
                g2s_feat = F.normalize(g2s_feat.reshape(B, -1)).reshape(B, -1, crop_H, crop_W)

                s_feat = sat_feat.reshape(1, -1, A, A)  # [B, C, H, W]->[1, B*C, H, W]
                corr = F.conv2d(s_feat, g2s_feat, groups=B)[0]  # [B, H, W]

                denominator = F.avg_pool2d(sat_feat.pow(2), (crop_H, crop_W), stride=1, divisor_override=1)  # [B, 4W]
                if self.args.use_uncertainty:
                    denominator = torch.sum(denominator, dim=1) * TF.center_crop(sat_uncer_list[level], [corr.shape[1], corr.shape[2]])[:, 0]
                else:
                    denominator = torch.sum(denominator, dim=1)  # [B, H, W]
                denominator = torch.maximum(torch.sqrt(denominator), torch.ones_like(denominator) * 1e-6)
                corr = 2 - 2 * corr / denominator

                B, corr_H, corr_W = corr.shape

                corr_maps.append(corr)

                max_index = torch.argmin(corr.reshape(B, -1), dim=1)
                pred_u = (max_index % corr_W - corr_W / 2) * meter_per_pixel  # / self.args.shift_range_lon
                pred_v = -(max_index // corr_W - corr_H / 2) * meter_per_pixel  # / self.args.shift_range_lat

                cos = torch.cos(gt_heading[:, 0] * self.args.rotation_range / 180 * np.pi)
                sin = torch.sin(gt_heading[:, 0] * self.args.rotation_range / 180 * np.pi)

                pred_u1 = pred_u * cos + pred_v * sin
                pred_v1 = - pred_u * sin + pred_v * cos

            if mode == 'train':
                return self.triplet_loss(corr_maps, gt_shift_u, gt_shift_v, gt_heading)
            else:
                return pred_u1, pred_v1  # [B], [B]

        if mode == 'train':

            loss, loss_decrease, shift_lat_decrease, shift_lon_decrease, thetas_decrease, loss_last, \
            shift_lat_last, shift_lon_last, theta_last, \
                = loss_func(shift_lats, shift_lons, thetas, gt_shift_v[:, 0], gt_shift_u[:, 0], gt_heading[:, 0],
                            torch.exp(-self.coe_R), torch.exp(-self.coe_R), torch.exp(-self.coe_R))

            trans_loss = corr(sat_feat_list, grd_feat_list, left_camera_k, gt_shift_u, gt_shift_v, gt_heading,
                              thetas[:, -1, -1:], mode)

            return loss, loss_decrease, shift_lat_decrease, shift_lon_decrease, thetas_decrease, loss_last, \
                   shift_lat_last, shift_lon_last, theta_last, \
                   grd_conf_list, trans_loss
        else:
            pred_u, pred_v = corr(sat_feat_list, grd_feat_list, left_camera_k, gt_shift_u, gt_shift_v, gt_heading,
                                  thetas[:, -1, -1:], mode)
            pred_orien = thetas[:, -1, -1]

            return pred_u, pred_v, pred_orien * self.args.rotation_range

    def CVattn_rot_corr(self, sat_map, grd_img_left, left_camera_k, gt_shift_u=None, gt_shift_v=None, gt_heading=None,
                        mode='train'):
        '''
        Args:
            sat_map: [B, C, A, A] A--> sidelength
            left_camera_k: [B, 3, 3]
            grd_img_left: [B, C, H, W]
            gt_shift_u: [B, 1] u->longitudinal
            gt_shift_v: [B, 1] v->lateral
            gt_heading: [B, 1] east as 0-degree
            mode:
            file_name:

        Returns:

        '''

        B, _, ori_grdH, ori_grdW = grd_img_left.shape

        sat_feat_list, sat_conf_list = self.SatFeatureNet(sat_map)
        if self.args.use_uncertainty:
            sat_uncer_list = self.uncertain_net(sat_feat_list)
        sat8, sat4, sat2 = sat_feat_list

        grd8, grd4, grd2 = self.GrdEnc(grd_img_left)
        # [H/8, W/8] [H/4, W/4] [H/2, W/2]
        grd_feat_list = self.GrdDec(grd8, grd4, grd2)

        # Generate multiscale grd2sat features
        shift_u = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        shift_v = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        heading = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)

        shift_us_all = []
        shift_vs_all = []
        headings_all = []
        for iter in range(self.N_iters):
            shift_us = []
            shift_vs = []
            headings = []
            for level in range(len(sat_feat_list)):
                sat_feat = sat_feat_list[level]
                grd_feat = grd_feat_list[level]

                if self.args.Optimizer == 'TransV1G2SP':
                    A = sat_feat.shape[-1]
                    grd_feat_proj, _, _, _ = self.project_grd_to_map(
                        grd_feat, None, shift_u, shift_v, heading, left_camera_k, A, ori_grdH, ori_grdW)

                    shift_u_new, shift_v_new, heading_new = self.Trans_update(
                        shift_u, shift_v, heading, grd_feat_proj, sat_feat)
                elif self.args.Optimizer == 'TransV1S2GP':
                    grd_H, grd_W = grd_feat.shape[-2:]
                    sat_feat_proj, _, sat_uv, mask = self.project_map_to_grd(
                        sat_feat, None, shift_u, shift_v, heading, level)
                    # [B, C, H, W], [B, 1, H, W], [3, B, C, H, W], [B, H, W, 2]

                    grd_feat = grd_feat * mask[:, None, :, :]

                    shift_u_new, shift_v_new, heading_new = self.Trans_update(shift_u, shift_v, heading,
                                                                               sat_feat_proj[:, :, grd_H // 2:, :],
                                                                               grd_feat[:, :, grd_H // 2:, :],
                                                                               )  # only need to compare bottom half

                shift_us.append(shift_u_new[:, 0])  # [B]
                shift_vs.append(shift_v_new[:, 0])  # [B]
                headings.append(heading_new[:, 0])

                shift_u = shift_u_new.clone()
                shift_v = shift_v_new.clone()
                heading = heading_new.clone()

            shift_us_all.append(torch.stack(shift_us, dim=1))  # [B, Level]
            shift_vs_all.append(torch.stack(shift_vs, dim=1))  # [B, Level]
            headings_all.append(torch.stack(headings, dim=1))  # [B, Level]

        shift_lats = torch.stack(shift_vs_all, dim=1)  # [B, N_iters, Level]
        shift_lons = torch.stack(shift_us_all, dim=1)  # [B, N_iters, Level]
        thetas = torch.stack(headings_all, dim=1)  # [B, N_iters, Level]


        def corr(sat_feat_list, grd_feat_list, left_camera_k, gt_shift_u=None, gt_shift_v=None, gt_heading=None,
                 pred_heading=None, mode='train'):
            '''
            Args:
                sat_map: [B, C, A, A] A--> sidelength
                left_camera_k: [B, 3, 3]
                grd_img_left: [B, C, H, W]
                gt_shift_u: [B, 1] u->longitudinal
                gt_shift_v: [B, 1] v->lateral
                gt_heading: [B, 1] east as 0-degree
                mode:
                file_name:

            Returns:

            '''

            B, _, ori_grdH, ori_grdW = grd_img_left.shape

            corr_maps = []

            for level in range(len(sat_feat_list)):
                meter_per_pixel = self.meters_per_pixel[level]

                sat_feat = sat_feat_list[level]
                grd_feat = grd_feat_list[level]

                A = sat_feat.shape[-1]

                # crop_grd = int(np.ceil(20/meter_per_pixel))
                # crop_sat = crop_grd + int(np.ceil(self.args.shift_range_lat * 3 / meter_per_pixel))

                # crop_H = int(A - self.args.shift_range_lat * 3 / meter_per_pixel)
                # crop_W = int(A - self.args.shift_range_lon * 3 / meter_per_pixel)
                crop_grd = int(A - self.args.shift_range_lat * 3 / meter_per_pixel)
                crop_sat = sat_feat.shape[-1]

                g2s_feat = TF.center_crop(grd_feat, [crop_grd, crop_grd])
                g2s_feat = F.normalize(g2s_feat.reshape(B, -1)).reshape(B, -1, crop_grd, crop_grd)

                # s_feat = sat_feat.reshape(1, -1, A, A)  # [B, C, H, W]->[1, B*C, H, W]
                s_feat = TF.center_crop(sat_feat, [crop_sat, crop_sat]).reshape(1, -1, crop_sat, crop_sat)
                corr = F.conv2d(s_feat, g2s_feat, groups=B)[0]  # [B, H, W]

                denominator = F.avg_pool2d(sat_feat.pow(2), (crop_grd, crop_grd), stride=1, divisor_override=1)  # [B, 4W]
                if self.args.use_uncertainty:
                    denominator = torch.sum(denominator, dim=1) * TF.center_crop(sat_uncer_list[level], [corr.shape[1], corr.shape[2]])[:, 0]
                else:
                    denominator = torch.sum(denominator, dim=1)  # [B, H, W]
                # denominator = torch.sum(denominator, dim=1)  # [B, H, W]
                denominator = torch.maximum(torch.sqrt(denominator), torch.ones_like(denominator) * 1e-6)
                corr = 2 - 2 * corr / denominator

                B, corr_H, corr_W = corr.shape

                corr_maps.append(corr)

                max_index = torch.argmin(corr.reshape(B, -1), dim=1)
                pred_u = (max_index % corr_W - corr_W / 2) * meter_per_pixel  # / self.args.shift_range_lon
                pred_v = -(max_index // corr_W - corr_H / 2) * meter_per_pixel  # / self.args.shift_range_lat

                cos = torch.cos(gt_heading[:, 0] * self.args.rotation_range / 180 * np.pi)
                sin = torch.sin(gt_heading[:, 0] * self.args.rotation_range / 180 * np.pi)

                pred_u1 = pred_u * cos + pred_v * sin
                pred_v1 = - pred_u * sin + pred_v * cos

            if mode == 'train':
                return self.triplet_loss(corr_maps, gt_shift_u, gt_shift_v, gt_heading)
            else:
                return pred_u1, pred_v1  # [B], [B]

        if mode == 'train':
            # Rotation Loss
            loss, loss_decrease, shift_lat_decrease, shift_lon_decrease, thetas_decrease, loss_last, \
            shift_lat_last, shift_lon_last, theta_last, \
                = loss_func(shift_lats, shift_lons, thetas, gt_shift_v[:, 0], gt_shift_u[:, 0], gt_heading[:, 0],
                            torch.exp(-self.coe_R), torch.exp(-self.coe_R), torch.exp(-self.coe_R))

            # Translation Loss
            shift_u = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
            shift_v = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
            heading = gt_heading

            grd2sat8, _, u, mask = self.project_grd_to_map(
                grd_feat_list[0], None, shift_u, shift_v, heading, left_camera_k, sat8.shape[-1], ori_grdH, ori_grdW)
            grd2sat4, _, _, _ = self.project_grd_to_map(
                grd_feat_list[1], None, shift_u, shift_v, heading, left_camera_k, sat4.shape[-1], ori_grdH, ori_grdW)
            grd2sat2, _, _, _ = self.project_grd_to_map(
                grd_feat_list[2], None, shift_u, shift_v, heading, left_camera_k, sat2.shape[-1], ori_grdH, ori_grdW)

            grd2sat8_attn = self.CVattn(grd2sat8, grd8, u, mask)
            grd2sat4_attn = grd2sat4 + self.Dec4(grd2sat8_attn, grd2sat4)
            grd2sat2_attn = grd2sat2 + self.Dec2(grd2sat4_attn, grd2sat2)

            grd_feat_list = [grd2sat8_attn, grd2sat4_attn, grd2sat2_attn]

            trans_loss = corr(sat_feat_list, grd_feat_list, left_camera_k, gt_shift_u, gt_shift_v, gt_heading,
                              thetas[:, -1, -1:], mode)

            return loss, loss_decrease, shift_lat_decrease, shift_lon_decrease, thetas_decrease, loss_last, \
                   shift_lat_last, shift_lon_last, theta_last, \
                   trans_loss

        else:
            pred_orien = thetas[:, -1, -1:]

            shift_u = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
            shift_v = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
            # Translation
            grd2sat8, _, u, mask = self.project_grd_to_map(
                grd_feat_list[0], None, shift_u, shift_v, pred_orien, left_camera_k, sat8.shape[-1], ori_grdH, ori_grdW)
            grd2sat4, _, _, _ = self.project_grd_to_map(
                grd_feat_list[1], None, shift_u, shift_v, pred_orien, left_camera_k, sat4.shape[-1], ori_grdH, ori_grdW)
            grd2sat2, _, _, _ = self.project_grd_to_map(
                grd_feat_list[2], None, shift_u, shift_v, pred_orien, left_camera_k, sat2.shape[-1], ori_grdH, ori_grdW)

            grd2sat8_attn = self.CVattn(grd2sat8, grd8, u, mask)
            grd2sat4_attn = grd2sat4 + self.Dec4(grd2sat8_attn, grd2sat4)
            grd2sat2_attn = grd2sat2 + self.Dec2(grd2sat4_attn, grd2sat2)

            grd_feat_list = [grd2sat8_attn, grd2sat4_attn, grd2sat2_attn]
            pred_u, pred_v = corr(sat_feat_list, grd_feat_list, left_camera_k, gt_shift_u, gt_shift_v, gt_heading,
                                  pred_orien, mode)

            return pred_u, pred_v, pred_orien[:, 0] * self.args.rotation_range

================
File: train_kitti_3DoF.py
================
import os

# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
# os.environ['CUDA_VISIBLE_DEVICES'] = '0'

import torch
import torch.optim as optim
from dataLoader.KITTI_dataset import load_train_data, load_test1_data, load_test2_data
from models_kitti import Model
import scipy.io as scio

import ssl

ssl._create_default_https_context = ssl._create_unverified_context  # for downloading pretrained VGG weights

import numpy as np
import os
import argparse

import time


def test1(net_test, args, save_path, epoch):
    net_test.eval()

    dataloader = load_test1_data(mini_batch, args.shift_range_lat, args.shift_range_lon, args.rotation_range)
    
    pred_lons = []
    pred_lats = []
    pred_oriens = []
    
    gt_lons = []
    gt_lats = []
    gt_oriens = []

    start_time = time.time()
    with torch.no_grad():
        for i, data in enumerate(dataloader, 0):

            sat_map, left_camera_k, grd_left_imgs, gt_shift_u, gt_shift_v, gt_heading = [item.to(device) for item in data[:-1]]

            if args.proj == 'CrossAttn':
                pred_u, pred_v, pred_orien = net_test.CVattn_rot_corr(sat_map, grd_left_imgs, left_camera_k, gt_heading=gt_heading, mode='test')
            else:
                pred_u, pred_v, pred_orien = net_test.rot_corr(sat_map, grd_left_imgs, left_camera_k, gt_heading=gt_heading, mode='test')

            pred_lons.append(pred_u.data.cpu().numpy())
            pred_lats.append(pred_v.data.cpu().numpy())
            pred_oriens.append(pred_orien.data.cpu().numpy())

            gt_lons.append(gt_shift_u[:, 0].data.cpu().numpy() * args.shift_range_lon)
            gt_lats.append(gt_shift_v[:, 0].data.cpu().numpy() * args.shift_range_lat)
            gt_oriens.append(gt_heading[:, 0].data.cpu().numpy() * args.rotation_range)

            # import pdb; pdb.set_trace()

            if i % 20 == 0:
                print(i)
    end_time = time.time()
    duration = (end_time - start_time) / len(dataloader) / mini_batch

    pred_lons = np.concatenate(pred_lons, axis=0)
    pred_lats = np.concatenate(pred_lats, axis=0)
    pred_oriens = np.concatenate(pred_oriens, axis=0)
    
    gt_lons = np.concatenate(gt_lons, axis=0)
    gt_lats = np.concatenate(gt_lats, axis=0)
    gt_oriens = np.concatenate(gt_oriens, axis=0)

    scio.savemat(os.path.join(save_path, 'test1_result.mat'), {'gt_lons': gt_lons, 'gt_lats': gt_lats, 'gt_oriens': gt_oriens,
                                                         'pred_lats': pred_lats, 'pred_lons': pred_lons, 'pred_oriens': pred_oriens})

    distance = np.sqrt((pred_lons - gt_lons) ** 2 + (pred_lats - gt_lats) ** 2)  # [N]

    init_dis = np.sqrt(gt_lats ** 2 + gt_lons ** 2)
    
    diff_lats = np.abs(pred_lats - gt_lats)
    diff_lons = np.abs(pred_lons - gt_lons)
   
    angle_diff = np.remainder(np.abs(pred_oriens - gt_oriens), 360)
    idx0 = angle_diff > 180
    angle_diff[idx0] = 360 - angle_diff[idx0]
  
    init_angle = np.abs(gt_oriens)

    metrics = [1, 3, 5]
    angles = [1, 3, 5]

    f = open(os.path.join(save_path, 'results.txt'), 'a')
    f.write('====================================\n')
    f.write('       EPOCH: ' + str(epoch) + '\n')
    print('====================================')
    print('       EPOCH: ' + str(epoch))
    print('Time per image (second): ' + str(duration) + '\n')
    print('Test1 results:')
    
    print('Distance average: (init, pred)', np.mean(init_dis), np.mean(distance))
    print('Distance median: (init, pred)', np.median(init_dis), np.median(distance))

    print('Lateral average: (init, pred)', np.mean(np.abs(gt_lats)), np.mean(diff_lats))
    print('Lateral median: (init, pred)', np.median(np.abs(gt_lats)), np.median(diff_lats))

    print('Longitudinal average: (init, pred)', np.mean(np.abs(gt_lons)), np.mean(diff_lons))
    print('Longitudinal median: (init, pred)', np.median(np.abs(gt_lons)), np.median(diff_lons))

    print('Angle average (init, pred): ', np.mean(np.abs(gt_oriens)), np.mean(angle_diff))
    print('Angle median (init, pred): ', np.median(np.abs(gt_oriens)), np.median(angle_diff))

    for idx in range(len(metrics)):
        pred = np.sum(distance < metrics[idx]) / distance.shape[0] * 100
        init = np.sum(init_dis < metrics[idx]) / init_dis.shape[0] * 100

        line = 'distance within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    print('-------------------------')
    f.write('------------------------\n')

    for idx in range(len(metrics)):
        pred = np.sum(diff_lats < metrics[idx]) / diff_lats.shape[0] * 100
        init = np.sum(np.abs(gt_lats) < metrics[idx]) / gt_lats.shape[0] * 100

        line = 'lateral within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    for idx in range(len(metrics)):
        pred = np.sum(diff_lons < metrics[idx]) / diff_lons.shape[0] * 100
        init = np.sum(np.abs(gt_lons) < metrics[idx]) / gt_lons.shape[0] * 100

        line = 'longitudinal within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    for idx in range(len(angles)):
        pred = np.sum(angle_diff < angles[idx]) / angle_diff.shape[0] * 100
        init = np.sum(init_angle < angles[idx]) / angle_diff.shape[0] * 100
        line = 'angle within ' + str(angles[idx]) + ' degrees (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    print('====================================')
    f.write('====================================\n')
    f.close()

    net_test.train()
    return


def test2(net_test, args, save_path, epoch):
    ### net evaluation state
    net_test.eval()

    dataloader = load_test2_data(mini_batch, args.shift_range_lat, args.shift_range_lon, args.rotation_range)
    
    pred_lons = []
    pred_lats = []
    pred_oriens = []
    
    gt_lons = []
    gt_lats = []
    gt_oriens = []
    with torch.no_grad():
        for i, data in enumerate(dataloader, 0):

            sat_map, left_camera_k, grd_left_imgs, gt_shift_u, gt_shift_v, gt_heading = [item.to(device) for item in data[:-1]]

            if args.proj == 'CrossAttn':
                pred_u, pred_v, pred_orien = net_test.CVattn_rot_corr(sat_map, grd_left_imgs, left_camera_k, gt_heading=gt_heading, mode='test')
            else:
                pred_u, pred_v, pred_orien = net_test.rot_corr(sat_map, grd_left_imgs, left_camera_k, gt_heading=gt_heading, mode='test')

            pred_lons.append(pred_u.data.cpu().numpy())
            pred_lats.append(pred_v.data.cpu().numpy())
            pred_oriens.append(pred_orien.data.cpu().numpy())

            gt_lons.append(gt_shift_u[:, 0].data.cpu().numpy() * args.shift_range_lon)
            gt_lats.append(gt_shift_v[:, 0].data.cpu().numpy() * args.shift_range_lat)
            gt_oriens.append(gt_heading[:, 0].data.cpu().numpy() * args.rotation_range)

            if i % 20 == 0:
                print(i)

    pred_lons = np.concatenate(pred_lons, axis=0)
    pred_lats = np.concatenate(pred_lats, axis=0)
    pred_oriens = np.concatenate(pred_oriens, axis=0)
    
    gt_lons = np.concatenate(gt_lons, axis=0)
    gt_lats = np.concatenate(gt_lats, axis=0)
    gt_oriens = np.concatenate(gt_oriens, axis=0)

    scio.savemat(os.path.join(save_path, 'test2_result.mat'), {'gt_lons': gt_lons, 'gt_lats': gt_lats, 'gt_oriens': gt_oriens,
                                                         'pred_lats': pred_lats, 'pred_lons': pred_lons, 'pred_oriens': pred_oriens})

    distance = np.sqrt((pred_lons - gt_lons) ** 2 + (pred_lats - gt_lats) ** 2)  # [N]

    init_dis = np.sqrt(gt_lats ** 2 + gt_lons ** 2)
    
    diff_lats = np.abs(pred_lats - gt_lats)
    diff_lons = np.abs(pred_lons - gt_lons)
   
    angle_diff = np.remainder(np.abs(pred_oriens - gt_oriens), 360)
    idx0 = angle_diff > 180
    angle_diff[idx0] = 360 - angle_diff[idx0]
  
    init_angle = np.abs(gt_oriens)

    metrics = [1, 3, 5]
    angles = [1, 3, 5]

    f = open(os.path.join(save_path, 'results.txt'), 'a')
    f.write('====================================\n')
    f.write('       EPOCH: ' + str(epoch) + '\n')
    print('====================================')
    print('       EPOCH: ' + str(epoch))
    print('Test2 results:')
    
    print('Distance average: (init, pred)', np.mean(init_dis), np.mean(distance))
    print('Distance median: (init, pred)', np.median(init_dis), np.median(distance))

    print('Lateral average: (init, pred)', np.mean(np.abs(gt_lats)), np.mean(diff_lats))
    print('Lateral median: (init, pred)', np.median(np.abs(gt_lats)), np.median(diff_lats))

    print('Longitudinal average: (init, pred)', np.mean(np.abs(gt_lons)), np.mean(diff_lons))
    print('Longitudinal median: (init, pred)', np.median(np.abs(gt_lons)), np.median(diff_lons))

    print('Angle average (init, pred): ', np.mean(np.abs(gt_oriens)), np.mean(angle_diff))
    print('Angle median (init, pred): ', np.median(np.abs(gt_oriens)), np.median(angle_diff))

    for idx in range(len(metrics)):
        pred = np.sum(distance < metrics[idx]) / distance.shape[0] * 100
        init = np.sum(init_dis < metrics[idx]) / init_dis.shape[0] * 100

        line = 'distance within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    print('-------------------------')
    f.write('------------------------\n')

    
    for idx in range(len(metrics)):
        pred = np.sum(diff_lats < metrics[idx]) / diff_lats.shape[0] * 100
        init = np.sum(np.abs(gt_lats) < metrics[idx]) / gt_lats.shape[0] * 100

        line = 'lateral within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    for idx in range(len(metrics)):

        pred = np.sum(diff_lons < metrics[idx]) / diff_lons.shape[0] * 100
        init = np.sum(np.abs(gt_lons) < metrics[idx]) / gt_lons.shape[0] * 100

        line = 'longitudinal within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    print('-------------------------')
    # f.write('------------------------\n')
    
    for idx in range(len(angles)):
        pred = np.sum(angle_diff < angles[idx]) / angle_diff.shape[0] * 100
        init = np.sum(init_angle < angles[idx]) / angle_diff.shape[0] * 100
        line = 'angle within ' + str(angles[idx]) + ' degrees (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    print('====================================')
    f.write('====================================\n')
    f.close()
    result = np.sum((diff_lats < metrics[0])) / diff_lats.shape[0] * 100

    net_test.train()


    return result


def train(net, lr, args, save_path):

    for epoch in range(args.resume, args.epochs):
        net.train()

        base_lr = lr

        if epoch > 0:
            base_lr = 1e-5

        optimizer = optim.Adam(net.parameters(), lr=base_lr)
        optimizer.zero_grad()

        trainloader = load_train_data(mini_batch, args.shift_range_lat, args.shift_range_lon, args.rotation_range)

        loss_vec = []

        print('batch_size:', mini_batch, '\n num of batches:', len(trainloader))

        for Loop, Data in enumerate(trainloader, 0):
            # get the inputs

            sat_map, left_camera_k, grd_left_imgs, gt_shift_u, gt_shift_v, gt_heading = [item.to(device) for item in Data[:-1]]
            file_name = Data[-1]

            if args.proj == 'CrossAttn':
                opt_loss, loss_decrease, shift_lat_decrease, shift_lon_decrease, thetas_decrease, loss_last, \
                shift_lat_last, shift_lon_last, theta_last, \
                corr_loss = net.CVattn_rot_corr(sat_map, grd_left_imgs, left_camera_k, gt_shift_u, gt_shift_v, gt_heading, mode='train')
            else:
                opt_loss, loss_decrease, shift_lat_decrease, shift_lon_decrease, thetas_decrease, loss_last, \
                shift_lat_last, shift_lon_last, theta_last, \
                grd_conf_list, corr_loss = \
                    net.rot_corr(sat_map, grd_left_imgs, left_camera_k, gt_shift_u, gt_shift_v, gt_heading, mode='train')

            loss = opt_loss + corr_loss * torch.exp(-net.coe_T) + net.coe_T + net.coe_R

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()  # This step is responsible for updating weights

            loss_vec.append(loss.item())

            if Loop % 10 == 9:  #

                level = 2
                print('Epoch: ' + str(epoch) + ' Loop: ' + str(Loop) + ' Delta: Level-' + str(level) +
                      ' loss: ' + str(np.round(loss_decrease[level].item(), decimals=4)) +
                      ' lat: ' + str(np.round(shift_lat_decrease[level].item(), decimals=2)) +
                      ' lon: ' + str(np.round(shift_lon_decrease[level].item(), decimals=2)) +
                      ' rot: ' + str(np.round(thetas_decrease[level].item(), decimals=2)))

                print('Epoch: ' + str(epoch) + ' Loop: ' + str(Loop) + ' Last : Level-' + str(level) +
                        ' loss: ' + str(np.round(loss_last[level].item(), decimals=4)) +
                        ' lat: ' + str(np.round(shift_lat_last[level].item(), decimals=2)) +
                        ' lon: ' + str(np.round(shift_lon_last[level].item(), decimals=2)) +
                        ' rot: ' + str(np.round(theta_last[level].item(), decimals=2))
                        )

                print('Epoch: ' + str(epoch) + ' Loop: ' + str(Loop) +
                        ' triplet loss: ' + str(np.round(corr_loss.item(), decimals=4)) +
                        ' coe_R: ' + str(np.round(net.coe_R.item(), decimals=2)) +
                        ' coe_T: ' + str(np.round(net.coe_T.item(), decimals=2))
                        )

        print('Save Model ...')

        torch.save(net.state_dict(), os.path.join(save_path, 'model_' + str(epoch) + '.pth'))

        test1(net, args, save_path, epoch)
        test2(net, args, save_path, epoch)

    print('Finished Training')


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--resume', type=int, default=0, help='resume the trained model')
    parser.add_argument('--test', type=int, default=0, help='test with trained model')

    parser.add_argument('--epochs', type=int, default=5, help='number of training epochs')

    parser.add_argument('--lr', type=float, default=1e-4, help='learning rate')  # 1e-2

    parser.add_argument('--rotation_range', type=float, default=10., help='degree')
    parser.add_argument('--shift_range_lat', type=float, default=20., help='meters')
    parser.add_argument('--shift_range_lon', type=float, default=20., help='meters')

    parser.add_argument('--batch_size', type=int, default=3, help='batch size')

    parser.add_argument('--level', type=int, default=3, help='2, 3, 4, -1, -2, -3, -4')
    parser.add_argument('--N_iters', type=int, default=2, help='any integer')

    parser.add_argument('--Optimizer', type=str, default='TransV1G2SP', help='')

    parser.add_argument('--proj', type=str, default='CrossAttn', help='geo, CrossAttn')

    parser.add_argument('--use_uncertainty', type=int, default=1, help='0 or 1')

    args = parser.parse_args()

    return args



def getSavePath(args):
    save_path = './ModelsKitti/3DoF/'\
                + 'lat' + str(args.shift_range_lat) + 'm_lon' + str(args.shift_range_lon) + 'm_rot' + str(
        args.rotation_range) \
                + '_Nit' + str(args.N_iters) + '_' + str(args.Optimizer) + '_' + str(args.proj)

    if args.use_uncertainty:
        save_path = save_path + '_Uncertainty'

    if not os.path.exists(save_path):
        os.makedirs(save_path)

    print('save_path:', save_path)

    return save_path


if __name__ == '__main__':

    if torch.cuda.is_available():
        device = torch.device("cuda:0")
    else:
        device = torch.device("cpu")

    np.random.seed(2022)

    args = parse_args()

    mini_batch = args.batch_size

    save_path = getSavePath(args)
    net = Model(args)
    net.to(device)

    if args.test:
        net.load_state_dict(torch.load(os.path.join(save_path, 'model_4.pth')), strict=False)

        test1(net, args, save_path, epoch=0)
        test2(net, args, save_path, epoch=0)

    else:
        if args.resume:
            net.load_state_dict(torch.load(os.path.join(save_path, 'model_' + str(args.resume - 1) + '.pth')))
            print("resume from " + 'model_' + str(args.resume - 1) + '.pth')

        lr = args.lr
        train(net, lr, args, save_path)

================
File: README.md
================
# Boosting 3-DoF Ground-to-Satellite Camera Localization Accuracy via Geometry-Guided Cross-View Transformer, ICCV 2023

![Framework](./Framework.png)

# Abstract
Image retrieval-based cross-view localization methods often lead to very coarse camera pose estimation, due to the limited sampling density of the database satellite images. In this paper, we propose a method to increase the accuracy of a ground camera's location and orientation by estimating the relative rotation and translation between the ground-level image and its matched/retrieved satellite image.
Our approach designs a geometry-guided cross-view transformer that combines the benefits of conventional geometry and learnable cross-view transformers to map the ground-view observations to an overhead view. 
Given the synthesized overhead view and observed satellite feature maps, we construct a neural pose optimizer with strong global information embedding ability to estimate the relative rotation between them. After aligning their rotations, we develop an uncertainty-guided spatial correlation to generate a probability map of the vehicle locations, from which the relative translation can be determined.
Experimental results demonstrate that our method significantly outperforms the state-of-the-art. Notably, the likelihood of restricting the vehicle lateral pose to be within 1m of its Ground Truth (GT) value on the cross-view KITTI dataset has been improved from $35.54\%$ to $76.44\%$, and the likelihood of restricting the vehicle orientation to be within $1^{\circ}$ of its GT value has been improved from $19.64\%$ to $99.10\%$.

### Experiment Dataset
We use three existing dataset to do the experiments: KITTI, Ford-AV and Oxford RobotCar. For our collected satellite images for KITTI and Ford-AV, please first fill this [Google Form](https://forms.gle/Bm8jNLiUxFeQejix7), we will then send you the link for download. 

- KITTI: Please first download the raw data (ground images) from http://www.cvlibs.net/datasets/kitti/raw_data.php, and store them according to different date (not category). 
Your dataset folder structure should be like: 

KITTI:

  raw_data:
  
    2011_09_26:
    
      2011_09_26_drive_0001_sync:
      
        image_00:
	
	image_01:
	
	image_02:
	
	image_03:
	
	oxts:
	
      ...
      
    2011_09_28:
    
    2011_09_29:
    
    2011_09_30:
    
    2011_10_03:
  
  satmap:
  
    2011_09_26:
    
    2011_09_29:
    
    2011_09_30:
    
    2011_10_03:

- Ford-AV: The ground images and camera calibration files can be accessed from https://avdata.ford.com/downloads/default.aspx. Please follow their original structure to save them on your computer. For the satellite images, please put them under their corresponding log folder. Here is an example:


Ford:

  2017-08-04:
  
    V2:
    
      Log1:
      
        2017-08-04-V2-Log1-FL
	
        SatelliteMaps_18:
	
        grd_sat_quaternion_latlon.txt
	
        grd_sat_quaternion_latlon_test.txt

  2017-10-26:
  
  Calibration-V2:


- For the Cross-view Oxford RobotCar dataset, please refer to this github page: https://github.com/tudelft-iv/CrossViewMetricLocalization.git.
- For the VIGOR dataset, please refer to the following two github pages:
  https://github.com/Jeff-Zilence/VIGOR.git
  https://github.com/tudelft-iv/SliceMatch.git

### Codes

1. Training on 2DoF(only location) pose estimation:

    python train_kitti_2DoF.py --batch_size 1 


    python train_ford_2DoF.py --batch_size 1 --train_log_start 0 --train_log_end 1 
    
    python train_ford_2DoF.py --batch_size 1 --train_log_start 1 --train_log_end 2 
    
    python train_ford_2DoF.py --batch_size 1 --train_log_start 2 --train_log_end 3
    
    python train_ford_2DoF.py --batch_size 1 --train_log_start 3 --train_log_end 4 
    
    python train_ford_2DoF.py --batch_size 1 --train_log_start 4 --train_log_end 5 
    
    python train_ford_2DoF.py --batch_size 1 --train_log_start 5 --train_log_end 6
    
    
    python train_oxford_2DoF.py --batch_size 1

    python train_VIGOR_2DoF.py --area cross
   
    python train_VIGOR_2DoF.py --area same


3. Training on 3DoF (joint location and translation) pose estimation:

    python train_kitti_3DoF.py --batch_size 1 


    python train_ford_3DoF.py --batch_size 1 --train_log_start 0 --train_log_end 1 
    
    python train_ford_3DoF.py --batch_size 1 --train_log_start 1 --train_log_end 2 
    
    python train_ford_3DoF.py --batch_size 1 --train_log_start 2 --train_log_end 3
    
    python train_ford_3DoF.py --batch_size 1 --train_log_start 3 --train_log_end 4 
    
    python train_ford_3DoF.py --batch_size 1 --train_log_start 4 --train_log_end 5 
    
    python train_ford_3DoF.py --batch_size 1 --train_log_start 5 --train_log_end 6

2. Evaluation:

    Plz simply add "--test 1" after the training commands. E.g. 

    python train_kitti_3DoF.py --batch_size 1 --test 1


You are free to change batch size according to your own GPU memory. 

### Models:
Our trained models are available [here](https://anu365-my.sharepoint.com/:f:/g/personal/u6293587_anu_edu_au/Eofuoj1mCP1OqVEU9WC46BMBae0UK_pyFCh7qxNhPXEMtw?e=bPWf6K). 



### Publications
This work is submitted to ICCV 2023.

================
File: utils.py
================
import numpy as np
import torch

CameraGPS_shift = [1.08, 0.26]
Satmap_zoom = 18
Camera_height = 1.65 #meter
Camera_distance = 0.54 #meter

SatMap_original_sidelength = 512 # 0.2 m per pixel
SatMap_process_sidelength = 512 # 0.2 m per pixel
Default_lat = 49.015

CameraGPS_shift_left = [1.08, 0.26]
CameraGPS_shift_right = [1.08, 0.8]  # 0.26 + 0.54

EPS = 1e-7

def get_satmap_zoom():
    return Satmap_zoom

def get_camera_height():
    return Camera_height

def get_camera_distance():
    return Camera_distance

def get_original_satmap_sidelength():
    return SatMap_original_sidelength

def get_process_satmap_sidelength():
    return SatMap_process_sidelength

# x: east shift in meter, y: south shift in meter
# return lat and lon after shift
# Curvature formulas from https://en.wikipedia.org/wiki/Earth_radius#Meridional
def meter2latlon(lat, lon, x, y):
    r = 6378137 # equatorial radius
    flatten = 1/298257 # flattening
    E2 = flatten * (2- flatten)
    m = r * np.pi/180  
    coslat = np.cos(lat * np.pi/180)
    w2 = 1/(1-E2 *(1-coslat*coslat))
    w = np.sqrt(w2)
    kx = m * w * coslat
    ky = m * w * w2 * (1-E2)
    lon += x / kx 
    lat -= y / ky
    
    return lat, lon   

def gps2meters(lat_s, lon_s, lat_d, lon_d ):
    r = 6378137 # equatorial radius
    flatten = 1/298257 # flattening
    E2 = flatten * (2- flatten)
    m = r * np.pi/180  
    lat = (lat_s+lat_d)/2
    coslat = np.cos(lat * np.pi/180)
    w2 = 1/(1-E2 *(1-coslat*coslat))
    w = np.sqrt(w2)
    kx = m * w * coslat
    ky = m * w * w2 * (1-E2)
    x = (lon_d-lon_s)*kx
    y = (lat_s-lat_d)*ky # y: from top to bottom
    
    return [x,y]


def gps2utm(lat, lon, lat0=49.015):
    # from paper "Vision meets Robotics: The KITTI Dataset"

    r = 6378137.
    s = np.cos(lat0 * np.pi / 180)

    x = s * r * np.pi * lon / 180
    y = s * r * np.log(np.tan(np.pi * (90 + lat) / 360))

    return x, y

def gps2utm_torch(lat, lon, lat0=torch.tensor(49.015)):
    # from paper "Vision meets Robotics: The KITTI Dataset"

    r = 6378137.
    s = torch.cos(lat0 * np.pi / 180)

    x = s * r * np.pi * lon / 180
    y = s * r * torch.log(torch.tan(np.pi * (90 + lat) / 360))

    return x, y


def gps2meters_torch(lat_s, lon_s, lat_d=torch.tensor([49.015]), lon_d=torch.tensor([8.43])):
    # inputs: torch array: [n]
    r = 6378137 # equatorial radius
    flatten = 1/298257 # flattening
    E2 = flatten * (2- flatten)
    m = r * np.pi/180  
    lat = lat_d[0]
    coslat = np.cos(lat * np.pi/180)
    w2 = 1/(1-E2 *(1-coslat*coslat))
    w = np.sqrt(w2)
    kx = m * w * coslat
    ky = m * w * w2 * (1-E2)
    
    x = (lon_d-lon_s)*kx
    y = (lat_s-lat_d)*ky # y: from top to bottom
    
    return x,y


def gps2shiftmeters(latlon ):
    # torch array: [B,S,2]

    r = 6378137 # equatoristereoal radius
    flatten = 1/298257 # flattening
    E2 = flatten * (2- flatten)
    m = r * np.pi/180  
    lat = latlon[0,0,0]
    coslat = torch.cos(lat * np.pi/180)
    w2 = 1/(1-E2 *(1-coslat*coslat))
    w = torch.sqrt(w2)
    kx = m * w * coslat
    ky = m * w * w2 * (1-E2)

    shift_x = (latlon[:,:1,1]-latlon[:,:,1])*kx #B,S east
    shift_y = (latlon[:,:,0]-latlon[:,:1,0])*ky #B,S south
    shift = torch.cat([shift_x.unsqueeze(-1),shift_y.unsqueeze(-1)],dim=-1) #[B,S,2] #shift from 0
    
    # shift from privious
    S = latlon.size()[1]
    shift = shift[:,1:,:]-shift[:,:(S-1),:]
    
    return shift


def gps2distance(lat_s, lon_s, lat_d, lon_d ):
    x,y = gps2meters_torch(lat_s, lon_s, lat_d, lon_d )
    dis = torch.sqrt(torch.pow(x, 2)+torch.pow(y,2))
    return dis


def get_meter_per_pixel(lat=Default_lat, zoom=Satmap_zoom, scale=SatMap_process_sidelength/SatMap_original_sidelength):
    meter_per_pixel = 156543.03392 * np.cos(lat * np.pi/180.) / (2**zoom)	
    meter_per_pixel /= 2 # because use scale 2 to get satmap 
    meter_per_pixel /= scale
    return meter_per_pixel


def gps2shiftscale(latlon):
    # torch array: [B,S,2]
    
    shift = gps2shiftmeters(latlon)
    
    # turn meter to -1~1
    meter_per_pixel = get_meter_per_pixel(scale=1)
    win_range = meter_per_pixel*SatMap_original_sidelength
    shift /= win_range//2
    
    return shift

def get_camera_max_meter_shift():
    return np.linalg.norm(CameraGPS_shift)

def get_camera_gps_shift(heading):
    shift_x = CameraGPS_shift[0] * np.cos(heading%(2*np.pi)) + CameraGPS_shift[1] * np.sin(heading%(2*np.pi))
    shift_y = CameraGPS_shift[1] * np.cos(heading%(2*np.pi)) - CameraGPS_shift[0] * np.sin(heading%(2*np.pi))
    return shift_x, shift_y


def get_camera_gps_shift_left(heading):
    shift_x = CameraGPS_shift_left[0] * np.cos(heading%(2*np.pi)) + CameraGPS_shift_left[1] * np.sin(heading%(2*np.pi))
    shift_y = CameraGPS_shift_left[0] * np.sin(heading%(2*np.pi)) - CameraGPS_shift_left[1] * np.cos(heading%(2*np.pi))
    return shift_x, shift_y


def get_camera_gps_shift_right(heading):
    shift_x = CameraGPS_shift_right[0] * np.cos(heading%(2*np.pi)) + CameraGPS_shift_right[1] * np.sin(heading%(2*np.pi))
    shift_y = CameraGPS_shift_right[0] * np.sin(heading%(2*np.pi)) - CameraGPS_shift_right[1] * np.cos(heading%(2*np.pi))
    return shift_x, shift_y


def get_height_config():
    start = 0 #-15 -7 0
    end = 0 
    count = 1 #16 8 1
    return start, end, count

================
File: train_kitti_2DoF.py
================
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

import os

# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
# os.environ['CUDA_VISIBLE_DEVICES'] = '0'

import torch
import torch.optim as optim
from dataLoader.KITTI_dataset import load_train_data, load_test1_data, load_test2_data
import scipy.io as scio

import ssl

ssl._create_default_https_context = ssl._create_unverified_context  # for downloading pretrained VGG weights

from models_kitti import Model

import numpy as np
import os
import argparse

from utils import gps2distance


########################### ranking test ############################
def test1(net_test, args, save_path, epoch):
    ### net evaluation state
    net_test.eval()

    dataloader = load_test1_data(mini_batch, args.shift_range_lat, args.shift_range_lon, args.rotation_range)
    
    pred_lons = []
    pred_lats = []

    gt_lons = []
    gt_lats = []

    with torch.no_grad():
        for i, data in enumerate(dataloader, 0):
            sat_map, left_camera_k, grd_left_imgs, gt_shift_u, gt_shift_v, gt_heading = [item.to(device) for item in data[:-1]]

            if args.proj == 'CrossAttn':
                pred_u, pred_v = net_test.CVattn_corr(sat_map, grd_left_imgs, left_camera_k, gt_heading=gt_heading, mode='test')
            else:
                pred_u, pred_v = net_test.corr(sat_map, grd_left_imgs, left_camera_k, gt_heading=gt_heading, mode='test')

            pred_lons.append(pred_u.data.cpu().numpy())
            pred_lats.append(pred_v.data.cpu().numpy())

            gt_lons.append(gt_shift_u[:, 0].data.cpu().numpy() * args.shift_range_lon)
            gt_lats.append(gt_shift_v[:, 0].data.cpu().numpy() * args.shift_range_lat)

            if i % 20 == 0:
                print(i)

    pred_lons = np.concatenate(pred_lons, axis=0)
    pred_lats = np.concatenate(pred_lats, axis=0)

    gt_lons = np.concatenate(gt_lons, axis=0)
    gt_lats = np.concatenate(gt_lats, axis=0)
    scio.savemat(os.path.join(save_path, 'test1_result.mat'), {'gt_lons': gt_lons, 'gt_lats': gt_lats,
                                                         'pred_lats': pred_lats, 'pred_lons': pred_lons})

    distance = np.sqrt((pred_lons - gt_lons) ** 2 + (pred_lats - gt_lats) ** 2)  # [N]

    init_dis = np.sqrt(gt_lats ** 2 + gt_lons ** 2)
    
    diff_lats = np.abs(pred_lats - gt_lats)
    diff_lons = np.abs(pred_lons - gt_lons)

    metrics = [1, 3, 5]

    f = open(os.path.join(save_path, 'results.txt'), 'a')
    f.write('====================================\n')
    f.write('       EPOCH: ' + str(epoch) + '\n')
    print('====================================')
    print('       EPOCH: ' + str(epoch))
    print('Test1 results:')
    
    print('Distance average: (init, pred)', np.mean(init_dis), np.mean(distance))
    print('Distance median: (init, pred)', np.median(init_dis), np.median(distance))

    print('Lateral average: (init, pred)', np.mean(np.abs(gt_lats)), np.mean(diff_lats))
    print('Lateral median: (init, pred)', np.median(np.abs(gt_lats)), np.median(diff_lats))

    print('Longitudinal average: (init, pred)', np.mean(np.abs(gt_lons)), np.mean(diff_lons))
    print('Longitudinal median: (init, pred)', np.median(np.abs(gt_lons)), np.median(diff_lons))

    for idx in range(len(metrics)):
        pred = np.sum(distance < metrics[idx]) / distance.shape[0] * 100
        init = np.sum(init_dis < metrics[idx]) / init_dis.shape[0] * 100

        line = 'distance within ' + str(metrics[idx]) + ' meters (pred, init): ' + str(pred) + ' ' + str(init)
        print(line)
        f.write(line + '\n')

    print('-------------------------')
    f.write('------------------------\n')

    for idx in range(len(metrics)):
        pred = np.sum(diff_lats < metrics[idx]) / diff_lats.shape[0] * 100
        init = np.sum(np.abs(gt_lats) < metrics[idx]) / gt_lats.shape[0] * 100

        line = 'lateral      within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    for idx in range(len(metrics)):
        pred = np.sum(diff_lons < metrics[idx]) / diff_lons.shape[0] * 100
        init = np.sum(np.abs(gt_lons) < metrics[idx]) / gt_lons.shape[0] * 100

        line = 'longitudinal within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    print('====================================')
    f.write('====================================\n')
    f.close()

    net_test.train()

    return


def test2(net_test, args, save_path, epoch):
    ### net evaluation state
    net_test.eval()

    dataloader = load_test2_data(mini_batch, args.shift_range_lat, args.shift_range_lon, args.rotation_range)
    pred_lons = []
    pred_lats = []
    gt_lons = []
    gt_lats = []

    with torch.no_grad():
        for i, data in enumerate(dataloader, 0):
            sat_map, left_camera_k, grd_left_imgs, gt_shift_u, gt_shift_v, gt_heading = [item.to(device) for item
                                                                                                   in data[:-1]]
            if args.proj == 'CrossAttn':
                pred_u, pred_v = net_test.CVattn_corr(sat_map, grd_left_imgs, left_camera_k, gt_heading=gt_heading, mode='test')
            else:
                pred_u, pred_v = net_test.corr(sat_map, grd_left_imgs, left_camera_k, gt_heading=gt_heading, mode='test')

            pred_lons.append(pred_u.data.cpu().numpy())
            pred_lats.append(pred_v.data.cpu().numpy())
            gt_lons.append(gt_shift_u[:, 0].data.cpu().numpy() * args.shift_range_lon)
            gt_lats.append(gt_shift_v[:, 0].data.cpu().numpy() * args.shift_range_lat)

            if i % 20 == 0:
                print(i)

    pred_lons = np.concatenate(pred_lons, axis=0)
    pred_lats = np.concatenate(pred_lats, axis=0)
    gt_lons = np.concatenate(gt_lons, axis=0)
    gt_lats = np.concatenate(gt_lats, axis=0)

    scio.savemat(os.path.join(save_path, 'test2_result.mat'), {'gt_lons': gt_lons, 'gt_lats': gt_lats,
                                                         'pred_lats': pred_lats, 'pred_lons': pred_lons})

    distance = np.sqrt((pred_lons - gt_lons) ** 2 + (pred_lats - gt_lats) ** 2)  # [N]

    init_dis = np.sqrt(gt_lats ** 2 + gt_lons ** 2)
    diff_lats = np.abs(pred_lats - gt_lats)
    diff_lons = np.abs(pred_lons - gt_lons)

    metrics = [1, 3, 5]

    f = open(os.path.join(save_path, 'test_results.txt'), 'a')
    f.write('====================================\n')
    f.write('       EPOCH: ' + str(epoch) + '\n')
    print('====================================')
    print('       EPOCH: ' + str(epoch))
    print('Test2 results:')
    
    print('Distance average: (init, pred)', np.mean(init_dis), np.mean(distance))
    print('Distance median: (init, pred)', np.median(init_dis), np.median(distance))

    print('Lateral average: (init, pred)', np.mean(np.abs(gt_lats)), np.mean(diff_lats))
    print('Lateral median: (init, pred)', np.median(np.abs(gt_lats)), np.median(diff_lats))

    print('Longitudinal average: (init, pred)', np.mean(np.abs(gt_lons)), np.mean(diff_lons))
    print('Longitudinal median: (init, pred)', np.median(np.abs(gt_lons)), np.median(diff_lons))

    for idx in range(len(metrics)):
        pred = np.sum(distance < metrics[idx]) / distance.shape[0] * 100
        init = np.sum(init_dis < metrics[idx]) / init_dis.shape[0] * 100

        line = 'distance within ' + str(metrics[idx]) + ' meters (pred, init): ' + str(pred) + ' ' + str(init)
        print(line)
        f.write(line + '\n')

    print('-------------------------')
    f.write('------------------------\n')

    diff_lats = np.abs(pred_lats - gt_lats)
    diff_lons = np.abs(pred_lons - gt_lons)
    for idx in range(len(metrics)):
        pred = np.sum(diff_lats < metrics[idx]) / diff_lats.shape[0] * 100
        init = np.sum(np.abs(gt_lats) < metrics[idx]) / gt_lats.shape[0] * 100

        line = 'lateral      within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    for idx in range(len(metrics)):
        pred = np.sum(diff_lons < metrics[idx]) / diff_lons.shape[0] * 100
        init = np.sum(np.abs(gt_lons) < metrics[idx]) / gt_lons.shape[0] * 100

        line = 'longitudinal within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    print('====================================')
    f.write('====================================\n')
    f.close()

    net_test.train()

    return


def train(net, lr, args, save_path):

    for epoch in range(args.resume, args.epochs):
        net.train()

        base_lr = lr
        base_lr = base_lr * ((1.0 - float(epoch) / 100.0) ** (1.0))

        optimizer = optim.Adam(net.parameters(), lr=base_lr)
        optimizer.zero_grad()

        trainloader = load_train_data(mini_batch, args.shift_range_lat, args.shift_range_lon, args.rotation_range)

        loss_vec = []

        print('batch_size:', mini_batch, '\n num of batches:', len(trainloader))

        for Loop, Data in enumerate(trainloader, 0):

            sat_map, left_camera_k, grd_left_imgs, gt_shift_u, gt_shift_v, gt_heading = [item.to(device) for item in Data[:-1]]
            file_name = Data[-1]

            optimizer.zero_grad()

            if args.proj == 'CrossAttn':
                loss = net.CVattn_corr(sat_map, grd_left_imgs, left_camera_k, gt_shift_u, gt_shift_v, gt_heading, mode='train')
            else:
                loss = net.corr(sat_map, grd_left_imgs, left_camera_k, gt_shift_u, gt_shift_v, gt_heading, mode='train', file_name=file_name)

            loss.backward()

            optimizer.step()
            optimizer.zero_grad()

            loss_vec.append(loss.item())

            if Loop % 10 == 9:  #
                print('Epoch: ' + str(epoch) + ' Loop: ' + str(Loop) + ' T Loss: ' + str(loss.item()))

        print('Save Model ...')

        torch.save(net.state_dict(), os.path.join(save_path, 'model_' + str(epoch) + '.pth'))

        test1(net, args, save_path, epoch)
        test2(net, args, save_path, epoch)

    print('Finished Training')


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--resume', type=int, default=0, help='resume the trained model')
    parser.add_argument('--test', type=int, default=1, help='test with trained model')

    parser.add_argument('--epochs', type=int, default=5, help='number of training epochs')

    parser.add_argument('--lr', type=float, default=1e-4, help='learning rate')  # 1e-2

    parser.add_argument('--rotation_range', type=float, default=0., help='degree')
    parser.add_argument('--shift_range_lat', type=float, default=20., help='meters')
    parser.add_argument('--shift_range_lon', type=float, default=20., help='meters')

    parser.add_argument('--coe_triplet', type=float, default=1, help='degree')

    parser.add_argument('--batch_size', type=int, default=1, help='batch size')

    parser.add_argument('--level', type=int, default=3, help='2, 3, 4, -1, -2, -3, -4')
    parser.add_argument('--N_iters', type=int, default=5, help='any integer')

    parser.add_argument('--Optimizer', type=str, default='TransV1G2SP', help='it does not matter in the orientation-aligned setting')

    parser.add_argument('--proj', type=str, default='CrossAttn', help='geo, polar, nn, CrossAttn')
    parser.add_argument('--use_uncertainty', type=int, default=1, help='0 or 1')

    args = parser.parse_args()

    return args



def getSavePath(args):
    save_path = './ModelsKitti/2DoF/'\
                + '/lat' + str(args.shift_range_lat) + 'm_lon' + str(args.shift_range_lon) \
                + 'm_' + str(args.proj) \

    if args.use_uncertainty:
        save_path = save_path + '_Uncertainty'

    if not os.path.exists(save_path):
        os.makedirs(save_path)

    print('save_path:', save_path)

    return save_path


if __name__ == '__main__':

    if torch.cuda.is_available():
        device = torch.device("cuda:0")
    else:
        device = torch.device("cpu")

    np.random.seed(2022)

    args = parse_args()

    mini_batch = args.batch_size

    save_path = getSavePath(args)

    net = Model(args)
    net.to(device)

    if args.test:
        net.load_state_dict(torch.load(os.path.join(save_path, 'model_4.pth')))
        test1(net, args, save_path, epoch=4)
        test2(net, args, save_path, epoch=4)

    else:
        if args.resume:
            net.load_state_dict(torch.load(os.path.join(save_path, 'model_' + str(args.resume - 1) + '.pth')))
            print("resume from " + 'model_' + str(args.resume - 1) + '.pth')

        lr = args.lr

        train(net, lr, args, save_path)

================
File: swin_transformer.py
================
# --------------------------------------------------------
# Swin Transformer
# Copyright (c) 2021 Microsoft
# Licensed under The MIT License [see LICENSE for details]
# Written by Ze Liu
# --------------------------------------------------------

import torch
import torch.nn as nn
import torch.utils.checkpoint as checkpoint
from timm.models.layers import DropPath, to_2tuple, trunc_normal_


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


def window_partition(x, window_size):
    """
    Args:
        x: (B, H, W, C)
        window_size (int): window size

    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    """
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows


def window_reverse(windows, window_size, H, W):
    """
    Args:
        windows: (num_windows*B, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image

    Returns:
        x: (B, H, W, C)
    """
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x


class WindowAttention(nn.Module):
    r""" Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.

    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
        self.register_buffer("relative_position_index", relative_position_index)

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask=None):
        """
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
        """
        B_, N, C = x.shape
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)
        # [B, num_heads, N, C//num_heads]

        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))  # [B, num_heads, N, N]

        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
        attn = attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)

        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

    def extra_repr(self) -> str:
        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'

    def flops(self, N):
        # calculate flops for 1 window with token length of N
        flops = 0
        # qkv = self.qkv(x)
        flops += N * self.dim * 3 * self.dim
        # attn = (q @ k.transpose(-2, -1))
        flops += self.num_heads * N * (self.dim // self.num_heads) * N
        #  x = (attn @ v)
        flops += self.num_heads * N * N * (self.dim // self.num_heads)
        # x = self.proj(x)
        flops += N * self.dim * self.dim
        return flops


class SwinTransformerBlock(nn.Module):
    r""" Swin Transformer Block.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resulotion.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        if min(self.input_resolution) <= self.window_size:
            # if window size is larger than input resolution, we don't partition windows
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        assert 0 <= self.shift_size < self.window_size, "shift_size must in 0-window_size"

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        if self.shift_size > 0:
            # calculate attention mask for SW-MSA
            H, W = self.input_resolution
            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1

            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
        else:
            attn_mask = None

        self.register_buffer("attn_mask", attn_mask)

    def forward(self, x):
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"

        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, C)

        # cyclic shift
        if self.shift_size > 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
        else:
            shifted_x = x

        # partition windows
        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C

        # W-MSA/SW-MSA
        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C

        # merge windows
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C

        # reverse cyclic shift
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        x = x.view(B, H * W, C)

        # FFN
        x = shortcut + self.drop_path(x)
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, " \
               f"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}"

    def flops(self):
        flops = 0
        H, W = self.input_resolution
        # norm1
        flops += self.dim * H * W
        # W-MSA/SW-MSA
        nW = H * W / self.window_size / self.window_size
        flops += nW * self.attn.flops(self.window_size * self.window_size)
        # mlp
        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio
        # norm2
        flops += self.dim * H * W
        return flops


class PatchMerging(nn.Module):
    r""" Patch Merging Layer.

    Args:
        input_resolution (tuple[int]): Resolution of input feature.
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(4 * dim)

    def forward(self, x):
        """
        x: B, H*W, C
        """
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"
        assert H % 2 == 0 and W % 2 == 0, f"x size ({H}*{W}) are not even."

        x = x.view(B, H, W, C)

        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.norm(x)
        x = self.reduction(x)

        return x

    def extra_repr(self) -> str:
        return f"input_resolution={self.input_resolution}, dim={self.dim}"

    def flops(self):
        H, W = self.input_resolution
        flops = H * W * self.dim
        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim
        return flops


class BasicLayer(nn.Module):
    """ A basic Swin Transformer layer for one stage.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resolution.
        depth (int): Number of blocks.
        num_heads (int): Number of attention heads.
        window_size (int): Local window size.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
    """

    def __init__(self, dim, input_resolution, depth, num_heads, window_size,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):

        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.depth = depth
        self.use_checkpoint = use_checkpoint

        # build blocks
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,
                                 num_heads=num_heads, window_size=window_size,
                                 shift_size=0 if (i % 2 == 0) else window_size // 2,
                                 mlp_ratio=mlp_ratio,
                                 qkv_bias=qkv_bias, qk_scale=qk_scale,
                                 drop=drop, attn_drop=attn_drop,
                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                                 norm_layer=norm_layer)
            for i in range(depth)])

        # patch merging layer
        if downsample is not None:
            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)
        else:
            self.downsample = None

    def forward(self, x):
        for blk in self.blocks:
            if self.use_checkpoint:
                x = checkpoint.checkpoint(blk, x)
            else:
                x = blk(x)
        if self.downsample is not None:
            x = self.downsample(x)
        return x

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}"

    def flops(self):
        flops = 0
        for blk in self.blocks:
            flops += blk.flops()
        if self.downsample is not None:
            flops += self.downsample.flops()
        return flops


class PatchEmbed(nn.Module):
    r""" Image to Patch Embedding

    Args:
        img_size (int): Image size.  Default: 224.
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
        if self.norm is not None:
            x = self.norm(x)
        return x

    def flops(self):
        Ho, Wo = self.patches_resolution
        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])
        if self.norm is not None:
            flops += Ho * Wo * self.embed_dim
        return flops


class SwinTransformer(nn.Module):
    r""" Swin Transformer
        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -
          https://arxiv.org/pdf/2103.14030

    Args:
        img_size (int | tuple(int)): Input image size. Default 224
        patch_size (int | tuple(int)): Patch size. Default: 4
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        embed_dim (int): Patch embedding dimension. Default: 96
        depths (tuple(int)): Depth of each Swin Transformer layer.
        num_heads (tuple(int)): Number of attention heads in different layers.
        window_size (int): Window size. Default: 7
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None
        drop_rate (float): Dropout rate. Default: 0
        attn_drop_rate (float): Attention dropout rate. Default: 0
        drop_path_rate (float): Stochastic depth rate. Default: 0.1
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
        patch_norm (bool): If True, add normalization after patch embedding. Default: True
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,
                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],
                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, **kwargs):
        super().__init__()

        self.num_classes = num_classes
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.ape = ape
        self.patch_norm = patch_norm
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))
        self.mlp_ratio = mlp_ratio

        # split image into non-overlapping patches
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)
        num_patches = self.patch_embed.num_patches
        patches_resolution = self.patch_embed.patches_resolution
        self.patches_resolution = patches_resolution

        # absolute position embedding
        if self.ape:
            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
            trunc_normal_(self.absolute_pos_embed, std=.02)

        self.pos_drop = nn.Dropout(p=drop_rate)

        # stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

        # build layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),
                               input_resolution=(patches_resolution[0] // (2 ** i_layer),
                                                 patches_resolution[1] // (2 ** i_layer)),
                               depth=depths[i_layer],
                               num_heads=num_heads[i_layer],
                               window_size=window_size,
                               mlp_ratio=self.mlp_ratio,
                               qkv_bias=qkv_bias, qk_scale=qk_scale,
                               drop=drop_rate, attn_drop=attn_drop_rate,
                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                               norm_layer=norm_layer,
                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                               use_checkpoint=use_checkpoint)
            self.layers.append(layer)

        self.norm = norm_layer(self.num_features)
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'absolute_pos_embed'}

    @torch.jit.ignore
    def no_weight_decay_keywords(self):
        return {'relative_position_bias_table'}

    def forward_features(self, x):
        x = self.patch_embed(x)
        if self.ape:
            x = x + self.absolute_pos_embed
        x = self.pos_drop(x)

        for layer in self.layers:
            x = layer(x)

        x = self.norm(x)  # B L C
        x = self.avgpool(x.transpose(1, 2))  # B C 1
        x = torch.flatten(x, 1)
        return x

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x

    def flops(self):
        flops = 0
        flops += self.patch_embed.flops()
        for i, layer in enumerate(self.layers):
            flops += layer.flops()
        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)
        flops += self.num_features * self.num_classes
        return flops



class TransOptimizerS2GP_V1(nn.Module):
    def __init__(self):
        super(TransOptimizerS2GP_V1, self).__init__()

        self.level_1 = SwinTransformer(img_size=[64, 512], patch_size=4, in_chans=64, num_classes=3,
                 embed_dim=48, depths=[1], num_heads=[3],
                 window_size=8, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False)

        self.level_2 = SwinTransformer(img_size=[32, 256], patch_size=4, in_chans=128, num_classes=3,
                 embed_dim=48, depths=[1], num_heads=[3],
                 window_size=8, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False)

        self.level_4 = SwinTransformer(img_size=[16, 128], patch_size=4, in_chans=256, num_classes=3,
                 embed_dim=48, depths=[1], num_heads=[3],
                 window_size=8, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False)

    def forward(self, pred_feat, ref_feat):
        r = pred_feat - ref_feat  # [B, C, H, W]
        B, C, _, _ = r.shape

        if C == 256:
            x = self.level_4(r)  
        elif C == 128:
            x = self.level_2(r)
        elif C == 64:
            x = self.level_1(r)
       
        return x # [B, 3]
        


class TransOptimizerG2SP_V1(nn.Module):
    def __init__(self):
        super(TransOptimizerG2SP_V1, self).__init__()

        self.level_1 = SwinTransformer(img_size=[256, 256], patch_size=4, in_chans=64, num_classes=3,
                 embed_dim=48, depths=[1], num_heads=[3],
                 window_size=8, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False)

        self.level_2 = SwinTransformer(img_size=[128, 128], patch_size=4, in_chans=128, num_classes=3,
                 embed_dim=48, depths=[1], num_heads=[3],
                 window_size=8, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False)

        self.level_4 = SwinTransformer(img_size=[64, 64], patch_size=4, in_chans=256, num_classes=3,
                 embed_dim=48, depths=[1], num_heads=[3],
                 window_size=8, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False)
        

    def forward(self, pred_feat, ref_feat):
        r = pred_feat - ref_feat  # [B, C, H, W]
        B, C, _, _ = r.shape

        if C == 256:
            x = self.level_4(r)
        elif C == 128:
            x = self.level_2(r)
        elif C == 64:
            x = self.level_1(r)
       
        return x # [B, 3]

================
File: Oxford_dataset.py
================
import random

import numpy as np
import os
from PIL import Image
from torch.utils.data import Dataset

import torch
import pandas as pd
import utils
import torchvision.transforms.functional as TF
from torchvision import transforms
import torch.nn.functional as F

from torch.utils.data import DataLoader
from torchvision import transforms
import cv2
import math

###############
""" Cross view localization experiment: vehicle perception range: 19.7m  experiment """
###############


#root_dir = '/media/yujiao/6TB/FeiWu/Oxford_dataset/Oxford_ground/' # '../../data/Kitti' # '../Data' #'..\\Data'
root_dir = '/home/users/u6293587/Oxford_dataset/Oxford_ground/'

GrdImg_H = 154  # 256 # original: 375 #224, 256
GrdImg_W = 231  # 1024 # original:1242 #1248, 1024
GrdOriImg_H = 800
GrdOriImg_W = 1200
num_thread_workers = 2

# train_file = './dataLoader/train_files.txt'
train_file = './dataLoader/oxford/training.txt'
test1_file = './dataLoader/oxford/test1_j.txt'
test2_file = './dataLoader/oxford/test2_j.txt'
test3_file = './dataLoader/oxford/test3_j.txt'
val_file = "./oxford/validation.txt"


class SatGrdDataset(Dataset):
    """
    output:
    sat img
    left_camera_k
    grd img
    gt_shift_x       pixel
    gt_shift_y       pixel
    theta            (-1,1)
    0.1235 meter_per_pixel
    sat_map shape:  torch.Size([1, 3, 512, 512])
    grd image shape:  torch.Size([1, 3, 154, 231])
    """
    def __init__(self, root, file, transform=None, rotation_range=0):

        self.root = root

        if transform != None:
            self.satmap_transform = transform[0]
            self.grdimage_transform = transform[1]
        self.rotation_range=rotation_range

        #broken ground image idx
        self.yaws = np.load("./dataLoader/oxford/train_yaw.npy")

        broken = [937, 9050, 11811, 12388, 16584]
        self.train_yaw = []
        for i in range(len(self.yaws)):
            if i not in broken:
                self.train_yaw.append(self.yaws[i])  #loading yaws

        primary = np.array([[619400., 5736195.],
                            [619400., 5734600.],
                            [620795., 5736195.],
                            [620795., 5734600.],
                            [620100., 5735400.]])
        secondary = np.array([[900., 900.],  # tl
                              [492., 18168.],  # bl
                              [15966., 1260.],  # tr
                              [15553., 18528.],  # br
                              [8255., 9688.]])  # c
        n = primary.shape[0]
        pad = lambda x: np.hstack([x, np.ones((x.shape[0], 1))])
        unpad = lambda x: x[:, :-1]
        X = pad(primary)
        Y = pad(secondary)
        A, res, rank, s = np.linalg.lstsq(X, Y)

        self.transform = lambda x: unpad(np.dot(pad(x), A))
        self.sat_map = cv2.imread(
            "./dataLoader/oxford/satellite_map_new.png")  # read whole over-view map

        print(self.sat_map.shape)

        with open(file, 'r') as f:
            self.file_name = f.readlines()    # read training file

        trainlist = []
        with open("./dataLoader/oxford/"+'training.txt', 'r') as filehandle:
            filecontents = filehandle.readlines()
            for line in filecontents:
                content = line[:-1]
                trainlist.append(content.split(" "))
        self.trainList = trainlist
        self.trainNum = len(trainlist)
        trainarray = np.array(trainlist)
        self.trainUTM = np.transpose(trainarray[:, 2:].astype(np.float64))

    def __len__(self):
        return len(self.file_name)

    def get_file_list(self):
        return self.file_name

    def __getitem__(self, idx):
        img_idx=idx
        # =================== read camera intrinsice for left and right cameras ====================

        fx = float(964.828979) * GrdImg_W / GrdOriImg_W
        cx = float(643.788025) * GrdImg_W / GrdOriImg_W
        fy = float(964.828979) * GrdImg_H / GrdOriImg_H
        cy = float(484.407990) * GrdImg_H / GrdOriImg_H

        left_camera_k = [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]
        left_camera_k = torch.from_numpy(np.asarray(left_camera_k, dtype=np.float32))
        # if not self.stereo:
        # =================== read ground img ===================================
        line = self.file_name[idx]
        grdimg=line.split(" ")[0]
        grdimg=root_dir+grdimg

        left_img_name = os.path.join(grdimg)
        #print("********: ",left_img_name)

        grd_img = cv2.imread(left_img_name)
        # cv2.imwrite("./test_visual/grd_img.png", grd_img)


        grd_img = Image.fromarray(cv2.cvtColor(grd_img, cv2.COLOR_BGR2RGB))
        grd_img= self.grdimage_transform(grd_img)

        # =================== position in satellite map ===================================
        image_coord = np.round(self.transform(np.array([[self.trainUTM[0, img_idx], self.trainUTM[1, img_idx]]]))[0])

        # =================== set random offset ===================================
        alpha = 2 * math.pi * random.random()
        r = 200 * np.sqrt(2) * random.random()
        row_offset = int(r * math.cos(alpha))
        col_offset = int(r * math.sin(alpha))

        sat_coord_row = int(image_coord[1] + row_offset)  # sat center location
        sat_coord_col = int(image_coord[0] + col_offset)   # sat: 0.07904 m/pixel

        # print(sat_coord_row, sat_coord_col)
        # =================== crop satellite map ===================================

        # img = self.sat_map[sat_coord_row - 400 - 200:sat_coord_row + 400 + 200,
        #       sat_coord_col - 400 - 200:sat_coord_col + 400 + 200,
        #       :]  # load at each side extra 200 pixels to avoid blank after rotation
        #cv exp:
        img = self.sat_map[sat_coord_row - 1036-400:sat_coord_row  + 1036+400,
              sat_coord_col  - 1036-400:sat_coord_col  + 1036+400,
              :]

        #=================== set rotation random ===================================
        theta = np.random.uniform(-1, 1)
        # rdm= theta * self.rotation_range/ np.pi * 180        # radian  ground truth
        rdm = theta * self.rotation_range                      #degree
        #======================================================================

        # rotate_angle = self.train_yaw[img_idx] / np.pi * 180-90 +rdm*180/np.pi   # degree
        # rotate_angle = self.train_yaw[img_idx] / np.pi * 180 - 90 + rdm            # degree
        # rot_matrix = cv2.getRotationMatrix2D((600, 600), rotate_angle, 1)
        # img = cv2.warpAffine(img, rot_matrix, (1200, 1200))
        # img = img[200:1000, 200:1000, :]
        # sat_img = cv2.resize(img, (512, 512), interpolation=cv2.INTER_AREA)# 0.1235

        rotate_angle = self.train_yaw[img_idx] / np.pi * 180 - 90 + rdm            # degree
        rot_matrix = cv2.getRotationMatrix2D((1436,1436), rotate_angle, 1)
        img = cv2.warpAffine(img, rot_matrix, (2872, 2872))
        #img = img[200:-200, 200:-200, :] #1738
        img = img[400:-400, 400:-400, :]  # 2072
        # print("******** img shape: ",img.shape)
        #sat_img = cv2.resize(img, (512, 512), interpolation=cv2.INTER_AREA)# 0.1235
        #cv experiment
        sat_img = cv2.resize(img, (512, 512), interpolation=cv2.INTER_AREA)
        # cv2.imwrite("./test_visual/sat.png", sat_img )
        # cv2.imwrite("./sat_img.png", sat_img)
        # img[:, :, 0] -= 103.939  # Blue
        # img[:, :, 1] -= 116.779  # Green
        # img[:, :, 2] -= 123.6  # Red
        sat_img = Image.fromarray(cv2.cvtColor(sat_img, cv2.COLOR_BGR2RGB))
        sat_img = self.satmap_transform(sat_img)
        # row_offset_resized = int(np.round((789+ row_offset) / 1578 * 512 - 256))
        # col_offset_resized = int(np.round((789 + col_offset) / 1578 * 512 - 256))

        # #cv experiment
        # row_offset_resized = int(np.round((1236+ row_offset) / 2472 * 512 - 256))
        # col_offset_resized = int(np.round((1236 + col_offset) / 2472 * 512 - 256))
        row_offset_resized = int(np.round((1036+ row_offset) / 2072 * 512 - 256))
        col_offset_resized = int(np.round((1036 + col_offset) / 2072 * 512 - 256))

        #=================== set ground truth ===================================
        x, y = np.meshgrid(np.linspace(-256 + col_offset_resized, 256 + col_offset_resized, 512),
                           np.linspace(-256 + row_offset_resized, 256 + row_offset_resized, 512))


        d = np.sqrt(x * x + y * y)
        sigma, mu = 4, 0.0
        img0 =np.exp(-((d - mu) ** 2 / (2.0 * sigma ** 2)))
        rot_matrix = cv2.getRotationMatrix2D((256, 256), rotate_angle, 1)
        img0 = cv2.warpAffine(img0, rot_matrix, (512, 512))
        # cv2.imwrite("./test_visual/GT.png", img0)

        a = np.where(img0 == img0.max())
        y = a[0]
        x = a[1]

        gt_shift_x = (x[0]-256)   # pixel  right positive parallel heading
        gt_shift_y = (y[0]-256)   # pixel  down positive vertical heading         0.1235 meter_per_pixel



        return sat_img, left_camera_k, grd_img, \
               torch.tensor(gt_shift_x, dtype=torch.float32).reshape(1), \
               torch.tensor(gt_shift_y, dtype=torch.float32).reshape(1), \
               torch.tensor(theta, dtype=torch.float32).reshape(1) \


class SatGrdDatasetVal(Dataset):

    def __init__(self, root, transform=None, rotation_range=0):

        self.root = root

        if transform is not None:
            self.satmap_transform = transform[0]
            self.grdimage_transform = transform[1]
        self.rotation_range=rotation_range

        #broken ground image idx
        # self.yaws = np.load("./dataLoader/oxford/val_yaw.npy") # for debug
        self.yaws = np.load("./dataLoader/oxford/val_yaw.npy")  # for debug
        primary = np.array([[619400., 5736195.],
                            [619400., 5734600.],
                            [620795., 5736195.],
                            [620795., 5734600.],
                            [620100., 5735400.]])
        secondary = np.array([[900., 900.],  # tl
                              [492., 18168.],  # bl
                              [15966., 1260.],  # tr
                              [15553., 18528.],  # br
                              [8255., 9688.]])  # c
        n = primary.shape[0]
        pad = lambda x: np.hstack([x, np.ones((x.shape[0], 1))])
        unpad = lambda x: x[:, :-1]
        X = pad(primary)
        Y = pad(secondary)
        A, res, rank, s = np.linalg.lstsq(X, Y)

        self.transform = lambda x: unpad(np.dot(pad(x), A))
        self.sat_map = cv2.imread("./dataLoader/oxford/satellite_map_new.png")  # read whole over-view map
        vallist = []
        with open("./dataLoader/oxford/"+'validation.txt', 'r') as filehandle:
            filecontents = filehandle.readlines()
            for line in filecontents:
                content = line[:-1]
                vallist.append(content.split(" "))
        self.valList = vallist
        self.valNum = len(vallist)
        valarray = np.array(vallist)
        self.valUTM = np.transpose(valarray[:, 2:].astype(np.float64))

    def __len__(self):
        return self.valNum

    def get_file_list(self):
        return self.valList

    def __getitem__(self, idx):
        img_idx=idx
        # =================== read camera intrinsice for left and right cameras ====================

        fx = float(964.828979) * GrdImg_W / GrdOriImg_W
        cx = float(643.788025) * GrdImg_W / GrdOriImg_W
        fy = float(964.828979) * GrdImg_H / GrdOriImg_H
        cy = float(484.407990) * GrdImg_H / GrdOriImg_H

        left_camera_k = [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]
        left_camera_k = torch.from_numpy(np.asarray(left_camera_k, dtype=np.float32))

        # =================== read ground img ===================================
        line = self.valList[idx]
        grdimg=root_dir+line[0]

        left_img_name = os.path.join(grdimg)
        grd_img = cv2.imread(left_img_name)
        # cv2.imwrite("./test_visual/grd_img.png", grd_img)

        grd_img = Image.fromarray(cv2.cvtColor(grd_img, cv2.COLOR_BGR2RGB))
        grd_img= self.grdimage_transform(grd_img)

        # =================== position in satellite map ===================================
        image_coord = np.round(self.transform(np.array([[self.valUTM[0, img_idx], self.valUTM[1, img_idx]]]))[0])
        col_split = int((image_coord[0]) // 1036)
        if np.round(image_coord[0] - 1036 * col_split) < 600:
            col_split -= 1
        col_pixel = int(np.round(image_coord[0] - 1036 * col_split))

        row_split = int((image_coord[1]) // 1036)
        if np.round(image_coord[1] - 1036 * row_split) < 600:
            row_split -= 1
        row_pixel = int(np.round(image_coord[1] - 1036 * row_split))

        img = self.sat_map[row_split * 1036 - 600:row_split * 1036 + 2072 + 600,
              col_split * 1036 - 600:col_split * 1036 + 2072 + 600,
              :]  # read extra 200 pixels at each side to avoid blank after rotation

        # =================== set rotation random ===================================
        theta = np.random.uniform(-1, 1)
        # rdm= theta * self.rotation_range/ np.pi * 180        # radian  ground truth
        rdm = theta * self.rotation_range  # degree
        # ======================================================================
        rotate_angle = self.yaws[img_idx] / np.pi * 180-90 +rdm  # degree
        rot_matrix = cv2.getRotationMatrix2D((1636,1636), rotate_angle, 1)  # rotate satellite image
        # print("img indx shape: ",idx,img.shape)
        img = cv2.warpAffine(img, rot_matrix, (3272, 3272))
        img = img[600:-600, 600:-600, :]
        # print("### img indx shape: ", idx, img.shape)
        img = cv2.resize(img, (512, 512), interpolation=cv2.INTER_AREA)
        # cv2.imwrite("./test_visual/sat.png", img)
        # img[:, :, 0] -= 103.939  # Blue
        # img[:, :, 1] -= 116.779  # Green
        # img[:, :, 2] -= 123.6  # Red
        sat_img=Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))
        sat_img=self.satmap_transform(sat_img)

        row_offset_resized = int(-(row_pixel / 2072  * 512 - 256))
        col_offset_resized = int(-(col_pixel / 2072  * 512 - 256))
    ################################################
        x, y = np.meshgrid(np.linspace(-256 + col_offset_resized, 256 + col_offset_resized, 512),
                           np.linspace(-256 + row_offset_resized, 256 + row_offset_resized, 512))
        d = np.sqrt(x * x + y * y)
        sigma, mu = 4, 0.0
        img0 =np.exp(-((d - mu) ** 2 / (2.0 * sigma ** 2)))
        rot_matrix = cv2.getRotationMatrix2D((256, 256), rotate_angle, 1)
        img0 = cv2.warpAffine(img0, rot_matrix, (512, 512))
        # cv2.imwrite("./test_visual/GT.png", img0)
        # print("********** pos: ",img0.shape,np.where(img0==img0.max()))
        a=np.where(img0==img0.max())
        y=a[0]
        x=a[1]
    ####################################################

        gt_shift_x=(x[0]-256)   # right positive parallel heading
        gt_shift_y=(y[0]-256)   #down positive vertical heading

        return sat_img, left_camera_k, grd_img, \
               torch.tensor(gt_shift_x, dtype=torch.float32).reshape(1), \
               torch.tensor(gt_shift_y, dtype=torch.float32).reshape(1), \
               torch.tensor(theta, dtype=torch.float32).reshape(1) \


class SatGrdDatasetTest(Dataset):

    def __init__(self, root, transform=None, rotation_range=0, test=0):
        self.root = root
        if transform != None:
            self.satmap_transform = transform[0]
            self.grdimage_transform = transform[1]
        self.rotation_range = rotation_range

        # broken ground image idx

        # self.yaws = np.load("./dataLoader/oxford/train_yaw.npy")

        with open('./dataLoader/oxford/test_yaw.npy', 'rb') as f:
            self.val_yaw = np.load(f)
        if test == 2:
                self.val_yaw=self.val_yaw[1672+1:1672+1708+1]
        elif test == 3:
                self.val_yaw=self.val_yaw[1672+1708+1:1672+1708+1708+1]

        primary = np.array([[619400., 5736195.],
                            [619400., 5734600.],
                            [620795., 5736195.],
                            [620795., 5734600.],
                            [620100., 5735400.]])
        secondary = np.array([[900., 900.],  # tl
                              [492., 18168.],  # bl
                              [15966., 1260.],  # tr
                              [15553., 18528.],  # br
                              [8255., 9688.]])  # c
        n = primary.shape[0]
        pad = lambda x: np.hstack([x, np.ones((x.shape[0], 1))])
        unpad = lambda x: x[:, :-1]
        X = pad(primary)
        Y = pad(secondary)
        A, res, rank, s = np.linalg.lstsq(X, Y)

        self.transform = lambda x: unpad(np.dot(pad(x), A))
        # self.sat_map = cv2.imread(
        #     "./dataLoader/oxford/satellite_map_new.png")  # read whole over-view map

        self.sat_map = cv2.imread(
            "./dataLoader/oxford/satellite_map_new.png")  # for debug

        test_2015_08_14_14_54_57 = []
        with open('./dataLoader/oxford/test1_j.txt', 'r') as filehandle:
            filecontents = filehandle.readlines()
            for line in filecontents:
                content = line[:-1]
                test_2015_08_14_14_54_57.append(content.split(" "))
        test_2015_08_12_15_04_18 = []
        with open('./dataLoader/oxford/test2_j.txt', 'r') as filehandle:
            filecontents = filehandle.readlines()
            for line in filecontents:
                content = line[:-1]
                test_2015_08_12_15_04_18.append(content.split(" "))
        test_2015_02_10_11_58_05 = []
        with open('./dataLoader/oxford/test3_j.txt', 'r') as filehandle:
            filecontents = filehandle.readlines()
            for line in filecontents:
                content = line[:-1]
                test_2015_02_10_11_58_05.append(content.split(" "))

        if test==0:
            testlist = test_2015_08_14_14_54_57 + test_2015_08_12_15_04_18 + test_2015_02_10_11_58_05
        elif test==1:
            testlist = test_2015_08_14_14_54_57
        elif test ==2:
            testlist = test_2015_08_12_15_04_18
        else:
            testlist = test_2015_02_10_11_58_05
        self.valList = testlist
        self.valNum = len(testlist)
        valarray = np.array(testlist)
        self.valUTM = np.transpose(valarray[:,2:].astype(np.float64))
        print("len.........: ",self.valNum )

    def __len__(self):
        return self.valNum

    def get_file_list(self):
        return self.valList

    def __getitem__(self, idx):
        img_idx = idx
        # =================== read camera intrinsice for left and right cameras ====================
        fx = float(964.828979) * GrdImg_W / GrdOriImg_W
        cx = float(643.788025) * GrdImg_W / GrdOriImg_W
        fy = float(964.828979) * GrdImg_H / GrdOriImg_H
        cy = float(484.407990) * GrdImg_H / GrdOriImg_H

        left_camera_k = [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]
        left_camera_k = torch.from_numpy(np.asarray(left_camera_k, dtype=np.float32))
        # =================== read ground img ===================================
        line = self.valList[idx]
        grdimg = root_dir + line[0]

        left_img_name = os.path.join(grdimg)
        grd_img = cv2.imread(left_img_name)
        # cv2.imwrite("./test_visual/grd_img.png", grd_img)

        grd_img = Image.fromarray(cv2.cvtColor(grd_img, cv2.COLOR_BGR2RGB))
        grd_img = self.grdimage_transform(grd_img)

        # grd_img= grd_img.astype(np.float32)
        # grd_img[:, :, 0] -= 103.939  # Blue
        # grd_img[:, :, 1] -= 116.779  # Green
        # grd_img[:, :, 2] -= 123.6  # Red

        # =================== position in satellite map ===================================
        image_coord = np.round(self.transform(np.array([[self.valUTM[0, img_idx], self.valUTM[1, img_idx]]]))[0])
        col_split = int((image_coord[0]) // 1036)
        if np.round(image_coord[0] - 1036 * col_split) < 600:
            col_split -= 1
        col_pixel = int(np.round(image_coord[0] -1036 * col_split))

        row_split = int((image_coord[1]) // 1036)
        if np.round(image_coord[1] - 1036 * row_split) < 600:
            row_split -= 1
        row_pixel = int(np.round(image_coord[1] - 1036 * row_split))

        img = self.sat_map[row_split * 1036 - 600:row_split * 1036 + 2072 + 600,
              col_split * 1036 - 600:col_split * 1036+ 2072 + 600,
              :]  # read extra 200 pixels at each side to avoid blank after rotation
        # cv2.imwrite("./test_visual/crop_sat.png", img)
        # =================== set rotation random ===================================
        theta = np.random.uniform(-1, 1)
        rdm = theta * self.rotation_range  # degree
        # ======================================================================

        rotate_angle = self.val_yaw[img_idx] / np.pi * 180 - 90+rdm  # degree
        rot_matrix = cv2.getRotationMatrix2D((1636, 1636), rotate_angle, 1)  # rotate satellite image

        img = cv2.warpAffine(img, rot_matrix, (3272, 3272))
        img = img[600:-600, 600:-600, :]
        img = cv2.resize(img, (512, 512), interpolation=cv2.INTER_AREA)
        # cv2.imwrite("./test_visual/sat.png", img)
        # img[:, :, 0] -= 103.939  # Blue
        # img[:, :, 1] -= 116.779  # Green
        # img[:, :, 2] -= 123.6  # Red
        sat_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        sat_img = self.satmap_transform(sat_img)

        row_offset_resized = int(-(row_pixel / 2072 * 512 - 256))
        col_offset_resized = int(-(col_pixel / 2072 * 512 - 256))

        ################################################
        x, y = np.meshgrid(np.linspace(-256 + col_offset_resized, 256 + col_offset_resized, 512),
                           np.linspace(-256 + row_offset_resized, 256 + row_offset_resized, 512))
        d = np.sqrt(x * x + y * y)
        sigma, mu = 4, 0.0
        img0 = np.exp(-((d - mu) ** 2 / (2.0 * sigma ** 2)))
        rot_matrix = cv2.getRotationMatrix2D((256, 256), rotate_angle, 1)
        img0 = cv2.warpAffine(img0, rot_matrix, (512, 512))
        # cv2.imwrite("./test_visual/GT.png", img0)
        # print("********** pos: ",img0.shape,np.where(img0==img0.max()))
        a = np.where(img0 == img0.max())
        y = a[0]
        x = a[1]
        ####################################################

        gt_shift_x = (x[0] - 256)  # right positive parallel heading
        gt_shift_y = (y[0] - 256)  # down positive vertical heading
        # print("************* yx: ",gt_shift_x,gt_shift_y)
        # cv2.imwrite("./GT.png", img0)


        return sat_img, left_camera_k, grd_img, \
               torch.tensor(gt_shift_x, dtype=torch.float32).reshape(1), \
               torch.tensor(gt_shift_y, dtype=torch.float32).reshape(1), \
               torch.tensor(theta, dtype=torch.float32).reshape(1) \



"""
load dataset, shuffle=False, for load_test_data, testNum=0, test all test datasets
"""



def load_val_data(batch_size, rotation_range=0):

    print("loding validation dataset..............")
    satmap_transform = transforms.Compose([
        transforms.ToTensor()
    ])

    Grd_h = GrdImg_H
    Grd_w = GrdImg_W

    grdimage_transform = transforms.Compose([
        transforms.Resize(size=[Grd_h, Grd_w]),
        transforms.ToTensor(),
    ])

    val_set = SatGrdDatasetVal(root=root_dir,
                              transform=(satmap_transform, grdimage_transform),
                              rotation_range=rotation_range)
    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, pin_memory=True,
                            num_workers=num_thread_workers, drop_last=False)
    return val_loader


def load_test_data(batch_size, rotation_range=0, testNum=0):
    print("loading test dataset..............")
    SatMap_process_sidelength = utils.get_process_satmap_sidelength()

    satmap_transform = transforms.Compose([
        transforms.Resize(size=[SatMap_process_sidelength, SatMap_process_sidelength]),
        transforms.ToTensor(),
    ])

    Grd_h = GrdImg_H
    Grd_w = GrdImg_W

    grdimage_transform = transforms.Compose([
        transforms.Resize(size=[Grd_h, Grd_w]),
        transforms.ToTensor(),
    ])


    test_set = SatGrdDatasetTest(root=root_dir,
                              transform=(satmap_transform, grdimage_transform),
                              rotation_range=rotation_range,test=testNum)

    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, pin_memory=True,
                            num_workers=num_thread_workers, drop_last=False)
    return test_loader


def load_train_data(batch_size, rotation_range=0):
    print("loding train dataset..............")
    satmap_transform = transforms.Compose([
        transforms.ToTensor()
    ])

    Grd_h = GrdImg_H
    Grd_w = GrdImg_W

    grdimage_transform = transforms.Compose([
        transforms.Resize(size=[Grd_h, Grd_w]),
        transforms.ToTensor(),
    ])

    train_set = SatGrdDataset(root=root_dir, file=train_file,
                              transform=(satmap_transform, grdimage_transform),
                              rotation_range=rotation_range)

    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True,
                              num_workers=num_thread_workers, drop_last=False)
    return train_loader

================
File: model_oxford.py
================
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import torch
from torchvision import transforms
import utils
import torchvision.transforms.functional as TF

from VGG import VGGUnet, VGGUnet_G2S, Encoder, Decoder, Decoder2, Decoder4, VGGUnetTwoDec
from jacobian import grid_sample

from RNNs import NNrefine, Uncertainty
from swin_transformer import TransOptimizerG2SP_V1
from swin_transformer_cross import TransOptimizerG2SP, TransOptimizerG2SPV2
from cross_attention import CrossViewAttention

EPS = utils.EPS


class ModelOxford(nn.Module):
    def __init__(self, args):  # device='cuda:0',
        super(ModelOxford, self).__init__()

        self.args = args

        self.level = 3
        self.N_iters = args.N_iters

        self.rotation_range = args.rotation_range

        self.SatFeatureNet = VGGUnet(self.level)
        if self.args.proj == 'nn':
            self.GrdFeatureNet = VGGUnet_G2S(self.level)
        elif self.args.proj == 'CrossAttn':
            self.GrdEnc = Encoder()
            self.GrdDec = Decoder()
            self.Dec4 = Decoder4()
            self.Dec2 = Decoder2()
            self.CVattn = CrossViewAttention(blocks=2, dim=256, heads=4, dim_head=16, qkv_bias=False)
        else:
            self.GrdFeatureNet = VGGUnet(self.level)

        self.meter_per_pixel = 0.0924 * self.args.sat_ori_res / 512  # 0.144375

        if self.args.Optimizer == 'NN':
            self.NNrefine = NNrefine()
        elif self.args.Optimizer == 'TransV1':
            self.TransRefine = TransOptimizerG2SP_V1()
        elif self.args.Optimizer == 'TransVfeat':
            self.TransRefine = TransOptimizerG2SP(pose_from='feature')
        elif self.args.Optimizer == 'TransVattn':
            self.TransRefine = TransOptimizerG2SP(pose_from='attention')
        elif self.args.Optimizer == 'TransV2feat':
            self.TransRefine = TransOptimizerG2SPV2(pose_from='feature')
        elif self.args.Optimizer == 'TransV2attn':
            self.TransRefine = TransOptimizerG2SPV2(pose_from='attention')

        if self.args.use_uncertainty:
            self.uncertain_net = Uncertainty()

        torch.autograd.set_detect_anomaly(True)
        # Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function.
        # Any backward computation that generate nan value will raise an error.

    def get_warp_sat2real(self, satmap_sidelength):
        # realword: X: East, Y:Down, Z: North   origin is set to the ground plane
        # satmap_sidelength = 512
        i = j = torch.arange(0, satmap_sidelength).cuda()  # to(self.device)
        ii, jj = torch.meshgrid(i, j)  # i:h,j:w

        uv = torch.stack([jj, ii], dim=-1).float()  # shape = [satmap_sidelength, satmap_sidelength, 2]

        u0 = v0 = satmap_sidelength // 2
        uv_center = uv - torch.tensor([u0, v0]).cuda()

        # meter_per_pixel = 0.1235
        meter_per_pixel = self.meter_per_pixel * 512 / satmap_sidelength
        R = torch.tensor([[1, 0], [0, -1]]).float().cuda()
        Aff_sat2real = meter_per_pixel * R  # shape = [2,2]

        XZ = torch.einsum('ij, hwj -> hwi', Aff_sat2real,
                          uv_center)  # shape = [satmap_sidelength, satmap_sidelength, 2]

        # Z = 0.4023 * torch.ones_like(XY[..., :1])
        Y = torch.zeros_like(XZ[..., :1])

        XYZ = torch.cat([XZ[..., 0:1], Y, XZ[..., 1:]], dim=-1).unsqueeze(
            dim=0)  # [1, satmap_sidelength, satmap_sidelength, 3]

        return XYZ

    def seq_warp_real2camera(self, ori_shift_u, ori_shift_v, ori_heading, XYZ_1, ori_camera_k, grd_H, grd_W, ori_grdH,
                             ori_grdW):
        B = ori_heading.shape[0]

        shift_u_meters = ori_shift_u * self.meter_per_pixel
        shift_v_meters = ori_shift_v * self.meter_per_pixel
        heading = ori_heading * self.rotation_range / 180 * np.pi
        cos = torch.cos(heading)
        sin = torch.sin(heading)
        zeros = torch.zeros_like(cos)
        ones = torch.ones_like(cos)
        # R =torch.cat([cos, -sin, zeros, sin, cos, zeros, zeros, zeros, ones], dim=-1) # shape = [B,9]
        R = torch.cat([cos, zeros, -sin, zeros, ones, zeros, sin, zeros, cos], dim=-1)
        R = R.view(B, 3, 3)  # shape = [B,3,3]
        R_inv = torch.inverse(R)
        camera_height = utils.get_camera_height()
        height = camera_height * torch.ones_like(shift_u_meters)
        T = torch.cat([-shift_u_meters, height, shift_v_meters, ], dim=-1)  # shape = [B, 3]
        # T = torch.unsqueeze(T, dim=-1)  # shape = [B,3,1]

        camera_k = ori_camera_k.clone()
        camera_k[:, :1, :] = ori_camera_k[:, :1,
                             :] * grd_W / ori_grdW  # original size input into feature get network/ output of feature get network
        camera_k[:, 1:2, :] = ori_camera_k[:, 1:2, :] * grd_H / ori_grdH

        KR_FL = torch.matmul(camera_k, R_inv)  # [B, 3, 3]
        XYZc = XYZ_1[:, :, :, :] + T[:, None, None, :]  # [B, H, W, 3]
        uv1 = torch.sum(KR_FL[:, None, None, :, :] * XYZc[:, :, :, None, :], dim=-1)  # [B, H, W, 3]

        uv1_last = torch.maximum(uv1[:, :, :, 2:], torch.ones_like(uv1[:, :, :, 2:]) * 1e-6)
        uv = uv1[..., :2] / uv1_last  # shape = [B, H, W, 2]
        mask = torch.greater(uv1_last, torch.ones_like(uv1[:, :, :, 2:]) * 1e-6)

        return uv, mask

    def project_grd_to_map(self, grd_f, shift_u, shift_v, heading, camera_k, satmap_sidelength, ori_grdH, ori_grdW):

        B, C, H, W = grd_f.size()

        XYZ_1 = self.get_warp_sat2real(satmap_sidelength)  # [ sidelength,sidelength,4]

        uv, mask = self.seq_warp_real2camera(shift_u, shift_v, heading, XYZ_1, camera_k, H, W, ori_grdH, ori_grdW)  # [B, S, E, H, W,2]
        grd_f_trans, _ = grid_sample(grd_f, uv)

        if self.args.proj == 'CrossAttn':
            return grd_f_trans, mask, uv[..., 0]
        else:
            return grd_f_trans, mask

    def forward(self, sat_map, grd_img_left, left_camera_k, gt_shift_u=None, gt_shift_v=None, gt_heading=None,
                mode='train', file_name=None, gt_depth=None):

        '''
        :param sat_map: [B, C, A, A] A--> sidelength
        :param left_camera_k: [B, 3, 3]
        :param grd_img_left: [B, C, H, W]
        :return:
        '''

        B, _, ori_grdH, ori_grdW = grd_img_left.shape
        A = sat_map.shape[-1]
        sat_align_cam_trans = self.project_grd_to_map(
            grd_img_left, None, gt_shift_u, gt_shift_v, gt_heading, left_camera_k, 512, ori_grdH, ori_grdW)
        # print("sat_align_cam_trans: ",sat_align_cam_trans.size)

        grd_img = transforms.ToPILImage()(sat_align_cam_trans[0])
        grd_img.save('./grd2sat.png')
        sat_align_cam = transforms.ToPILImage()(grd_img_left[0])
        sat_align_cam.save('./grd.png')
        sat = transforms.ToPILImage()(sat_map[0])
        sat.save('./sat.png')


    def Trans_update(self, shift_u, shift_v, heading, grd_feat_proj, sat_feat, mask):
        B = shift_u.shape[0]
        grd_feat_norm = torch.norm(grd_feat_proj.reshape(B, -1), p=2, dim=-1)
        grd_feat_norm = torch.maximum(grd_feat_norm, 1e-6 * torch.ones_like(grd_feat_norm))
        grd_feat_proj = grd_feat_proj / grd_feat_norm[:, None, None, None]

        delta = self.TransRefine(grd_feat_proj, sat_feat, mask)  # [B, 3]
        # print('=======================')
        # print('delta.shape: ', delta.shape)
        # print('shift_u.shape', shift_u.shape)
        # print('=======================')

        shift_u_new = shift_u + delta[:, 0:1]
        shift_v_new = shift_v + delta[:, 1:2]
        heading_new = heading + delta[:, 2:3]

        B = shift_u.shape[0]

        rand_u = torch.distributions.uniform.Uniform(-1, 1).sample([B, 1]).to(shift_u.device)
        rand_v = torch.distributions.uniform.Uniform(-1, 1).sample([B, 1]).to(shift_u.device)
        rand_u.requires_grad = True
        rand_v.requires_grad = True
        shift_u_new = torch.where((shift_u_new > -2.5) & (shift_u_new < 2.5), shift_u_new, rand_u)
        shift_v_new = torch.where((shift_v_new > -2.5) & (shift_v_new < 2.5), shift_v_new, rand_v)

        return shift_u_new, shift_v_new, heading_new



    def triplet_loss(self, corr_maps, gt_shift_u, gt_shift_v, gt_heading):
        # cos = torch.cos(gt_heading[:, 0] * self.args.rotation_range / 180 * np.pi)
        # sin = torch.sin(gt_heading[:, 0] * self.args.rotation_range / 180 * np.pi)
        #
        # gt_delta_x = - gt_shift_u[:, 0] * self.args.shift_range_lon
        # gt_delta_y = - gt_shift_v[:, 0] * self.args.shift_range_lat
        #
        # gt_delta_x_rot = - gt_delta_x * cos + gt_delta_y * sin
        # gt_delta_y_rot = gt_delta_x * sin + gt_delta_y * cos

        losses = []
        for level in range(len(corr_maps)):

            corr = corr_maps[level]
            B, corr_H, corr_W = corr.shape

            w = torch.round(corr_W / 2 - 0.5 + gt_shift_u / np.power(2, 3 - level)).reshape(-1)
            h = torch.round(corr_H / 2 - 0.5 + gt_shift_v / np.power(2, 3 - level)).reshape(-1)

            pos = corr[range(B), h.long(), w.long()]  # [B]
            # print(pos.shape)
            pos_neg = pos.reshape(-1, 1, 1) - corr  # [B, H, W]
            loss = torch.sum(torch.log(1 + torch.exp(pos_neg * 10))) / (B * (corr_H * corr_W - 1))
            # import pdb; pdb.set_trace()
            losses.append(loss)

        return torch.sum(torch.stack(losses, dim=0))

    def corr(self, sat_map, grd_img_left, left_camera_k, gt_shift_u=None, gt_shift_v=None, gt_heading=None,
                 mode='train', epoch=None):
        '''
        Args:
            sat_map: [B, C, A, A] A--> sidelength
            left_camera_k: [B, 3, 3]
            grd_img_left: [B, C, H, W]
            gt_shift_u: [B, 1] u->longitudinal
            gt_shift_v: [B, 1] v->lateral
            gt_heading: [B, 1] east as 0-degree
            mode:
            file_name:

        Returns:

        '''
        B, _, ori_grdH, ori_grdW = grd_img_left.shape

        sat_feat_list, sat_conf_list = self.SatFeatureNet(sat_map)
        if self.args.use_uncertainty:
            sat_uncer_list = self.uncertain_net(sat_feat_list)

        grd_feat_list, grd_conf_list = self.GrdFeatureNet(grd_img_left)

        shift_u = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        shift_v = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        # heading = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)

        corr_maps = []

        for level in range(len(sat_feat_list)):
            # meter_per_pixel = self.meters_per_pixel[level]

            sat_feat = sat_feat_list[level]
            grd_feat = grd_feat_list[level]

            A = sat_feat.shape[-1]

            heading = gt_heading

            grd_feat_proj, mask = self.project_grd_to_map(
                grd_feat, shift_u, shift_v, heading, left_camera_k, A, ori_grdH, ori_grdW)

            # crop_H = int(A - self.args.shift_range_lat * 3 / meter_per_pixel)
            # crop_W = int(A - self.args.shift_range_lon * 3 / meter_per_pixel)

            radius_pixel = int(np.ceil(200 * np.sqrt(2) / self.args.ori_sat_res * 512))
            crop_H = int(A - radius_pixel * 2 / np.power(2, 3 - level))
            crop_W = int(A - radius_pixel * 2 / np.power(2, 3 - level))

            g2s_feat = TF.center_crop(grd_feat_proj, [crop_H, crop_W])
            g2s_feat = F.normalize(g2s_feat.reshape(B, -1)).reshape(B, -1, crop_H, crop_W)

            s_feat = sat_feat.reshape(1, -1, A, A)  # [B, C, H, W]->[1, B*C, H, W]
            corr = F.conv2d(s_feat, g2s_feat, groups=B)[0]  # [B, H, W]

            denominator = F.avg_pool2d(sat_feat.pow(2), (crop_H, crop_W), stride=1, divisor_override=1)  # [B, 4W]
            if self.args.use_uncertainty:
                denominator = torch.sum(denominator, dim=1) * TF.center_crop(sat_uncer_list[level], [corr.shape[1], corr.shape[2]])[:, 0]
            else:
                denominator = torch.sum(denominator, dim=1)  # [B, H, W]
            denominator = torch.maximum(torch.sqrt(denominator), torch.ones_like(denominator) * 1e-6)
            corr = 2 - 2 * corr / denominator

            B, corr_H, corr_W = corr.shape

            corr_maps.append(corr)

            max_index = torch.argmin(corr.reshape(B, -1), dim=1)
            pred_u = (max_index % corr_W - corr_W / 2) #* meter_per_pixel  # / self.args.shift_range_lon
            pred_v = (max_index // corr_W - corr_H / 2) #* meter_per_pixel  # / self.args.shift_range_lat

        if mode == 'train':
            return self.triplet_loss(corr_maps, gt_shift_u, gt_shift_v, gt_heading)
        else:
            return pred_u * 2, pred_v * 2  # [B], [B]


    def forward(self, sat_map, grd_img_left, left_camera_k, gt_shift_u=None, gt_shift_v=None, gt_heading=None,
                 mode='train', epoch=None):
        '''
        Args:
            sat_map: [B, C, A, A] A--> sidelength
            left_camera_k: [B, 3, 3]
            grd_img_left: [B, C, H, W]
            gt_shift_u: [B, 1] u->longitudinal
            gt_shift_v: [B, 1] v->lateral
            gt_heading: [B, 1] east as 0-degree
            mode:
            file_name:

        Returns:

        '''
        B, _, ori_grdH, ori_grdW = grd_img_left.shape

        sat_feat_list, sat_conf_list = self.SatFeatureNet(sat_map)
        if self.args.use_uncertainty:
            sat_uncer_list = self.uncertain_net(sat_feat_list)

        sat8, sat4, sat2 = sat_feat_list

        grd8, grd4, grd2 = self.GrdEnc(grd_img_left)
        # [H/8, W/8] [H/4, W/4] [H/2, W/2]
        grd_feat_list = self.GrdDec(grd8, grd4, grd2)


        B, _, ori_grdH, ori_grdW = grd_img_left.shape

        shift_u = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        shift_v = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        # heading = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)

        grd2sat8, mask, u = self.project_grd_to_map(
            grd_feat_list[0], shift_u, shift_v, gt_heading, left_camera_k, sat8.shape[-1], ori_grdH, ori_grdW,
        )
        grd2sat4, _, _ = self.project_grd_to_map(
            grd_feat_list[1], shift_u, shift_v, gt_heading, left_camera_k, sat4.shape[-1], ori_grdH, ori_grdW,
        )
        grd2sat2, _, _ = self.project_grd_to_map(
            grd_feat_list[2], shift_u, shift_v, gt_heading, left_camera_k, sat2.shape[-1], ori_grdH, ori_grdW,
        )

        grd2sat8_attn = self.CVattn(grd2sat8, grd8, u, mask)
        grd2sat4_attn = grd2sat4 + self.Dec4(grd2sat8_attn, grd2sat4)
        grd2sat2_attn = grd2sat2 + self.Dec2(grd2sat4_attn, grd2sat2)

        grd_feat_list = [grd2sat8_attn, grd2sat4_attn, grd2sat2_attn]

        corr_maps = []

        for level in range(len(sat_feat_list)):
            # meter_per_pixel = self.meters_per_pixel[level]

            sat_feat = sat_feat_list[level]
            grd_feat = grd_feat_list[level]

            meter_per_pixel = self.meter_per_pixel * np.power(2, 3 - level)
            pad = int(10 / meter_per_pixel)
            sat_feat = F.pad(sat_feat, (pad, pad, pad, pad), 'constant', 0)

            A = sat_feat.shape[-1]

            radius_pixel = int(np.ceil(200 * np.sqrt(2) / self.args.sat_ori_res * 512))
            crop_H = int(A - radius_pixel * 2 / np.power(2, 3 - level))
            crop_W = int(A - radius_pixel * 2 / np.power(2, 3 - level))

            g2s_feat = TF.center_crop(grd_feat, [crop_H, crop_W])
            g2s_feat = F.normalize(g2s_feat.reshape(B, -1)).reshape(B, -1, crop_H, crop_W)

            s_feat = sat_feat.reshape(1, -1, A, A)  # [B, C, H, W]->[1, B*C, H, W]
            corr = F.conv2d(s_feat, g2s_feat, groups=B)[0]  # [B, H, W]

            denominator = F.avg_pool2d(sat_feat.pow(2), (crop_H, crop_W), stride=1, divisor_override=1)  # [B, 4W]
            if self.args.use_uncertainty:
                denominator = torch.sum(denominator, dim=1) * TF.center_crop(sat_uncer_list[level], [corr.shape[1], corr.shape[2]])[:, 0]
            else:
                denominator = torch.sum(denominator, dim=1)  # [B, H, W]
            denominator = torch.maximum(torch.sqrt(denominator), torch.ones_like(denominator) * 1e-6)
            corr = 2 - 2 * corr / denominator

            B, corr_H, corr_W = corr.shape

            corr_maps.append(corr)

            max_index = torch.argmin(corr.reshape(B, -1), dim=1)
            pred_u = (max_index % corr_W - (corr_W / 2+0.5)) #* meter_per_pixel  # / self.args.shift_range_lon
            pred_v = (max_index // corr_W -(corr_H / 2+0.5)) #* meter_per_pixel  # / self.args.shift_range_lat

        if mode == 'train':
            # return self.triplet_loss(corr_maps, gt_shift_u, gt_shift_v, gt_heading)
            return corr_maps[0], corr_maps[1], corr_maps[2]
        else:
            return pred_u * 2, pred_v * 2  # [B], [B]

================
File: train_ford_2DoF.py
================
import os

# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
# os.environ['CUDA_VISIBLE_DEVICES'] = '1'

import torch

import torch.optim as optim

from dataLoader.Ford_dataset import SatGrdDatasetFord, SatGrdDatasetFordTest, train_logs, train_logs_img_inds, test_logs, test_logs_img_inds

import scipy.io as scio

import ssl

ssl._create_default_https_context = ssl._create_unverified_context  # for downloading pretrained VGG weights

from models_ford import ModelFord

import numpy as np
import os
import argparse

from torch.utils.data import DataLoader
import time

torch.autograd.set_detect_anomaly(True)

def test1(net, args, save_path, test_log_ind=1, epoch=0, device=torch.device("cuda:0")):

    net.eval()
    mini_batch = args.batch_size

    np.random.seed(2022)
    torch.manual_seed(2022)

    test_set = SatGrdDatasetFordTest(logs=test_logs[test_log_ind:test_log_ind+1],
                                     logs_img_inds=test_logs_img_inds[test_log_ind:test_log_ind+1],
                                     shift_range_lat=args.shift_range_lat,
                                     shift_range_lon=args.shift_range_lon,
                                     rotation_range=args.rotation_range, whole=args.test_whole)
    testloader = DataLoader(test_set, batch_size=mini_batch, shuffle=False, pin_memory=True,
                            num_workers=2, drop_last=False)

    pred_shifts = []
    gt_shifts = []

    start_time = time.time()
    with torch.no_grad():
        for i, data in enumerate(testloader, 0):
            sat_map, grd_img, gt_shift_u, gt_shift_v, gt_heading, R_FL, T_FL = \
                [item.to(device) for item in data[:-1]]

            if args.proj == 'CrossAttn':
                shifts_u, shifts_v = \
                    net.CrossAttn_corr(sat_map, grd_img, R_FL, T_FL, gt_shift_u, gt_shift_v, torch.zeros_like(gt_heading), mode='test')

            shifts = torch.stack([shifts_u, shifts_v], dim=-1)
            gt_shift = torch.stack([gt_shift_u, gt_shift_v], dim=-1)  # [B, 2]

            pred_shifts.append(shifts.data.cpu().numpy())
            gt_shifts.append(gt_shift.data.cpu().numpy())

            if i % 20 == 0:
                print(i)

    end_time = time.time()
    duration = (end_time - start_time)/len(testloader)

    pred_shifts = np.concatenate(pred_shifts, axis=0)
    gt_shifts = np.concatenate(gt_shifts, axis=0) * np.array([args.shift_range_lat, args.shift_range_lon]).reshape(1, 2)

    if not os.path.exists(save_path):
        os.makedirs(save_path)

    scio.savemat(os.path.join(save_path, str(test_log_ind) + '_result.mat'), {'gt_shifts': gt_shifts, 'pred_shifts': pred_shifts})

    distance = np.sqrt(np.sum((pred_shifts - gt_shifts) ** 2, axis=1))  # [N]

    init_dis = np.sqrt(np.sum(gt_shifts ** 2, axis=1))
    diff_shifts = np.abs(pred_shifts - gt_shifts)

    diff_lats = diff_shifts[:, 0]
    diff_lons = diff_shifts[:, 1]

    gt_lats = gt_shifts[:, 0]
    gt_lons = gt_shifts[:, 1]

    metrics = [1, 3, 5]

    f = open(os.path.join(save_path, str(test_log_ind) + '_results.txt'), 'a')
    f.write('====================================\n')
    f.write('       EPOCH: ' + str(epoch) + '\n')
    f.write('Time per image (second): ' + str(duration) + '\n')
    print('====================================')
    print('       EPOCH: ' + str(epoch))
    print(str(test_log_ind) + ' Validation results:')

    print('Distance average: (init, pred)', np.mean(init_dis), np.mean(distance))
    print('Distance median: (init, pred)', np.median(init_dis), np.median(distance))

    print('Lateral average: (init, pred)', np.mean(np.abs(gt_lats)), np.mean(diff_lats))
    print('Lateral median: (init, pred)', np.median(np.abs(gt_lats)), np.median(diff_lats))

    print('Longitudinal average: (init, pred)', np.mean(np.abs(gt_lons)), np.mean(diff_lons))
    print('Longitudinal median: (init, pred)', np.median(np.abs(gt_lons)), np.median(diff_lons))


    for idx in range(len(metrics)):
        pred = np.sum(distance < metrics[idx]) / distance.shape[0] * 100
        init = np.sum(init_dis < metrics[idx]) / init_dis.shape[0] * 100

        line = 'distance within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    print('-------------------------')
    f.write('------------------------\n')

    for idx in range(len(metrics)):
        pred = np.sum(diff_lats < metrics[idx]) / diff_lats.shape[0] * 100
        init = np.sum(np.abs(gt_lats) < metrics[idx]) / gt_lats.shape[0] * 100

        line = 'lateral      within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

        pred = np.sum(diff_lons < metrics[idx]) / diff_lons.shape[0] * 100
        init = np.sum(np.abs(gt_lons) < metrics[idx]) / gt_lons.shape[0] * 100

        line = 'longitudinal within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    print('====================================')
    f.write('====================================\n')
    f.close()

    net.train()
    return


def train(args, save_path, train_log_start=1, train_log_end=2):

    for epoch in range(args.resume, args.epochs):
        net.train()

        base_lr = 1e-4
        if epoch >= 2:
            base_lr = 1e-5

        optimizer = optim.Adam(net.parameters(), lr=base_lr)
        optimizer.zero_grad()

        train_set = SatGrdDatasetFord(logs=train_logs[train_log_start:train_log_end],
                                      logs_img_inds=train_logs_img_inds[train_log_start:train_log_end],
                                      shift_range_lat=args.shift_range_lat,
                                      shift_range_lon=args.shift_range_lon,
                                      rotation_range=args.rotation_range,
                                      whole=args.train_whole)
        trainloader = DataLoader(train_set, batch_size=mini_batch, shuffle=True, pin_memory=True,
                                 num_workers=1, drop_last=False)
        print('batch_size:', mini_batch, '\n num of batches:', len(trainloader))

        for Loop, Data in enumerate(trainloader, 0):
            sat_map, grd_img, gt_shift_u, gt_shift_v, theta, R_FL, T_FL = \
                [item.to(device) for item in Data[:-1]]

            optimizer.zero_grad()

            if args.proj == 'CrossAttn':
                loss = net.CrossAttn_corr(sat_map, grd_img, R_FL, T_FL, gt_shift_u, gt_shift_v, theta, mode='train', epoch=epoch)

            loss.backward()
            optimizer.step()  # This step is responsible for updating weights
            optimizer.zero_grad()

            if Loop % 10 == 9:  #

                print('Epoch: ' + str(epoch) + ' Loop: ' + str(Loop) +
                      ' triplet loss: ' + str(np.round(loss.item(), decimals=4))
                      )

        print('Save Model ...')

        if not os.path.exists(save_path):
            os.makedirs(save_path)

        torch.save(net.state_dict(), os.path.join(save_path, 'model_' + str(epoch) + '.pth'))
        test1(net, args, save_path, test_log_ind=train_log_start, epoch=epoch)

    print('Finished Training')


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--resume', type=int, default=0, help='resume the trained model')
    parser.add_argument('--test', type=int, default=1, help='test with trained model')

    parser.add_argument('--epochs', type=int, default=5, help='number of training epochs')

    parser.add_argument('--lr', type=float, default=1e-4, help='learning rate')

    parser.add_argument('--rotation_range', type=float, default=0., help='degree')
    parser.add_argument('--shift_range_lat', type=float, default=20., help='meters')
    parser.add_argument('--shift_range_lon', type=float, default=20., help='meters')

    parser.add_argument('--batch_size', type=int, default=3, help='batch size')
    parser.add_argument('--level', type=int, default=3, help='2, 3, 4, -1, -2, -3, -4')
    parser.add_argument('--N_iters', type=int, default=2, help='any integer')

    parser.add_argument('--Optimizer', type=str, default='TransV1G2SP', help='it does not matter in the orientation-aligned setting')

    parser.add_argument('--train_log_start', type=int, default=0, help='')
    parser.add_argument('--train_log_end', type=int, default=1, help='')
    parser.add_argument('--test_log_ind', type=int, default=0, help='')

    parser.add_argument('--proj', type=str, default='CrossAttn', help='geo, CrossAttn')

    parser.add_argument('--train_whole', type=int, default=0, help='0 or 1')
    parser.add_argument('--test_whole', type=int, default=0, help='0 or 1')

    parser.add_argument('--use_uncertainty', type=int, default=1, help='0 or 1')

    args = parser.parse_args()

    return args


def getSavePath(args):
    save_path = './ModelsFord/2DoF/' \
                + 'Log_' + str(args.train_log_start+1) + 'lat' + str(args.shift_range_lat) + 'm_lon' + str(args.shift_range_lon) + 'm' \
                + '_' + str(args.proj)

    if args.use_uncertainty:
        save_path = save_path + '_Uncertainty'

    if not os.path.exists(save_path):
        os.makedirs(save_path)

    print('save_path:', save_path)

    return save_path



if __name__ == '__main__':

    if torch.cuda.is_available():
        device = torch.device("cuda:0")
    else:
        device = torch.device("cpu")

    np.random.seed(2022)

    args = parse_args()

    mini_batch = args.batch_size

    save_path = getSavePath(args)

    net = ModelFord(args)
    net.to(device)

    if args.test:
        net.load_state_dict(torch.load(os.path.join(save_path, 'model_4.pth')), strict=False)
        test1(net, args, save_path, args.train_log_start, device=device)

    else:

        if args.resume:
            net.load_state_dict(torch.load(os.path.join(save_path, 'model_' + str(args.resume - 1) + '.pth')))
            print("resume from " + 'model_' + str(args.resume - 1) + '.pth')

        train(args, save_path, train_log_start=args.train_log_start, train_log_end=args.train_log_end)

================
File: models_ford.py
================
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import torch
from torchvision import transforms
import utils
import os
import torchvision.transforms.functional as TF

from VGG import VGGUnet, Encoder, Decoder2, Decoder4, VGGUnetTwoDec, Decoder
from jacobian import grid_sample

# from models_kitti import normalize_feature
# from transformer import LocalFeatureTransformer
# from position_encoding import PositionEncoding, PositionEncodingSine
from RNNs import NNrefine, Uncertainty
from swin_transformer import TransOptimizerS2GP_V1, TransOptimizerG2SP_V1
from swin_transformer_cross import TransOptimizerG2SP, TransOptimizerG2SPV2
from cross_attention import CrossViewAttention

EPS = utils.EPS



class ModelFord(nn.Module):
    def __init__(self, args):  # device='cuda:0',
        super(ModelFord, self).__init__()
        '''
        loss_method: 0: direct R T loss 1: feat loss 2: noise aware feat loss
        '''
        self.args = args

        self.level = args.level
        self.N_iters = args.N_iters

        self.SatFeatureNet = VGGUnet(self.level)

        if self.args.proj == 'CrossAttn':
            self.GrdEnc = Encoder()
            self.GrdDec = Decoder()
            self.Dec4 = Decoder4()
            self.Dec2 = Decoder2()
            self.CVattn = CrossViewAttention(blocks=2, dim=256, heads=4, dim_head=16, qkv_bias=False)

        else:
            self.GrdFeatureNet = VGGUnet(self.level)

        self.TransRefine = TransOptimizerG2SP_V1()

        if self.args.rotation_range > 0:
            self.coe_R = nn.Parameter(torch.tensor(-5., dtype=torch.float32), requires_grad=True)
            self.coe_T = nn.Parameter(torch.tensor(-3., dtype=torch.float32), requires_grad=True)

        if self.args.use_uncertainty:
            self.uncertain_net = Uncertainty()

        ori_grdH = 256
        ori_grdW = 1024
        ori_A = 512
        self.ori_A = ori_A
        xyz_w = []
        K_FLs = []
        for level in range(4):
            A = ori_A / (2 ** (3 - level))
            xyz = self.sat2world(A)  # [1, grd_H, grd_W, 3] under the grd camera coordinates
            xyz_w.append(xyz)

            grd_H, grd_W = ori_grdH / (2 ** (3 - level)), ori_grdW / (2 ** (3 - level))
            K_FL = self.get_K_FL(grd_H, grd_W, ori_grdH, ori_grdW)
            K_FLs.append(K_FL)

        self.xyz_w = xyz_w

        self.K_FLs = K_FLs  # [1, 3, 3]

        meter_per_pixel = 0.22  # this is fixed for the ford dataset
        self.meters_per_pixel = []
        for level in range(4):
            self.meters_per_pixel.append(meter_per_pixel * (2 ** (3 - level)))


        torch.autograd.set_detect_anomaly(True)
        # Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function.
        # Any backward computation that generate nan value will raise an error.

    def ori_K_FL(self):
        K_FL = torch.tensor([945.391406, 0.0, 855.502825, 0.0, 945.668274, 566.372868, 0.0, 0.0, 1.0],
                            dtype=torch.float32, requires_grad=True).reshape(1, 3, 3)
        # Original image resolution
        H_FL = 860
        W_FL = 1656

        # Network input image resolution
        H = 256
        W = 1024

        ori_camera_k = torch.zeros_like(K_FL)

        ori_camera_k[0, 0] = K_FL[0, 0] / W_FL * W
        ori_camera_k[0, 1] = K_FL[0, 1] / H_FL * H
        ori_camera_k[0, 2] = K_FL[0, 2]

        return ori_camera_k

    def get_K_FL(self, grd_H, grd_W, ori_grdH, ori_grdW):
        ori_camera_k = self.ori_K_FL()

        camera_k = ori_camera_k.clone()
        camera_k[:, :1, :] = ori_camera_k[:, :1,
                             :] * grd_W / ori_grdW  # original size input into feature get network/ output of feature get network
        camera_k[:, 1:2, :] = ori_camera_k[:, 1:2, :] * grd_H / ori_grdH

        return camera_k

    def sat2world(self, satmap_sidelength):
        # realword: X: North, Y:East, Z: Down   origin is set to the height of camera

        i = j = torch.arange(0, satmap_sidelength).cuda()  # to(self.device)
        ii, jj = torch.meshgrid(i, j)  # i:h,j:w

        uv = torch.stack([jj, ii], dim=-1).float()  # shape = [satmap_sidelength, satmap_sidelength, 2]

        u0 = v0 = satmap_sidelength // 2
        uv_center = uv - torch.tensor([u0, v0]).cuda()

        meter_per_pixel = 0.22  # this is fixed for the ford dataset
        meter_per_pixel *= self.ori_A / satmap_sidelength
        R = torch.tensor([[0, -1], [1, 0]]).float().cuda()
        Aff_sat2real = meter_per_pixel * R  # shape = [2,2]

        XY = torch.einsum('ij, hwj -> hwi', Aff_sat2real,
                          uv_center)  # shape = [satmap_sidelength, satmap_sidelength, 2]

        Z = torch.ones_like(XY[..., :1])

        XYZ = torch.cat([XY, Z], dim=-1).unsqueeze(dim=0)  # [1, satmap_sidelength, satmap_sidelength, 3]

        return XYZ

    def World2GrdImgPixCoordinates(self, R_FL, T_FL, shift_u, shift_v, theta, level):
        B = shift_u.shape[0]
        Xw = self.xyz_w[level].detach().to(shift_u.device).repeat(B, 1, 1, 1)

        shift_u_meters = self.args.shift_range_lat * shift_u
        shift_v_meters = self.args.shift_range_lon * shift_v
        Tw = torch.cat([-shift_v_meters, shift_u_meters, torch.zeros_like(shift_v_meters)], dim=-1)  # [B, 3]

        yaw = theta * self.args.rotation_range / 180 * np.pi
        cos = torch.cos(yaw)
        sin = torch.sin(yaw)
        zeros = torch.zeros_like(cos)
        ones = torch.ones_like(cos)
        Rw = torch.cat([cos, -sin, zeros, sin, cos, zeros, zeros, zeros, ones], dim=-1)  # shape = [B, 9]
        Rw = Rw.view(B, 3, 3)  # shape = [B, 3, 3]

        Xb = torch.sum(Rw[:, None, None, :, :] * Xw[:, :, :, None, :], dim=-1) + Tw[:, None, None, :]

        K_FL = self.K_FLs[level].detach().to(shift_u.device).repeat(B, 1, 1)
        R_FL_inv = torch.inverse(R_FL)
        KR_FL = torch.matmul(K_FL, R_FL_inv)
        uvw = torch.sum(KR_FL[:, None, None, :, :] * (Xb[:, :, :, None, :] - T_FL[:, None, None, None, :]), dim=-1)
        # [B, H, W, 3]

        # Xm = Xb[:, :, :, None, :] - T_FL[:, None, None, None, :]

        denominator = torch.maximum(uvw[:, :, :, 2:], torch.ones_like(uvw[:, :, :, 2:]) * 1e-6)
        uv = uvw[..., :2] / denominator

        H, W = uv.shape[1:-1]
        assert(H==W)

        mask = torch.greater(denominator, torch.ones_like(uvw[:, :, :, 2:]) * 1e-6)
        uv = uv * mask

        return uv, mask[:, :, :, 0]

    def project_grd_to_sat(self, grd_f, grd_c, R_FL, T_FL, shift_u, shift_v, theta, level):
        '''
        Args:
            grd_f: [B, C, H, W]
            grd_c: [B, 1, H, W]
            R_FL: [B, 3, 3]
            T_FL: [B, 3]
            shift_u: [B, 1]
            shift_v: [B, 1]
            theta: [B, 1]
            level: scalar, feature level
        '''

        uv, mask = self.World2GrdImgPixCoordinates(R_FL, T_FL, shift_u, shift_v, theta, level)

        grd_f_trans, _ = grid_sample(grd_f, uv, jac=None)
        # [B, C, H, W], [3, B, C, H, W]

        grd_f_trans = grd_f_trans * mask[:, None, :, :]

        if grd_c is not None:
            grd_c_trans, _ = grid_sample(grd_c, uv)  # [B, 1, H, W]
            grd_c_trans = grd_c_trans * mask[:, None, :, :]
        else:
            grd_c_trans = None

        return grd_f_trans, grd_c_trans, uv * mask[:, :, :, None], mask

    def Trans_update(self, shift_u, shift_v, theta, grd_feat_proj, sat_feat):
        B = shift_u.shape[0]
        grd_feat_norm = torch.norm(grd_feat_proj.reshape(B, -1), p=2, dim=-1)
        grd_feat_norm = torch.maximum(grd_feat_norm, 1e-6 * torch.ones_like(grd_feat_norm))
        grd_feat_proj = grd_feat_proj / grd_feat_norm[:, None, None, None]

        delta = self.TransRefine(grd_feat_proj, sat_feat)  # [B, 3]

        shift_u_new = shift_u + delta[:, 0:1]
        shift_v_new = shift_v + delta[:, 1:2]
        heading_new = theta + delta[:, 2:3]

        B = shift_u.shape[0]

        rand_u = torch.distributions.uniform.Uniform(-1, 1).sample([B, 1]).to(shift_u.device)
        rand_v = torch.distributions.uniform.Uniform(-1, 1).sample([B, 1]).to(shift_u.device)
        rand_u.requires_grad = True
        rand_v.requires_grad = True
        shift_u_new = torch.where((shift_u_new > -2.5) & (shift_u_new < 2.5), shift_u_new, rand_u)
        shift_v_new = torch.where((shift_v_new > -2.5) & (shift_v_new < 2.5), shift_v_new, rand_v)
        # shift_u_new = torch.where((shift_u_new > -2) & (shift_u_new < 2), shift_u_new, rand_u)
        # shift_v_new = torch.where((shift_v_new > -2) & (shift_v_new < 2), shift_v_new, rand_v)

        return shift_u_new, shift_v_new, heading_new

    def rot_corr(self, sat_map, grd_img_left, R_FL, T_FL, gt_shift_u=None, gt_shift_v=None, gt_theta=None, mode='train', epoch=None):
        '''
        :param sat_map: [B, C, A, A] A--> sidelength
        :param grd_img_left: [B, C, H, W]
        :return:
        '''

        B, _, ori_grdH, ori_grdW = grd_img_left.shape

        sat_feat_list, sat_conf_list = self.SatFeatureNet(sat_map)
        if self.args.use_uncertainty:
            sat_uncer_list = self.uncertain_net(sat_feat_list)

        grd_feat_list, grd_conf_list = self.GrdFeatureNet(grd_img_left)

        shift_u = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        shift_v = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        theta = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)

        shift_us_all = []
        shift_vs_all = []
        thetas_all = []
        for iter in range(self.N_iters):
            shift_us = []
            shift_vs = []
            thetas = []
            for level in range(len(sat_feat_list)):
                sat_feat = sat_feat_list[level]
                grd_feat = grd_feat_list[level]

                grd_feat_proj, _, grd_uv, mask = self.project_grd_to_sat(
                    grd_feat, None, R_FL, T_FL, shift_u, shift_v, theta, level)
                # [B, C, H, W], [B, 1, H, W], [B, H, W, 2]

                shift_u_new, shift_v_new, theta_new = self.Trans_update(shift_u, shift_v, theta,
                                                                     grd_feat_proj,
                                                                     sat_feat)

                shift_us.append(shift_u_new[:, 0])  # [B]
                shift_vs.append(shift_v_new[:, 0])  # [B]
                thetas.append(theta_new[:, 0])  # [B]

                shift_u = shift_u_new.clone()
                shift_v = shift_v_new.clone()
                theta = theta_new.clone()

            shift_us_all.append(torch.stack(shift_us, dim=1))  # [B, Level]
            shift_vs_all.append(torch.stack(shift_vs, dim=1))  # [B, Level]
            thetas_all.append(torch.stack(thetas, dim=1))  # [B, Level]

        shift_lats = torch.stack(shift_us_all, dim=1)  # [B, N_iters, Level]
        shift_lons = torch.stack(shift_vs_all, dim=1)  # [B, N_iters, Level]
        thetas = torch.stack(thetas_all, dim=1)  # [B, N_iters, Level]

        def corr(sat_feat_list, grd_feat_list, gt_shift_u=None, gt_shift_v=None, gt_heading=None,
                 pred_heading=None, mode='train'):

            B, _, ori_grdH, ori_grdW = grd_img_left.shape

            shift_u = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
            shift_v = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)

            corr_maps = []

            for level in range(len(sat_feat_list)):
                meter_per_pixel = self.meters_per_pixel[level]

                sat_feat = sat_feat_list[level]
                grd_feat = grd_feat_list[level]

                A = sat_feat.shape[-1]
                if mode == 'train':
                    theta = gt_heading[:, None] #+ np.random.uniform(-0.1, 0.1)
                else:
                    theta = pred_heading

                grd_feat_proj, _, grd_uv, mask = self.project_grd_to_sat(
                    grd_feat, None, R_FL, T_FL, shift_u, shift_v, theta, level)
                # [B, C, H, W], [B, 1, H, W], [3, B, C, H, W], [B, H, W, 2]

                crop_H = int(A - self.args.shift_range_lat * 3 / meter_per_pixel)
                crop_W = int(A - self.args.shift_range_lon * 3 / meter_per_pixel)
                g2s_feat = TF.center_crop(grd_feat_proj, [crop_H, crop_W])
                g2s_feat = F.normalize(g2s_feat.reshape(B, -1)).reshape(B, -1, crop_H, crop_W)

                s_feat = sat_feat.reshape(1, -1, A, A)  # [B, C, H, W]->[1, B*C, H, W]

                corr = F.conv2d(s_feat, g2s_feat, groups=B)[0]  # [B, H, W]

                denominator = F.avg_pool2d(sat_feat.pow(2), (crop_H, crop_W), stride=1, divisor_override=1)  # [B, 4W]
                if self.args.use_uncertainty:
                    denominator = torch.sum(denominator, dim=1) * TF.center_crop(sat_uncer_list[level], [corr.shape[1], corr.shape[2]])[:, 0]
                else:
                    denominator = torch.sum(denominator, dim=1)  # [B, H, W]

                denominator = torch.maximum(torch.sqrt(denominator), torch.ones_like(denominator) * 1e-6)
                corr = 2 - 2 * corr / denominator

                B, corr_H, corr_W = corr.shape

                corr_maps.append(corr)

                max_index = torch.argmin(corr.reshape(B, -1), dim=1)
                pred_u = (max_index % corr_W - corr_W / 2) * meter_per_pixel  # / self.args.shift_range_lon
                pred_v = (max_index // corr_W - corr_H / 2) * meter_per_pixel  # / self.args.shift_range_lat

                cos = torch.cos(gt_heading * self.args.rotation_range / 180 * np.pi)
                sin = torch.sin(gt_heading * self.args.rotation_range / 180 * np.pi)

                pred_u1 = - pred_u * cos + pred_v * sin
                pred_v1 = - pred_u * sin - pred_v * cos

            if mode == 'train':
                return self.triplet_loss(corr_maps, gt_shift_u, gt_shift_v, gt_heading)
            else:
                return pred_u1, pred_v1  # [B], [B]

        if mode == 'train':

            if self.args.rotation_range == 0:
                coe_heading = 0
            else:
                coe_heading = self.args.coe_heading

            loss, loss_decrease, shift_lat_decrease, shift_lon_decrease, thetas_decrease, loss_last, \
            shift_lat_last, shift_lon_last, theta_last, \
                = loss_func(shift_lats, shift_lons, thetas, gt_shift_u, gt_shift_v, gt_theta,
                            self.args.coe_shift_lat, self.args.coe_shift_lon, coe_heading)

            trans_loss = corr(sat_feat_list, grd_feat_list, gt_shift_u, gt_shift_v, gt_theta,
                              thetas[:, -1, -1:], mode)

            return loss, loss_decrease, shift_lat_decrease, shift_lon_decrease, thetas_decrease, loss_last, \
                   shift_lat_last, shift_lon_last, theta_last, \
                   grd_conf_list, trans_loss
        else:
            pred_u, pred_v = corr(sat_feat_list, grd_feat_list, gt_shift_u, gt_shift_v, gt_theta,
                                  thetas[:, -1, -1:], mode)
            pred_orien = thetas[:, -1, -1]

            return pred_u, pred_v, pred_orien

    def CrossAttn_rot_corr(self, sat_map, grd_img_left, R_FL, T_FL, gt_shift_u=None, gt_shift_v=None, gt_theta=None, mode='train', epoch=None,grd_name=None):

        B, _, ori_grdH, ori_grdW = grd_img_left.shape

        sat_feat_list, sat_conf_list = self.SatFeatureNet(sat_map)
        if self.args.use_uncertainty:
            sat_uncer_list = self.uncertain_net(sat_feat_list)

        grd8, grd4, grd2 = self.GrdEnc(grd_img_left)
        # [H/8, W/8] [H/4, W/4] [H/2, W/2]
        grd_feat_list = self.GrdDec(grd8, grd4, grd2)

        shift_u = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        shift_v = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        theta = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)

        shift_us_all = []
        shift_vs_all = []
        thetas_all = []
        for iter in range(self.N_iters):
            shift_us = []
            shift_vs = []
            thetas = []
            for level in range(len(sat_feat_list)):
                sat_feat = sat_feat_list[level]
                grd_feat = grd_feat_list[level]

                grd_feat_proj, grd_conf_proj, grd_uv, mask = self.project_grd_to_sat(
                    grd_feat, None, R_FL, T_FL, shift_u, shift_v, theta, level)
                # [B, C, H, W], [B, 1, H, W], [B, H, W, 2]

                shift_u_new, shift_v_new, theta_new = self.Trans_update(shift_u, shift_v, theta, grd_feat_proj, sat_feat)

                shift_us.append(shift_u_new[:, 0])  # [B]
                shift_vs.append(shift_v_new[:, 0])  # [B]
                thetas.append(theta_new[:, 0])  # [B]

                shift_u = shift_u_new.clone()
                shift_v = shift_v_new.clone()
                theta = theta_new.clone()

            shift_us_all.append(torch.stack(shift_us, dim=1))  # [B, Level]
            shift_vs_all.append(torch.stack(shift_vs, dim=1))  # [B, Level]
            thetas_all.append(torch.stack(thetas, dim=1))  # [B, Level]

        shift_lats = torch.stack(shift_us_all, dim=1)  # [B, N_iters, Level]
        shift_lons = torch.stack(shift_vs_all, dim=1)  # [B, N_iters, Level]
        thetas = torch.stack(thetas_all, dim=1)  # [B, N_iters, Level]

        def corr(sat_feat_list, grd_feat_list, gt_shift_u=None, gt_shift_v=None, gt_heading=None, mode='train'):
            '''
            Args:
                sat_map: [B, C, A, A] A--> sidelength
                left_camera_k: [B, 3, 3]
                grd_img_left: [B, C, H, W]
                gt_shift_u: [B, 1] u->longitudinal
                gt_shift_v: [B, 1] v->lateral
                gt_heading: [B, 1] east as 0-degree
                mode:
                file_name:

            Returns:

            '''

            B, _, ori_grdH, ori_grdW = grd_img_left.shape

            corr_maps = []
            # print("agrs: ",args)
            for level in range(len(sat_feat_list)):
                meter_per_pixel = self.meters_per_pixel[level]

                sat_feat = sat_feat_list[level]
                grd_feat = grd_feat_list[level]

                A = sat_feat.shape[-1]

                crop_H = int(A - self.args.shift_range_lat * 3 / meter_per_pixel)
                crop_W = int(A - self.args.shift_range_lon * 3 / meter_per_pixel)
                g2s_feat = TF.center_crop(grd_feat, [crop_H, crop_W])
                g2s_feat = F.normalize(g2s_feat.reshape(B, -1)).reshape(B, -1, crop_H, crop_W)

                s_feat = sat_feat.reshape(1, -1, A, A)  # [B, C, H, W]->[1, B*C, H, W]

                corr = F.conv2d(s_feat, g2s_feat, groups=B)[0]  # [B, H, W]

                denominator = F.avg_pool2d(sat_feat.pow(2), (crop_H, crop_W), stride=1, divisor_override=1)  # [B, 4W]
                if self.args.use_uncertainty:
                    denominator = torch.sum(denominator, dim=1) * TF.center_crop(sat_uncer_list[level], [corr.shape[1], corr.shape[2]])[:, 0]
                else:
                    denominator = torch.sum(denominator, dim=1)  # [B, H, W]
                # denominator = torch.sum(denominator, dim=1)  # [B, H, W]
                denominator = torch.maximum(torch.sqrt(denominator), torch.ones_like(denominator) * 1e-6)
                corr = 2 - 2 * corr / denominator

                B, corr_H, corr_W = corr.shape

                corr_maps.append(corr)
                # print("meter per pixel: ",self.meters_per_pixel[-1])

                max_index = torch.argmin(corr.reshape(B, -1), dim=1)
                pred_u = (max_index % corr_W - corr_W / 2) * meter_per_pixel  # / self.args.shift_range_lon
                pred_v = (max_index // corr_W - corr_H / 2) * meter_per_pixel  # / self.args.shift_range_lat

                cos = torch.cos(gt_heading * self.args.rotation_range / 180 * np.pi)
                sin = torch.sin(gt_heading * self.args.rotation_range / 180 * np.pi)

                pred_u1 = - pred_u * cos + pred_v * sin
                pred_v1 = - pred_u * sin - pred_v * cos

            if mode == 'train':
                return self.triplet_loss(corr_maps, gt_shift_u, gt_shift_v, gt_heading)
            else:
                return pred_u1, pred_v1  # [B], [B]

        if mode == 'train':

            loss, loss_decrease, shift_lat_decrease, shift_lon_decrease, thetas_decrease, loss_last, \
            shift_lat_last, shift_lon_last, theta_last, \
                = loss_func(shift_lats, shift_lons, thetas, gt_shift_u, gt_shift_v, gt_theta,
                            torch.exp(-self.coe_R), torch.exp(-self.coe_R), torch.exp(-self.coe_R))

            shift_u = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
            shift_v = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)

            grd2sat8, grd_conf_proj, grd_uv, mask = self.project_grd_to_sat(
                grd_feat_list[0], None, R_FL, T_FL, shift_u, shift_v, gt_theta.reshape(B, 1), level=0)
            grd2sat4, grd_conf_proj, _, _ = self.project_grd_to_sat(
                grd_feat_list[1], None, R_FL, T_FL, shift_u, shift_v, gt_theta.reshape(B, 1), level=1)
            grd2sat2, grd_conf_proj, _, _ = self.project_grd_to_sat(
                grd_feat_list[2], None, R_FL, T_FL, shift_u, shift_v, gt_theta.reshape(B, 1), level=2)

            grd2sat8_attn = self.CVattn(grd2sat8, grd8, grd_uv[:, :, :, 0], mask[..., None])
            grd2sat4_attn = grd2sat4 + self.Dec4(grd2sat8_attn, grd2sat4)
            grd2sat2_attn = grd2sat2 + self.Dec2(grd2sat4_attn, grd2sat2)

            grd_feat_list = [grd2sat8_attn, grd2sat4_attn, grd2sat2_attn]

            trans_loss = corr(sat_feat_list, grd_feat_list, gt_shift_u, gt_shift_v, gt_theta, mode)

            return loss, loss_decrease, shift_lat_decrease, shift_lon_decrease, thetas_decrease, loss_last, \
                   shift_lat_last, shift_lon_last, theta_last, \
                   trans_loss
        else:

            shift_u = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
            shift_v = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)

            grd2sat8, grd_conf_proj, grd_uv, mask = self.project_grd_to_sat(
                grd_feat_list[0], None, R_FL, T_FL, shift_u, shift_v, thetas[:, -1, -1:], level=0)
            grd2sat4, grd_conf_proj, _, _ = self.project_grd_to_sat(
                grd_feat_list[1], None, R_FL, T_FL, shift_u, shift_v, thetas[:, -1, -1:], level=1)
            grd2sat2, grd_conf_proj, _, _ = self.project_grd_to_sat(
                grd_feat_list[2], None, R_FL, T_FL, shift_u, shift_v, thetas[:, -1, -1:], level=2)

            grd2sat8_attn = self.CVattn(grd2sat8, grd8, grd_uv[:, :, :, 0], mask[..., None])
            grd2sat4_attn = grd2sat4 + self.Dec4(grd2sat8_attn, grd2sat4)
            grd2sat2_attn = grd2sat2 + self.Dec2(grd2sat4_attn, grd2sat2)

            grd_feat_list = [grd2sat8_attn, grd2sat4_attn, grd2sat2_attn]
            pred_u, pred_v = corr(sat_feat_list, grd_feat_list, gt_shift_u, gt_shift_v, gt_theta, mode)

            pred_orien = thetas[:, -1, -1]

            return pred_u, pred_v, pred_orien

    def CrossAttn_corr(self, sat_map, grd_img_left, R_FL, T_FL, gt_shift_u=None, gt_shift_v=None, gt_theta=None, mode='train', epoch=None):

        B, _, ori_grdH, ori_grdW = grd_img_left.shape

        sat_feat_list, sat_conf_list = self.SatFeatureNet(sat_map)
        if self.args.use_uncertainty:
            sat_uncer_list = self.uncertain_net(sat_feat_list)

        grd8, grd4, grd2 = self.GrdEnc(grd_img_left)
        # [H/8, W/8] [H/4, W/4] [H/2, W/2]
        grd_feat_list = self.GrdDec(grd8, grd4, grd2)

        shift_u = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)
        shift_v = torch.zeros([B, 1], dtype=torch.float32, requires_grad=True, device=sat_map.device)

        grd2sat8, grd_conf_proj, grd_uv, mask = self.project_grd_to_sat(
            grd_feat_list[0], None, R_FL, T_FL, shift_u, shift_v, gt_theta.reshape(B, 1), level=0)
        grd2sat4, grd_conf_proj, _, _ = self.project_grd_to_sat(
            grd_feat_list[1], None, R_FL, T_FL, shift_u, shift_v, gt_theta.reshape(B, 1), level=1)
        grd2sat2, grd_conf_proj, _, _ = self.project_grd_to_sat(
            grd_feat_list[2], None, R_FL, T_FL, shift_u, shift_v, gt_theta.reshape(B, 1), level=2)

        grd2sat8_attn = self.CVattn(grd2sat8, grd8, grd_uv[:, :, :, 0], mask[..., None])
        grd2sat4_attn = grd2sat4 + self.Dec4(grd2sat8_attn, grd2sat4)
        grd2sat2_attn = grd2sat2 + self.Dec2(grd2sat4_attn, grd2sat2)

        grd_feat_list = [grd2sat8_attn, grd2sat4_attn, grd2sat2_attn]

        corr_maps = []

        for level in range(len(sat_feat_list)):
            meter_per_pixel = self.meters_per_pixel[level]

            sat_feat = sat_feat_list[level]
            grd_feat = grd_feat_list[level]

            A = sat_feat.shape[-1]

            crop_H = int(A - self.args.shift_range_lat * 3 / meter_per_pixel)
            crop_W = int(A - self.args.shift_range_lon * 3 / meter_per_pixel)
            g2s_feat = TF.center_crop(grd_feat, [crop_H, crop_W])
            g2s_feat = F.normalize(g2s_feat.reshape(B, -1)).reshape(B, -1, crop_H, crop_W)

            s_feat = sat_feat.reshape(1, -1, A, A)  # [B, C, H, W]->[1, B*C, H, W]

            corr = F.conv2d(s_feat, g2s_feat, groups=B)[0]  # [B, H, W]

            denominator = F.avg_pool2d(sat_feat.pow(2), (crop_H, crop_W), stride=1, divisor_override=1)  # [B, 4W]
            if self.args.use_uncertainty:
                denominator = torch.sum(denominator, dim=1) * \
                              TF.center_crop(sat_uncer_list[level], [corr.shape[1], corr.shape[2]])[:, 0]
            else:
                denominator = torch.sum(denominator, dim=1)  # [B, H, W]

            denominator = torch.maximum(torch.sqrt(denominator), torch.ones_like(denominator) * 1e-6)
            corr = 2 - 2 * corr / denominator

            B, corr_H, corr_W = corr.shape

            corr_maps.append(corr)

            max_index = torch.argmin(corr.reshape(B, -1), dim=1)
            pred_u = (max_index % corr_W - corr_W / 2) * meter_per_pixel  # / self.args.shift_range_lon
            pred_v = (max_index // corr_W - corr_H / 2) * meter_per_pixel  # / self.args.shift_range_lat

            cos = torch.cos(gt_theta * self.args.rotation_range / 180 * np.pi)
            sin = torch.sin(gt_theta * self.args.rotation_range / 180 * np.pi)

            pred_u1 = - pred_u * cos + pred_v * sin
            pred_v1 = - pred_u * sin - pred_v * cos

        if mode == 'train':
            return self.triplet_loss(corr_maps, gt_shift_u, gt_shift_v, gt_theta)
        else:
            return pred_u1, pred_v1  # [B], [B]

    def triplet_loss(self, corr_maps, gt_shift_u, gt_shift_v, gt_heading):
        cos = torch.cos(gt_heading * self.args.rotation_range / 180 * np.pi)
        sin = torch.sin(gt_heading * self.args.rotation_range / 180 * np.pi)

        gt_delta_x = gt_shift_u * self.args.shift_range_lon
        gt_delta_y = gt_shift_v * self.args.shift_range_lat

        gt_delta_x_rot = - gt_delta_x * cos - gt_delta_y * sin
        gt_delta_y_rot = gt_delta_x * sin - gt_delta_y * cos

        losses = []
        for level in range(len(corr_maps)):
            meter_per_pixel = self.meters_per_pixel[level]

            corr = corr_maps[level]
            B, corr_H, corr_W = corr.shape

            w = torch.round(corr_W / 2 - 0.5 + gt_delta_x_rot / meter_per_pixel)
            h = torch.round(corr_H / 2 - 0.5 + gt_delta_y_rot / meter_per_pixel)

            pos = corr[range(B), h.long(), w.long()]  # [B]
            pos_neg = pos.reshape(-1, 1, 1) - corr  # [B, H, W]
            loss = torch.sum(torch.log(1 + torch.exp(pos_neg * 10))) / (B * (corr_H * corr_W - 1))
            # import pdb; pdb.set_trace()
            losses.append(loss)

        return torch.sum(torch.stack(losses, dim=0))



def loss_func(shift_lats, shift_lons, thetas,
              gt_shift_lat, gt_shift_lon, gt_theta,
              coe_shift_lat=100, coe_shift_lon=100, coe_theta=100):
    '''
    Args:
        loss_method:
        ref_feat_list:
        pred_feat_dict:
        gt_feat_dict:
        shift_lats: [B, N_iters, Level]
        shift_lons: [B, N_iters, Level]
        thetas: [B, N_iters, Level]
        gt_shift_lat: [B]
        gt_shift_lon: [B]
        gt_theta: [B]
        pred_uv_dict:
        gt_uv_dict:
        coe_shift_lat:
        coe_shift_lon:
        coe_theta:
        coe_L1:
        coe_L2:
        coe_L3:
        coe_L4:

    Returns:

    '''
    B = gt_shift_lat.shape[0]
    # shift_lats = torch.stack(shift_lats_all, dim=1)  # [B, N_iters, Level]
    # shift_lons = torch.stack(shift_lons_all, dim=1)  # [B, N_iters, Level]
    # thetas = torch.stack(thetas_all, dim=1)  # [B, N_iters, Level]

    shift_lat_delta0 = torch.abs(shift_lats - gt_shift_lat[:, None, None])  # [B, N_iters, Level]
    shift_lon_delta0 = torch.abs(shift_lons - gt_shift_lon[:, None, None])  # [B, N_iters, Level]
    thetas_delta0 = torch.abs(thetas - gt_theta[:, None, None])  # [B, N_iters, level]

    shift_lat_delta = torch.mean(shift_lat_delta0, dim=0)  # [N_iters, Level]
    shift_lon_delta = torch.mean(shift_lon_delta0, dim=0)  # [N_iters, Level]
    thetas_delta = torch.mean(thetas_delta0, dim=0)  # [N_iters, level]

    shift_lat_decrease = shift_lat_delta[0] - shift_lat_delta[-1]  # [level]
    shift_lon_decrease = shift_lon_delta[0] - shift_lon_delta[-1]  # [level]
    thetas_decrease = thetas_delta[0] - thetas_delta[-1]  # [level]

    losses = coe_shift_lat * shift_lat_delta + coe_shift_lon * shift_lon_delta + coe_theta * thetas_delta  # [N_iters, level]
    loss_decrease = losses[0] - losses[-1]  # [level]
    loss = torch.mean(losses)  # mean or sum
    loss_last = losses[-1]

    return loss, loss_decrease, shift_lat_decrease, shift_lon_decrease, thetas_decrease, loss_last, \
            shift_lat_delta[-1], shift_lon_delta[-1], thetas_delta[-1]
        

def normalize_feature(x):
    C, H, W = x.shape[-3:]
    norm = torch.norm(x.flatten(start_dim=-3), dim=-1)
    return x / norm[..., None, None, None]

================
File: train_vigor.py
================
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# from logging import _Level
import os

import torchvision.utils

# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
# os.environ['CUDA_VISIBLE_DEVICES'] = '1'

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms
from dataLoader.Vigor_dataset import load_vigor_data
from torch.utils.tensorboard import SummaryWriter
import torch.nn.functional as F
import scipy.io as scio

import ssl

ssl._create_default_https_context = ssl._create_unverified_context  # for downloading pretrained VGG weights

from model_vigor import ModelVIGOR

import numpy as np
import os
import argparse


def train(net, args):

    for epoch in range(args.resume, args.epochs):
        net.train()

        trainloader, valloader = load_vigor_data(args.batch_size, area=args.area, rotation_range=args.rotation_range,
                                                 train=True, weak_supervise=True)

        for Loop, Data in enumerate(trainloader, 0):
            # get the inputs

            # sat_map, grd_left_imgs, gt_shift_u, gt_shift_v = [item.to(device) for item in Data]
            grd, sat, gt_shift_u, gt_shift_v, gt_rot, meter_per_pixel = [item.to(device) for item in Data]
            # city = Data[-1]

            net.forward_projImg(sat, grd, meter_per_pixel, gt_shift_u, gt_shift_v, gt_rot, mode='train')

            break

        break

    print('Finished Training')


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--resume', type=int, default=0, help='resume the trained model')
    parser.add_argument('--test', type=int, default=0, help='test with trained model')
    parser.add_argument('--debug', type=int, default=0, help='debug to dump middle processing images')

    parser.add_argument('--epochs', type=int, default=20, help='number of training epochs')

    parser.add_argument('--rotation_range', type=float, default=180., help='degree')

    # parser.add_argument('--coe_shift_lat', type=float, default=0., help='meters')
    # parser.add_argument('--coe_shift_lon', type=float, default=0., help='meters')
    # parser.add_argument('--coe_heading', type=float, default=0., help='degree')

    parser.add_argument('--coe_triplet', type=float, default=1., help='degree')

    parser.add_argument('--batch_size', type=int, default=4, help='batch size')

    parser.add_argument('--N_iters', type=int, default=2, help='any integer')

    parser.add_argument('--direction', type=str, default='G2SP', help='G2SP' or 'S2GP')

    parser.add_argument('--Optimizer', type=str, default='TransV1', help='LM or SGD')

    parser.add_argument('--proj', type=str, default='CrossAttn', help='geo, polar, nn, CrossAttn')

    parser.add_argument('--use_uncertainty', type=int, default=1, help='0 or 1')

    parser.add_argument('--area', type=str, default='cross', help='same or cross')
    parser.add_argument('--multi_gpu', type=int, default=0, help='0 or 1')

    args = parser.parse_args()

    return args


def getSavePath(args):
    save_path = './ModelsVigor/Corr2D_' + str(args.direction) \
                + '_' + str(args.proj) + '_' + args.area

    if args.use_uncertainty:
        save_path = save_path + '_Uncertainty'

    print('save_path:', save_path)

    return save_path


if __name__ == '__main__':

    if torch.cuda.is_available():
        device = torch.device("cuda:0")
    else:
        device = torch.device("cpu")

    np.random.seed(2022)

    args = parse_args()

    mini_batch = args.batch_size

    save_path = getSavePath(args)

    net = ModelVIGOR(args)
    if args.multi_gpu:
        net = nn.DataParallel(net, dim=0)

    ### cudaargs.epochs, args.debug)
    net.to(device)
    ###########################

    train(net, args)

================
File: VGG.py
================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import torchvision.models as models
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torch

import utils



class VGGUnet(nn.Module):
    def __init__(self, level, estimate_depth=0):
        super(VGGUnet, self).__init__()
        # print('estimate_depth: ', estimate_depth)

        self.level = level

        vgg16 = torchvision.models.vgg16(pretrained=True)

        # load CNN from VGG16, the first three block
        self.conv0 = vgg16.features[0]
        self.conv2 = vgg16.features[2]  # \\64
        self.conv5 = vgg16.features[5]  #
        self.conv7 = vgg16.features[7]  # \\128
        self.conv10 = vgg16.features[10]
        self.conv12 = vgg16.features[12]
        self.conv14 = vgg16.features[14]  # \\256

        self.conv_dec1 = nn.Sequential(
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv14.out_channels + self.conv7.out_channels, self.conv7.out_channels, kernel_size=(3, 3),
                      stride=(1, 1), padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv7.out_channels, self.conv7.out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1,
                      bias=False),
        )

        self.conv_dec2 = nn.Sequential(
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv7.out_channels + self.conv2.out_channels, self.conv2.out_channels, kernel_size=(3, 3),
                      stride=(1, 1), padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv2.out_channels, self.conv2.out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1,
                      bias=False)
        )

        self.conv_dec3 = nn.Sequential(
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv2.out_channels + self.conv2.out_channels, 32, kernel_size=(3, 3),
                      stride=(1, 1), padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=1,
                      bias=False)
        )

        self.relu = nn.ReLU(inplace=True)
        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False,
                                     return_indices=True)

        self.conf0 = nn.Sequential(
            nn.ReLU(),
            nn.Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
            nn.Sigmoid(),
        )
        self.conf1 = nn.Sequential(
            nn.ReLU(),
            nn.Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
            nn.Sigmoid(),
        )
        self.conf2 = nn.Sequential(
            nn.ReLU(),
            nn.Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
            nn.Sigmoid(),
        )
        self.conf3 = nn.Sequential(
            nn.ReLU(),
            nn.Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
            nn.Sigmoid(),
        )

        self.estimate_depth = estimate_depth

        if estimate_depth:
            self.depth0 = nn.Sequential(
                nn.ReLU(),
                nn.Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
                nn.ReLU(),
                nn.Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
                nn.Tanh(),
            )
            self.depth1 = nn.Sequential(
                nn.ReLU(),
                nn.Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
                nn.ReLU(),
                nn.Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
                nn.Tanh(),
            )
            self.depth2 = nn.Sequential(
                nn.ReLU(),
                nn.Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
                nn.ReLU(),
                nn.Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
                nn.Tanh(),
            )
            self.depth3 = nn.Sequential(
                nn.ReLU(),
                nn.Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
                nn.ReLU(),
                nn.Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
                nn.Tanh(),
            )

            self.depth0[-2].weight.data.zero_()
            self.depth1[-2].weight.data.zero_()
            self.depth2[-2].weight.data.zero_()
            self.depth3[-2].weight.data.zero_()


    def forward(self, x):
        # block0
        x0 = self.conv0(x)
        x1 = self.relu(x0)
        x2 = self.conv2(x1)
        x3, ind3 = self.max_pool(x2)  # [H/2, W/2]

        x4 = self.relu(x3)
        x5 = self.conv5(x4)
        x6 = self.relu(x5)
        x7 = self.conv7(x6)
        x8, ind8 = self.max_pool(x7)  # [H/4, W/4]

        # block2
        x9 = self.relu(x8)
        x10 = self.conv10(x9)
        x11 = self.relu(x10)
        x12 = self.conv12(x11)
        x13 = self.relu(x12)
        x14 = self.conv14(x13)
        x15, ind15 = self.max_pool(x14)  # [H/8, W/8]

        # dec1
        x16 = F.interpolate(x15, [x8.shape[2], x9.shape[3]], mode="nearest")
        x17 = torch.cat([x16, x8], dim=1)
        x18 = self.conv_dec1(x17)  # [H/4, W/4]

        # dec2
        x19 = F.interpolate(x18, [x3.shape[2], x3.shape[3]], mode="nearest")
        x20 = torch.cat([x19, x3], dim=1)
        x21 = self.conv_dec2(x20)  # [H/2, W/2]

        x22 = F.interpolate(x21, [x2.shape[2], x2.shape[3]], mode="nearest")
        x23 = torch.cat([x22, x2], dim=1)
        x24 = self.conv_dec3(x23)  # [H, W]

        # c0 = 1 / (1 + self.conf0(x15))
        # c1 = 1 / (1 + self.conf1(x18))
        # c2 = 1 / (1 + self.conf2(x21))
        c0 = nn.Sigmoid()(-self.conf0(x15))
        c1 = nn.Sigmoid()(-self.conf1(x18))
        c2 = nn.Sigmoid()(-self.conf2(x21))
        c3 = nn.Sigmoid()(-self.conf3(x24))

        if self.estimate_depth:
            # its actually height, not depth
            d0 = process_depth(self.depth0(x15))
            d1 = process_depth(self.depth1(x18))
            d2 = process_depth(self.depth2(x21))
            d3 = process_depth(self.depth3(x24))

        x15 = L2_norm(x15)
        x18 = L2_norm(x18)
        x21 = L2_norm(x21)
        x24 = L2_norm(x24)


        if self.estimate_depth:
            if self.level == -1:
                return [x15], [c0], [d0]
            elif self.level == -2:
                return [x18], [c1], [d1]
            elif self.level == -3:
                return [x21], [c2], [d2]
            elif self.level == 2:
                return [x18, x21], [c1, c2], [d1, d2]
            elif self.level == 3:
                return [x15, x18, x21], [c0, c1, c2], [d0, d1, d2]
            elif self.level == 4:
                return [x15, x18, x21, x24], [c0, c1, c2, c3], [d0, d1, d2, d3]
        else:
            if self.level == -1:
                return [x15], [c0]
            elif self.level == -2:
                return [x18], [c1]
            elif self.level == -3:
                return [x21], [c2]
            elif self.level == 2:
                return [x18, x21], [c1, c2]
            elif self.level == 3:
                return [x15, x18, x21], [c0, c1, c2]
            elif self.level == 4:
                return [x15, x18, x21, x24], [c0, c1, c2, c3]




class VGGUnetTwoDec(nn.Module):
    def __init__(self):
        super(VGGUnetTwoDec, self).__init__()
        # print('estimate_depth: ', estimate_depth)

        vgg16 = torchvision.models.vgg16(pretrained=True)
        vgg16_ = torchvision.models.vgg16(pretrained=True)

        # load CNN from VGG16, the first three block
        self.conv0 = vgg16.features[0]
        self.conv2 = vgg16.features[2]  # \\64
        self.conv5 = vgg16.features[5]  #
        self.conv7 = vgg16.features[7]  # \\128
        self.conv10 = vgg16.features[10]
        self.conv12 = vgg16.features[12]
        self.conv14 = vgg16.features[14]  # \\256
        
        self.conv10_ = vgg16_.features[10]
        self.conv12_ = vgg16_.features[12]
        self.conv14_ = vgg16_.features[14]  # \\256

        self.conv_dec1_1 = nn.Sequential(
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv14.out_channels + self.conv7.out_channels, self.conv7.out_channels, kernel_size=(3, 3),
                      stride=(1, 1), padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv7.out_channels, self.conv7.out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1,
                      bias=False),
        )

        self.conv_dec1_2 = nn.Sequential(
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv7.out_channels + self.conv2.out_channels, self.conv2.out_channels, kernel_size=(3, 3),
                      stride=(1, 1), padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv2.out_channels, self.conv2.out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1,
                      bias=False)
        )
        
        self.conv_dec2_0 = nn.Sequential(
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv14.out_channels, self.conv14.out_channels, kernel_size=(3, 3),
                      stride=(1, 1), padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv14.out_channels, self.conv14.out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1,
                      bias=False),
        )
        
        self.conv_dec2_1 = nn.Sequential(
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv14.out_channels, self.conv7.out_channels, kernel_size=(3, 3),
                      stride=(1, 1), padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv7.out_channels, self.conv7.out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1,
                      bias=False),
        )

        self.conv_dec2_2 = nn.Sequential(
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv7.out_channels, self.conv2.out_channels, kernel_size=(3, 3),
                      stride=(1, 1), padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv2.out_channels, self.conv2.out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1,
                      bias=False)
        )


        self.relu = nn.ReLU(inplace=True)
        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False,
                                     return_indices=True)
       
    def forward(self, x):
        # block0
        x0 = self.conv0(x)
        x1 = self.relu(x0)
        x2 = self.conv2(x1)
        x3, ind3 = self.max_pool(x2)  # [H/2, W/2]

        x4 = self.relu(x3)
        x5 = self.conv5(x4)
        x6 = self.relu(x5)
        x7 = self.conv7(x6)
        x8, ind8 = self.max_pool(x7)  # [H/4, W/4]

        # ========== First decoder ==============
        # block2
        x9 = self.relu(x8)
        x10 = self.conv10(x9)
        x11 = self.relu(x10)
        x12 = self.conv12(x11)
        x13 = self.relu(x12)
        x14 = self.conv14(x13)
        x15, ind15 = self.max_pool(x14)  # [H/8, W/8]
        
        # dec1
        x16 = F.interpolate(x15, [x8.shape[2], x9.shape[3]], mode="nearest")
        x17 = torch.cat([x16, x8], dim=1)
        x18 = self.conv_dec1_1(x17)  # [H/4, W/4]

        # dec2
        x19 = F.interpolate(x18, [x3.shape[2], x3.shape[3]], mode="nearest")
        x20 = torch.cat([x19, x3], dim=1)
        x21 = self.conv_dec1_2(x20)  # [H/2, W/2]

        x15 = L2_norm(x15)
        x18 = L2_norm(x18)
        x21 = L2_norm(x21)
        
        # ========== Second decoder ==============
        
        # block2
        x9_ = x9.detach()
        x10_ = self.conv10_(x9_)
        x11_ = self.relu(x10_)
        x12_ = self.conv12_(x11_)
        x13_ = self.relu(x12_)
        x14_ = self.conv14_(x13_)
        x15_, ind15 = self.max_pool(x14_)  # [H/8, W/8]
        # # dec 0
        # x15_ = self.conv_dec2_0(x15.detach())
        
        # dec1
        x16_ = F.interpolate(x15_, [x8.shape[2], x8.shape[3]], mode="nearest")
        # x17_ = torch.cat([x16_, x8.detach()], dim=1)
        x18_ = self.conv_dec2_1(x16_)  # [H/4, W/4]

        # dec2
        x19_ = F.interpolate(x18_, [x3.shape[2], x3.shape[3]], mode="nearest")
        # x20_ = torch.cat([x19_, x3.detach()], dim=1)
        x21_ = self.conv_dec2_2(x19_)  # [H/2, W/2]

        x15_ = L2_norm(x15_)
        x18_ = L2_norm(x18_)
        x21_ = L2_norm(x21_)

        return [x15, x18, x21], [x15_, x18_, x21_]
            


class Encoder(nn.Module):
    def __init__(self,):
        super(Encoder, self).__init__()
     
        vgg16 = torchvision.models.vgg16(pretrained=True)

        # load CNN from VGG16, the first three block
        self.conv0 = vgg16.features[0]
        self.conv2 = vgg16.features[2]  # \\64
        self.conv5 = vgg16.features[5]  #
        self.conv7 = vgg16.features[7]  # \\128
        self.conv10 = vgg16.features[10]
        self.conv12 = vgg16.features[12]
        self.conv14 = vgg16.features[14]  # \\256

        self.relu = nn.ReLU(inplace=True)
        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False,
                                     return_indices=True)

    def forward(self, x):
        # block0
        x0 = self.conv0(x)
        x1 = self.relu(x0)
        x2 = self.conv2(x1)
        x3, ind3 = self.max_pool(x2)  # [H/2, W/2]

        x4 = self.relu(x3)
        x5 = self.conv5(x4)
        x6 = self.relu(x5)
        x7 = self.conv7(x6)
        x8, ind8 = self.max_pool(x7)  # [H/4, W/4]

        # block2
        x9 = self.relu(x8)
        x10 = self.conv10(x9)
        x11 = self.relu(x10)
        x12 = self.conv12(x11)
        x13 = self.relu(x12)
        x14 = self.conv14(x13)
        x15, ind15 = self.max_pool(x14)  # [H/8, W/8]
           
        return x15, x8, x3


class Decoder(nn.Module):
    def __init__(self,):
        super(Decoder, self).__init__()
        # print('estimate_depth: ', estimate_depth)

        self.conv_dec1 = nn.Sequential(
            nn.ReLU(inplace=True),
            nn.Conv2d(256 + 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
        )

        self.conv_dec2 = nn.Sequential(
            nn.ReLU(inplace=True),
            nn.Conv2d(128 + 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False)
        )


        self.relu = nn.ReLU(inplace=True)
        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False,
                                     return_indices=True)


    def forward(self, x15, x8, x3):


        # dec1
        x16 = F.interpolate(x15, [x8.shape[2], x8.shape[3]], mode="nearest")
        x17 = torch.cat([x16, x8], dim=1)
        x18 = self.conv_dec1(x17)  # [H/4, W/4]

        # dec2
        x19 = F.interpolate(x18, [x3.shape[2], x3.shape[3]], mode="nearest")
        x20 = torch.cat([x19, x3], dim=1)
        x21 = self.conv_dec2(x20)  # [H/2, W/2]

        x15 = L2_norm(x15)
        x18 = L2_norm(x18)
        x21 = L2_norm(x21)

        return [x15, x18, x21]





class Decoder4(nn.Module):
    def __init__(self,):
        super(Decoder4, self).__init__()

        self.conv_dec1 = nn.Sequential(
            nn.ReLU(inplace=True),
            nn.Conv2d(256 + 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
        )

        # self.conv_dec2 = nn.Sequential(
        #     nn.ReLU(inplace=True),
        #     nn.Conv2d(128 + 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
        #     nn.ReLU(inplace=True),
        #     nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False)
        # )

        self.relu = nn.ReLU(inplace=True)
        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False, return_indices=True)

    def forward(self, x15, x8):

        H, W = x15.shape[-2:]

        # dec1
        x16 = F.interpolate(x15, [2*H, 2*W], mode="nearest")
        x17 = torch.cat([x16, x8], dim=1)
        x18 = self.conv_dec1(x17)  # [H/4, W/4]

        x18 = L2_norm(x18)

        return x18


class Decoder2(nn.Module):
    def __init__(self):
        super(Decoder2, self).__init__()

        # self.conv_dec1 = nn.Sequential(
        #     nn.ReLU(inplace=True),
        #     nn.Conv2d(256 + 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
        #     nn.ReLU(inplace=True),
        #     nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
        # )

        self.conv_dec2 = nn.Sequential(
            nn.ReLU(inplace=True),
            nn.Conv2d(128 + 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False)
        )

        self.relu = nn.ReLU(inplace=True)
        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False,
                                     return_indices=True)

    def forward(self, x8, x3):

        H, W = x8.shape[-2:]

        # dec2
        x19 = F.interpolate(x8, [2*H, 2*W], mode="nearest")
        x20 = torch.cat([x19, x3], dim=1)
        x21 = self.conv_dec2(x20)  # [H/2, W/2]

        x21 = L2_norm(x21)

        return x21
  

#
# class Decoder4(nn.Module):
#     def __init__(self,):
#         super(Decoder4, self).__init__()
#
#         self.conv_dec1 = nn.Sequential(
#             nn.ReLU(inplace=True),
#             nn.Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
#         )
#
#         # self.conv_dec2 = nn.Sequential(
#         #     nn.ReLU(inplace=True),
#         #     nn.Conv2d(128 + 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
#         #     nn.ReLU(inplace=True),
#         #     nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False)
#         # )
#
#         self.relu = nn.ReLU(inplace=True)
#         self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False, return_indices=True)
#
#     def forward(self, x15):
#
#         H, W = x15.shape[-2:]
#
#         # dec1
#         x16 = F.interpolate(x15, [2*H, 2*W], mode="nearest")
#         # x17 = torch.cat([x16, x8], dim=1)
#         x18 = self.conv_dec1(x16)  # [H/4, W/4]
#
#         x18 = L2_norm(x18)
#
#         return x18
#
#
# class Decoder2(nn.Module):
#     def __init__(self):
#         super(Decoder2, self).__init__()
#
#         # self.conv_dec1 = nn.Sequential(
#         #     nn.ReLU(inplace=True),
#         #     nn.Conv2d(256 + 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
#         #     nn.ReLU(inplace=True),
#         #     nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
#         # )
#
#         self.conv_dec2 = nn.Sequential(
#             nn.ReLU(inplace=True),
#             nn.Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False)
#         )
#
#         self.relu = nn.ReLU(inplace=True)
#         self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False,
#                                      return_indices=True)
#
#     def forward(self, x8):
#
#         H, W = x8.shape[-2:]
#
#         # dec2
#         x19 = F.interpolate(x8, [2*H, 2*W], mode="nearest")
#         # x20 = torch.cat([x19, x3], dim=1)
#         x21 = self.conv_dec2(x19)  # [H/2, W/2]
#
#         x21 = L2_norm(x21)
#
#         return x21
#


class VGGUnet_G2S(nn.Module):
    def __init__(self, level):
        super(VGGUnet_G2S, self).__init__()
        # print('estimate_depth: ', estimate_depth)

        self.level = level

        vgg16 = torchvision.models.vgg16(pretrained=True)

        # load CNN from VGG16, the first three block
        self.conv0 = vgg16.features[0]
        self.conv2 = vgg16.features[2]  # \\64
        self.conv5 = vgg16.features[5]  #
        self.conv7 = vgg16.features[7]  # \\128
        self.conv10 = vgg16.features[10]
        self.conv12 = vgg16.features[12]
        self.conv14 = vgg16.features[14]  # \\256

        self.conv_dec1 = nn.Sequential(
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv14.out_channels + self.conv7.out_channels, self.conv7.out_channels, kernel_size=(3, 3),
                      stride=(1, 1), padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv7.out_channels, self.conv7.out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1,
                      bias=False),
        )

        self.conv_dec2 = nn.Sequential(
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv7.out_channels + self.conv2.out_channels, self.conv2.out_channels, kernel_size=(3, 3),
                      stride=(1, 1), padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv2.out_channels, self.conv2.out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1,
                      bias=False)
        )

        self.conv_dec3 = nn.Sequential(
            nn.ReLU(inplace=True),
            nn.Conv2d(self.conv2.out_channels + self.conv2.out_channels, 32, kernel_size=(3, 3),
                      stride=(1, 1), padding=1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=1,
                      bias=False)
        )

        self.relu = nn.ReLU(inplace=True)
        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False,
                                     return_indices=True)

        self.conf0 = nn.Sequential(
            nn.ReLU(),
            nn.Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
            nn.Sigmoid(),
        )
        self.conf1 = nn.Sequential(
            nn.ReLU(),
            nn.Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
            nn.Sigmoid(),
        )
        self.conf2 = nn.Sequential(
            nn.ReLU(),
            nn.Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
            nn.Sigmoid(),
        )
        self.conf3 = nn.Sequential(
            nn.ReLU(),
            nn.Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
            nn.Sigmoid(),
        )

    def forward(self, x):
        # block0
        x0 = self.conv0(x)
        x1 = self.relu(x0)
        x2 = self.conv2(x1)
        x3, ind3 = self.max_pool(x2)  # [H/2, W/2]

        B, C, H2, W2 = x2.shape
        x2_ = x2.reshape(B, C, H2 * 2, W2 // 2)

        B, C, H3, W3 = x3.shape
        x3_ = x3.reshape(B, C, H3*2, W3//2)

        x4 = self.relu(x3)
        x5 = self.conv5(x4)
        x6 = self.relu(x5)
        x7 = self.conv7(x6)
        x8, ind8 = self.max_pool(x7)  # [H/4, W/4]

        B, C, H8, W8 = x8.shape
        x8_ = x8.reshape(B, C, H8 * 2, W8 // 2)

        # block2
        x9 = self.relu(x8)
        x10 = self.conv10(x9)
        x11 = self.relu(x10)
        x12 = self.conv12(x11)
        x13 = self.relu(x12)
        x14 = self.conv14(x13)
        x15, ind15 = self.max_pool(x14)  # [H/8, W/8]

        B, C, H15, W15 = x15.shape
        x15_ = x15.reshape(B, C, H15 * 2, W15 // 2)

        # dec1
        x16 = F.interpolate(x15_, [x8_.shape[2], x8_.shape[3]], mode="nearest")
        x17 = torch.cat([x16, x8_], dim=1)
        x18 = self.conv_dec1(x17)  # [H/4, W/4]

        # dec2
        x19 = F.interpolate(x18, [x3_.shape[2], x3_.shape[3]], mode="nearest")
        x20 = torch.cat([x19, x3_], dim=1)
        x21 = self.conv_dec2(x20)  # [H/2, W/2]

        x22 = F.interpolate(x21, [x2_.shape[2], x2_.shape[3]], mode="nearest")
        x23 = torch.cat([x22, x2_], dim=1)
        x24 = self.conv_dec3(x23)  # [H, W]

        c0 = nn.Sigmoid()(-self.conf0(x15))
        c1 = nn.Sigmoid()(-self.conf1(x18))
        c2 = nn.Sigmoid()(-self.conf2(x21))
        c3 = nn.Sigmoid()(-self.conf3(x24))

        x15 = L2_norm(x15_)
        x18 = L2_norm(x18)
        x21 = L2_norm(x21)
        x24 = L2_norm(x24)

        if self.level == -1:
            return [x15], [c0]
        elif self.level == -2:
            return [x18], [c1]
        elif self.level == -3:
            return [x21], [c2]
        elif self.level == 2:
            return [x18, x21], [c1, c2]
        elif self.level == 3:
            return [x15, x18, x21], [c0, c1, c2]
        elif self.level == 4:
            return [x15, x18, x21, x24], [c0, c1, c2, c3]


def process_depth(d):
    B, _, H, W = d.shape
    d = (d + 1)/2
    d1 = torch.cat([d[:, :, :H//2, :] * 10, d[:, :, H//2 :, :] * 1.6], dim=2)
    return d1

# class VGGUnet(nn.Module):
#     def __init__(self, level):
#         super(VGGUnet, self).__init__()
#
#         self.level = level
#
#         vgg16 = torchvision.models.vgg16(pretrained=True)
#
#         # load CNN from VGG16, the first three block
#         self.conv0 = vgg16.features[0]
#         self.conv2 = vgg16.features[2]  # \\64
#         self.conv5 = vgg16.features[5]  #
#         self.conv7 = vgg16.features[7]  # \\128
#         self.conv10 = vgg16.features[10]
#         self.conv12 = vgg16.features[12]
#         self.conv14 = vgg16.features[14]  # \\256
#
#         self.bottleneck = nn.Sequential(
#             nn.ReLU(inplace=True),
#             nn.Conv2d(self.conv14.out_channels, self.conv14.out_channels, kernel_size=(3, 3),
#                       stride=(1, 1), padding=1, bias=False),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(self.conv14.out_channels, self.conv14.out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1,
#                       bias=False),
#         )
#
#         self.conv_dec1 = nn.Sequential(
#             nn.ReLU(inplace=True),
#             nn.Conv2d(self.conv14.out_channels + self.conv14.out_channels, self.conv14.out_channels, kernel_size=(3, 3),
#                       stride=(1, 1), padding=1, bias=False),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(self.conv14.out_channels, self.conv14.out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1,
#                       bias=False),
#         )
#
#         self.conv_dec2 = nn.Sequential(
#             nn.ReLU(inplace=True),
#             nn.Conv2d(self.conv14.out_channels + self.conv7.out_channels, self.conv7.out_channels, kernel_size=(3, 3),
#                       stride=(1, 1), padding=1, bias=False),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(self.conv7.out_channels, self.conv7.out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1,
#                       bias=False)
#         )
#
#         self.conv_dec3 = nn.Sequential(
#             nn.ReLU(inplace=True),
#             nn.Conv2d(self.conv7.out_channels + self.conv2.out_channels, self.conv2.out_channels, kernel_size=(3, 3),
#                       stride=(1, 1), padding=1, bias=False),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(self.conv2.out_channels, self.conv2.out_channels, kernel_size=(3, 3), stride=(1, 1), padding=1,
#                       bias=False)
#         )
#
#         self.relu = nn.ReLU(inplace=True)
#         self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False,
#                                      return_indices=True)
#
#         self.conf0 = nn.Sequential(
#             nn.ReLU(),
#             nn.Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
#             nn.Sigmoid())
#         self.conf1 = nn.Sequential(
#             nn.ReLU(),
#             nn.Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
#             nn.Sigmoid())
#         self.conf2 = nn.Sequential(
#             nn.ReLU(),
#             nn.Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
#             nn.Sigmoid())
#         self.conf3 = nn.Sequential(
#             nn.ReLU(),
#             nn.Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
#             nn.Sigmoid())
#
#         self.feat0 = nn.Sequential(
#             nn.ReLU(),
#             nn.Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
#             nn.Sigmoid())
#         self.feat1 = nn.Sequential(
#             nn.ReLU(),
#             nn.Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
#             nn.Sigmoid())
#         self.feat2 = nn.Sequential(
#             nn.ReLU(),
#             nn.Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
#             nn.Sigmoid())
#         self.feat3 = nn.Sequential(
#             nn.ReLU(),
#             nn.Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
#             nn.Sigmoid())
#
#     def forward(self, x):
#         # block0
#         x0 = self.conv0(x)
#         x1 = self.relu(x0)
#         x2 = self.conv2(x1)  # [H, W, 64]
#
#         x3, ind3 = self.max_pool(x2)
#         x4 = self.relu(x3)
#         x5 = self.conv5(x4)
#         x6 = self.relu(x5)
#         x7 = self.conv7(x6) # [H/2, W/2, 128]
#
#         x8, ind8 = self.max_pool(x7)
#         x9 = self.relu(x8)
#         x10 = self.conv10(x9)
#         x11 = self.relu(x10)
#         x14 = self.conv12(x11)
#         # x13 = self.relu(x12)
#         # x14 = self.conv14(x11)  # [H/4, W/4, 256]
#
#         x15, ind15 = self.max_pool(x14)
#         x16 = self.bottleneck(x15)   # [H/8, W/8, 256]
#
#         # dec1
#         x17 = F.interpolate(x16, [x14.shape[2], x14.shape[3]], mode="nearest")
#         x18 = torch.cat([x17, x14], dim=1)
#         x19 = self.conv_dec1(x18)  # [H/4, W/4, 256]
#
#         # dec2
#         x20 = F.interpolate(x19, [x7.shape[2], x7.shape[3]], mode="nearest")
#         x21 = torch.cat([x20, x7], dim=1)
#         x22 = self.conv_dec2(x21)  # [H/2, H/2, 128]
#
#         x23 = F.interpolate(x22, [x2.shape[2], x2.shape[3]], mode="nearest")
#         x24 = torch.cat([x23, x2], dim=1)
#         x25 = self.conv_dec3(x24)
#
#         # c0 = 1 / (1 + self.conf0(x15))
#         # c1 = 1 / (1 + self.conf1(x18))
#         # c2 = 1 / (1 + self.conf2(x21))
#         c0 = self.conf0(x16)
#         c1 = self.conf1(x19)
#         c2 = self.conf2(x22)
#         c3 = self.conf3(x25)
#
#         x16 = L2_norm(self.feat0(x16))
#         x19 = L2_norm(self.feat1(x19))
#         x22 = L2_norm(self.feat2(x22))
#         x25 = L2_norm(self.feat3(x25))
#
#         if self.level == -1:
#             return [x16], [c0]
#         elif self.level == -2:
#             return [x19], [c1]
#         elif self.level == -3:
#             return [x22], [c2]
#         elif self.level == -4:
#             return [x25], [c3]
#         elif self.level == 2:
#             return [x16, x19], [c0, c1]
#         elif self.level == 3:
#             return [x16, x19, x22], [c0, c1, c2]
#         elif self.level == 4:
#             return [x16, x19, x22, x25], [c0, c1, c2, c3]


def L2_norm(x):
    B, C, H, W = x.shape
    y = F.normalize(x.reshape(B, C*H*W))
    return y.reshape(B, C, H, W)


# class VGGUNet(nn.Module):
#     def __init__(
#         self, net="vgg16", pool="max", n_encoder_stages=3, n_decoder_convs=2, last_feature_channel=32
#     ):
#         super().__init__()
#
#         if net == "vgg16":
#             vgg = torchvision.models.vgg16(pretrained=True).features
#         elif net == "vgg19":
#             vgg = torchvision.models.vgg19(pretrained=True).features
#         else:
#             raise Exception("invalid vgg net")
#
#         encs = []
#         enc = []
#         encs_channels = []
#         channels = -1
#         for mod in vgg:
#             if isinstance(mod, nn.Conv2d):
#                 channels = mod.out_channels
#
#             if isinstance(mod, nn.MaxPool2d):
#                 encs.append(nn.Sequential(*enc))
#                 encs_channels.append(channels)
#                 n_encoder_stages -= 1
#                 if n_encoder_stages <= 0:
#                     break
#                 if pool == "average":
#                     enc = [
#                         nn.AvgPool2d(
#                             kernel_size=2, stride=2, padding=0, ceil_mode=False
#                         )
#                     ]
#                 elif pool == "max":
#                     enc = [
#                         nn.MaxPool2d(
#                             kernel_size=2, stride=2, padding=0, ceil_mode=False
#                         )
#                     ]
#                 else:
#                     raise Exception("invalid pool")
#             else:
#                 enc.append(mod)
#         self.encs = nn.ModuleList(encs)
#         self.conv_conf0 = nn.Sequential(
#             nn.Conv2d(encs_channels[-1], 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),
#             nn.ReLU(inplace=True),
#         )
#
#         cin = encs_channels[-1] + encs_channels[-2]
#         decs = []
#         for idx, cout in enumerate(reversed(encs_channels[:-1])):
#             decs.append(self._dec(cin, cout, n_convs=n_decoder_convs))
#             cin = cout + encs_channels[max(-idx - 3, -len(encs_channels))]
#
#         cin = cout
#         decs.append(nn.Conv2d(cin, last_feature_channel + 1, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False))
#         self.decs = nn.ModuleList(decs)
#
#     def _dec(self, channels_in, channels_out, n_convs=2):
#         mods = []
#         for idx in range(n_convs):
#
#             mods.append(
#                 nn.Conv2d(
#                     channels_in,
#                     channels_out,
#                     kernel_size=(3, 3),
#                     stride=(1, 1),
#                     padding=1,
#                     bias=False,
#                 )
#             )
#             mods.append(nn.ReLU())
#             channels_in = channels_out
#         return nn.Sequential(*mods)
#
#     def forward(self, x):
#         feats = []
#         for enc in self.encs:
#             x = enc(x)
#             feats.append(x)
#
#         for dec in self.decs:
#             x0 = feats.pop()
#             x1 = feats.pop()
#             x0 = F.interpolate(
#                 x0, size=(x1.shape[2], x1.shape[3]), mode="nearest"
#             )
#             x = torch.cat((x0, x1), dim=1)
#             x = dec(x)
#             feats.append(x)
#
#         x = feats.pop()
#         uncertainty, feat = torch.split(x, [1, x.shape[1] - 1], dim=1)
#         uncertainty = nn.ReLU()(uncertainty)
#         confidence = 1/(1+uncertainty)
#
#         y = torch.cat([confidence, feat], dim=1)
#
#         return y

================
File: cross_attention.py
================
from turtle import forward
import torch
import torch.nn as nn
import torch.nn.functional as F
from VGG import L2_norm
from swin_transformer_cross import SwinTransformerSelf


class CrossAttention(nn.Module):
    def __init__(self, dim, heads, dim_head, qkv_bias, norm=nn.LayerNorm):
        super(CrossAttention, self).__init__()

        self.scale = dim_head ** -0.5

        self.heads = heads
        self.dim_head = dim_head

        self.to_q = nn.Sequential(norm(dim), nn.Linear(dim, heads * dim_head, bias=qkv_bias))
        self.to_k = nn.Sequential(norm(dim), nn.Linear(dim, heads * dim_head, bias=qkv_bias))
        self.to_v = nn.Sequential(norm(dim), nn.Linear(dim, heads * dim_head, bias=qkv_bias))

        self.proj = nn.Linear(heads * dim_head, dim)
        self.prenorm = norm(dim)
        self.mlp = nn.Sequential(nn.Linear(dim, 2 * dim), nn.GELU(), nn.Linear(2 * dim, dim))
        self.postnorm = norm(dim)

    def forward(self, x, y):
        """
        x: (B, M, C)
        y: (B, M, N, C)
        """
        
        B, M, C = x.shape
        _, M, N, C = y.shape

        # Project with multiple heads
        q = self.to_q(x).reshape(B, M, 1, self.heads, self.dim_head).permute(0, 3, 1, 2, 4) # [B, heads, M, 1, dim_head]
        k = self.to_k(y).reshape(B, M, N, self.heads, self.dim_head).permute(0, 3, 1, 2, 4) # [B, heads, M, N, dim_head]
        v = self.to_v(y).reshape(B, M, N, self.heads, self.dim_head).permute(0, 3, 1, 2, 4) # [B, heads, M, N, dim_head]

        # Dot product attention along cameras
        dot = self.scale * torch.matmul(q, k.transpose(-1, -2)).reshape(B, self.heads, M, N, 1)  
        dot = dot.softmax(dim=-2)

        # Combine values (image level features).
        a = torch.sum(dot * v, dim=-2) # [B, self.heads, M, dim_heads]
        a = a.permute(0, 2, 1, 3).reshape(B, M, self.heads * self.dim_head)
        z = self.proj(a)

        z = self.prenorm(z)
        z = z + self.mlp(z)
        z = self.postnorm(z)  # [B, M, C]
    
        return z


def generate_xy_for_attn(x, y, u):
    '''
    x.shape = [B, C, S, S]
    y.shape = [B, C, H, W]
    uv.shape = [B, S, S]
    
    return:
    x.shape = [B, S^2, C]
    ys.shape = [B, S^2, 2H, C]
    '''
    B, C, S, _ = x.shape
    x = x.reshape(B, C, S*S).permute(0, 2, 1)
    
    _, C, H, W = y.shape
    
    with torch.no_grad():
        u_left = torch.floor(u)
        u_right = u_left + 1
        
        torch.clamp(u_left, 0, W -1, out=u_left)
        torch.clamp(u_right, 0, W -1, out=u_right)
    
    y = y.reshape(B, C*H, W)
    y_left = torch.gather(y, 2, u_left.long().view(B, 1, S*S).repeat(1, C*H, 1)).view(B, C, H, S*S)
    y_right = torch.gather(y, 2, u_right.long().view(B, 1, S*S).repeat(1, C*H, 1)).view(B, C, H, S*S)
    ys = torch.cat([y_left, y_right], dim=2).permute(0, 3, 2, 1)  # [B, S^2, 2H, C]
    
    return x, ys


def generate_y_for_attn(S, y, u):
    '''

    y.shape = [B, C, H, W]
    uv.shape = [B, S, S]

    return:

    ys.shape = [B, S^2, 2H, C]
    '''
    # B, C, S, _ = x.shape
    # x = x.reshape(B, C, S * S).permute(0, 2, 1)

    B, C, H, W = y.shape

    with torch.no_grad():
        u_left = torch.floor(u)
        u_right = u_left + 1

        torch.clamp(u_left, 0, W - 1, out=u_left)
        torch.clamp(u_right, 0, W - 1, out=u_right)

    y = y.reshape(B, C * H, W)
    y_left = torch.gather(y, 2, u_left.long().view(B, 1, S * S).repeat(1, C * H, 1)).view(B, C, H, S * S)
    y_right = torch.gather(y, 2, u_right.long().view(B, 1, S * S).repeat(1, C * H, 1)).view(B, C, H, S * S)
    ys = torch.cat([y_left, y_right], dim=2).permute(0, 3, 2, 1)  # [B, S^2, 2H, C]

    return ys
    

class CrossViewAttention(nn.Module):
    
    def __init__(self, blocks, dim, heads, dim_head, qkv_bias, norm=nn.LayerNorm) -> None:
        super(CrossViewAttention, self).__init__()

        self.blocks = blocks

        self.cross_attention_layers = nn.ModuleList()
        self.self_attention_layers = nn.ModuleList()
        for _ in range(blocks):
            self.self_attention_layers.append(
                SwinTransformerSelf(img_size=[64, 64], patch_size=1, in_chans=256, num_classes=3,
                                embed_dim=48, depths=[1], num_heads=[3],
                                window_size=8, mlp_ratio=4., qkv_bias=False, proj_bias=False, qk_scale=None,
                                drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                                norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                                use_checkpoint=False)
            )
            self.cross_attention_layers.append(
                CrossAttention(dim, heads, dim_head, qkv_bias, norm)
            )

    
    
    def forward(self, grd2sat, grd_x, u, geo_mask):
        '''
         grd2sat.shape = [B, C, S, S]
         grd_x.shape = [B, C, H, W]
         u.shape = [B, S, S]
        '''
        B, C, S, _ = grd2sat.shape

        x_attn = grd2sat
        y_attn = generate_y_for_attn(S, grd_x, u)

        for i in range(self.blocks):
            x_attn = self.self_attention_layers[i](x_attn, geo_mask)
            # x_attn = x_attn.reshape(B, C, S * S).permute(0, 2, 1)
            x_attn = self.cross_attention_layers[i](x_attn, y_attn)
            x_attn = x_attn.permute(0, 2, 1).reshape(B, C, S, S)

        # for layer in self.cross_attention_layers:
        #     x_attn = layer(x_attn, y_attn)
        
        # x_attn = x_attn.permute(0, 2, 1).reshape(B, C, S, S)
        
        return L2_norm(x_attn)

================
File: swin_transformer_cross.py
================
# --------------------------------------------------------
# Swin Transformer
# Copyright (c) 2021 Microsoft
# Licensed under The MIT License [see LICENSE for details]
# Written by Ze Liu
# --------------------------------------------------------

import torch
import torch.nn as nn
import torch.utils.checkpoint as checkpoint
from timm.models.layers import DropPath, to_2tuple, trunc_normal_
import torch.nn.functional as F


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


def window_partition(x, window_size):
    """
    Args:
        x: (B, H, W, C)
        window_size (int): window size

    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    """
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows


def window_reverse(windows, window_size, H, W):
    """
    Args:
        windows: (num_windows*B, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image

    Returns:
        x: (B, H, W, C)
    """
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x


class WindowAttention(nn.Module):
    """ Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.

    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, proj_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
        self.register_buffer("relative_position_index", relative_position_index)

        # self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.q_proj = nn.Linear(dim, dim, bias=qkv_bias)
        self.k_proj = nn.Linear(dim, dim, bias=qkv_bias)
        self.v_proj = nn.Linear(dim, dim, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim, bias=proj_bias)
        self.proj_drop = nn.Dropout(proj_drop)

        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, y, mask=None, geo_mask=None):
        """
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
            geo_mask: (0/-inf) mask with shape of (num_windows * B, Wh*Ww, 1) or None
            N = Wh*Ww
        """
        B_, N, C = x.shape
        B_, M, C = y.shape
        q = self.q_proj(x).reshape(B_, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        k = self.k_proj(y).reshape(B_, M, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        v = self.v_proj(y).reshape(B_, M, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        # [B, num_heads, N, C//num_heads]
        
        # qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        # q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)
    
        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))  # [B, num_heads, N, M]

        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
        attn = attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
        
        if geo_mask is not None:
            geo_mask = geo_mask.view(B_, 1, 1, N)
            attn = attn + geo_mask
            
            # attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + geo_mask.unsqueeze(1).unsqueeze(0)
            # attn = attn.view(-1, self.num_heads, N, N)
            
        attn = self.softmax(attn)

        attn = self.attn_drop(attn)   # [B, self.num_heads, N, N,]

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x, attn

    def extra_repr(self) -> str:
        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'

    def flops(self, N):
        # calculate flops for 1 window with token length of N
        flops = 0
        # qkv = self.qkv(x)
        flops += N * self.dim * 3 * self.dim
        # attn = (q @ k.transpose(-2, -1))
        flops += self.num_heads * N * (self.dim // self.num_heads) * N
        #  x = (attn @ v)
        flops += self.num_heads * N * N * (self.dim // self.num_heads)
        # x = self.proj(x)
        flops += N * self.dim * self.dim
        return flops


class SwinTransformerBlock(nn.Module):
    r""" Swin Transformer Block.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resulotion.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, proj_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        if min(self.input_resolution) <= self.window_size:
            # if window size is larger than input resolution, we don't partition windows
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        assert 0 <= self.shift_size < self.window_size, "shift_size must in 0-window_size"

        self.normx = norm_layer(dim)
        self.normy = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, proj_bias=proj_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        if self.shift_size > 0:
            # calculate attention mask for SW-MSA
            H, W = self.input_resolution
            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1

            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
        else:
            attn_mask = None

        self.register_buffer("attn_mask", attn_mask)

    def forward(self, x, y, geo_mask=None):
        '''
        x.shape = B, HW, C
        y.shape = B, HW, C
        geo_mask.shape = B, H, W, 1
        '''
        H, W = self.input_resolution
        B, L, C = x.shape
        assert x.shape == y.shape
        assert L == H * W, "input feature has wrong size"

        shortcut = x
        x = self.normx(x)
        x = x.view(B, H, W, C)
        
        y = self.normy(y).view(B, H, W, C)

        # cyclic shift
        if self.shift_size > 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
            shifted_y = torch.roll(y, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
            if geo_mask is not None:
                shifted_geo_mask = torch.roll(geo_mask, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
            else:
                shifted_geo_mask = None
        else:
            shifted_x = x
            shifted_y = y
            shifted_geo_mask = geo_mask

        # partition windows
        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C
        
        y_windows = window_partition(shifted_y, self.window_size)  # nW*B, window_size, window_size, C
        y_windows = y_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C
        
        if geo_mask is not None:
            geo_mask_windows = window_partition(shifted_geo_mask, self.window_size)  # nW*B, window_size, window_size, C
            geo_mask_windows = geo_mask_windows.view(-1, self.window_size * self.window_size, 1)  # nW*B, window_size*window_size, C
            geo_mask_windows = geo_mask_windows.masked_fill(geo_mask_windows == 0, float(-100.0)).masked_fill(geo_mask_windows > 0, float(0.0))
        else:
            geo_mask_windows = None

        # W-MSA/SW-MSA
        attn_windows, atten_matrix = self.attn(x_windows, y_windows, mask=self.attn_mask, geo_mask=geo_mask_windows)  # 
        # [nW*B, window_size*window_size, C], [nW*B, self.num_heads, window_size*window_size, window_size*window_size,]

        # merge windows
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C

        # reverse cyclic shift
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        x = x.view(B, H * W, C)

        # FFN
        x = shortcut + self.drop_path(x)
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x, atten_matrix

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, " \
               f"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}"

    def flops(self):
        flops = 0
        H, W = self.input_resolution
        # norm1
        flops += self.dim * H * W
        # W-MSA/SW-MSA
        nW = H * W / self.window_size / self.window_size
        flops += nW * self.attn.flops(self.window_size * self.window_size)
        # mlp
        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio
        # norm2
        flops += self.dim * H * W
        return flops


class SwinTransformerBlockSelf(nn.Module):
    r""" Swin Transformer Block.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resulotion.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, proj_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        if min(self.input_resolution) <= self.window_size:
            # if window size is larger than input resolution, we don't partition windows
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        assert 0 <= self.shift_size < self.window_size, "shift_size must in 0-window_size"

        self.normx = norm_layer(dim)
        # self.normy = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, proj_bias=proj_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        if self.shift_size > 0:
            # calculate attention mask for SW-MSA
            H, W = self.input_resolution
            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1

            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
        else:
            attn_mask = None

        self.register_buffer("attn_mask", attn_mask)

    def forward(self, x, geo_mask=None):
        '''
        x.shape = B, HW, C
        y.shape = B, HW, C
        geo_mask.shape = B, H, W, 1
        '''
        H, W = self.input_resolution
        B, L, C = x.shape
        # assert x.shape == y.shape
        assert L == H * W, "input feature has wrong size"

        shortcut = x
        x = self.normx(x)
        x = x.view(B, H, W, C)

        # y = self.normy(y).view(B, H, W, C)

        # cyclic shift
        if self.shift_size > 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
            # shifted_y = torch.roll(y, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
            if geo_mask is not None:
                shifted_geo_mask = torch.roll(geo_mask, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
            else:
                shifted_geo_mask = None
        else:
            shifted_x = x
            # shifted_y = y
            shifted_geo_mask = geo_mask

        # partition windows
        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C

        # y_windows = window_partition(shifted_y, self.window_size)  # nW*B, window_size, window_size, C
        # y_windows = y_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C

        if geo_mask is not None:
            geo_mask_windows = window_partition(shifted_geo_mask, self.window_size)  # nW*B, window_size, window_size, C
            geo_mask_windows = geo_mask_windows.view(-1, self.window_size * self.window_size,
                                                     1)  # nW*B, window_size*window_size, C
            geo_mask_windows = geo_mask_windows.masked_fill(geo_mask_windows == 0, float(-100.0)).masked_fill(
                geo_mask_windows > 0, float(0.0))
        else:
            geo_mask_windows = None

        # W-MSA/SW-MSA
        attn_windows, atten_matrix = self.attn(x_windows, x_windows, mask=self.attn_mask, geo_mask=geo_mask_windows)  #
        # [nW*B, window_size*window_size, C], [nW*B, self.num_heads, window_size*window_size, window_size*window_size,]

        # merge windows
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C

        # reverse cyclic shift
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        x = x.view(B, H * W, C)

        # FFN
        x = shortcut + self.drop_path(x)
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x, atten_matrix

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, " \
               f"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}"

    def flops(self):
        flops = 0
        H, W = self.input_resolution
        # norm1
        flops += self.dim * H * W
        # W-MSA/SW-MSA
        nW = H * W / self.window_size / self.window_size
        flops += nW * self.attn.flops(self.window_size * self.window_size)
        # mlp
        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio
        # norm2
        flops += self.dim * H * W
        return flops


class PatchMerging(nn.Module):
    r""" Patch Merging Layer.

    Args:
        input_resolution (tuple[int]): Resolution of input feature.
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(4 * dim)

    def forward(self, x):
        """
        x: B, H*W, C
        """
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"
        assert H % 2 == 0 and W % 2 == 0, f"x size ({H}*{W}) are not even."

        x = x.view(B, H, W, C)

        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.norm(x)
        x = self.reduction(x)

        return x

    def extra_repr(self) -> str:
        return f"input_resolution={self.input_resolution}, dim={self.dim}"

    def flops(self):
        H, W = self.input_resolution
        flops = H * W * self.dim
        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim
        return flops


def PatchMergingMask(x):
    """
    x: B, H, W, 1
    """
    
    B, H, W, _ = x.shape
    x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
    x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
    x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
    x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
    x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
    x = torch.sum(x, dim=-1, keepdims=True)
    
    return x
    

class BasicLayer(nn.Module):
    """ A basic Swin Transformer layer for one stage.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resolution.
        depth (int): Number of blocks.
        num_heads (int): Number of attention heads.
        window_size (int): Local window size.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
    """

    def __init__(self, dim, input_resolution, depth, num_heads, window_size,
                 mlp_ratio=4., qkv_bias=True, proj_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):

        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.depth = depth
        self.use_checkpoint = use_checkpoint

        # build blocks
        # self.blocks = nn.ModuleList([
        #     SwinTransformerBlock(dim=dim, input_resolution=input_resolution,
        #                          num_heads=num_heads, window_size=window_size,
        #                          shift_size=0 if (i % 2 == 0) else window_size // 2,
        #                          mlp_ratio=mlp_ratio,
        #                          qkv_bias=qkv_bias, qk_scale=qk_scale,
        #                          drop=drop, attn_drop=attn_drop,
        #                          drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
        #                          norm_layer=norm_layer)
        #     for i in range(depth)])
        
        self.self_attention_xs = nn.ModuleList()
        self.cross_attention = nn.ModuleList()
        self.self_attention_ys = nn.ModuleList()
        
        for i in range(depth):
            self.self_attention_xs.append(
                SwinTransformerBlock(dim=dim, input_resolution=input_resolution,
                                 num_heads=num_heads, window_size=window_size,
                                 shift_size=0 if (i % 2 == 0) else window_size // 2,
                                 mlp_ratio=mlp_ratio,
                                 qkv_bias=qkv_bias, proj_bias=proj_bias, qk_scale=qk_scale,
                                 drop=drop, attn_drop=attn_drop,
                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                                 norm_layer=norm_layer)
            )
            self.cross_attention.append(
                SwinTransformerBlock(dim=dim, input_resolution=input_resolution,
                                 num_heads=num_heads, window_size=window_size,
                                 shift_size=0 if (i % 2 == 0) else window_size // 2,
                                 mlp_ratio=mlp_ratio,
                                 qkv_bias=qkv_bias, proj_bias=proj_bias, qk_scale=qk_scale,
                                 drop=drop, attn_drop=attn_drop,
                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                                 norm_layer=norm_layer)
            )
            self.self_attention_ys.append(
                SwinTransformerBlock(dim=dim, input_resolution=input_resolution,
                                 num_heads=num_heads, window_size=window_size,
                                 shift_size=0 if (i % 2 == 0) else window_size // 2,
                                 mlp_ratio=mlp_ratio,
                                 qkv_bias=qkv_bias, proj_bias=proj_bias, qk_scale=qk_scale,
                                 drop=drop, attn_drop=attn_drop,
                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                                 norm_layer=norm_layer)
            )

        # patch merging layer
        if downsample is not None:
            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)
        else:
            self.downsample = None

    def forward(self, x, y, geo_mask=None):
        '''
        x.shape = [B, HW, C]
        y.shape = [B, HW, C]
        geo_mask.shape = [B, H, W, 1]
        '''
        
        for i in range(self.depth):
            x, self_atten_matrix_x = self.self_attention_xs[i](x, x, geo_mask)
            x, cross_atten_matrix = self.cross_attention[i](x, y, geo_mask=None)
            y, self_atten_matrix_y = self.self_attention_ys[i](y, y, geo_mask=None)
            
        # for blk in self.blocks:
        #     if self.use_checkpoint:
        #         x = checkpoint.checkpoint(blk, x, y, geo_mask)
        #     else:
        #         x = blk(x, y, geo_mask)
        if self.downsample is not None:
            x = self.downsample(x)
            y = self.downsample(y)
            geo_mask = PatchMergingMask(geo_mask)
        return x, geo_mask, cross_atten_matrix, y
        # x.shape = [B, H*W, C]
        # geo_mask.shape = [B, H, W, 1]
        # [nW*B, self.num_heads, window_size*window_size, window_size*window_size,]


    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}"

    def flops(self):
        flops = 0
        for blk in self.blocks:
            flops += blk.flops()
        if self.downsample is not None:
            flops += self.downsample.flops()
        return flops


class BasicLayerSelf(nn.Module):
    """ A basic Swin Transformer layer for one stage.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resolution.
        depth (int): Number of blocks.
        num_heads (int): Number of attention heads.
        window_size (int): Local window size.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
    """

    def __init__(self, dim, input_resolution, depth, num_heads, window_size,
                 mlp_ratio=4., qkv_bias=True, proj_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):

        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.depth = depth
        self.use_checkpoint = use_checkpoint

        # build blocks
        self.blocks = nn.ModuleList([
            SwinTransformerBlockSelf(dim=dim, input_resolution=input_resolution,
                                 num_heads=num_heads, window_size=window_size,
                                 shift_size=0 if (i % 2 == 0) else window_size // 2,
                                 mlp_ratio=mlp_ratio,
                                 qkv_bias=qkv_bias, proj_bias=proj_bias, qk_scale=qk_scale,
                                 drop=drop, attn_drop=attn_drop,
                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                                 norm_layer=norm_layer)
            for i in range(depth)])

        # self.self_attention_xs = nn.ModuleList()
        # self.cross_attention = nn.ModuleList()
        # self.self_attention_ys = nn.ModuleList()
        #
        # for i in range(depth):
        #     self.self_attention_xs.append(
        #         SwinTransformerBlock(dim=dim, input_resolution=input_resolution,
        #                              num_heads=num_heads, window_size=window_size,
        #                              shift_size=0 if (i % 2 == 0) else window_size // 2,
        #                              mlp_ratio=mlp_ratio,
        #                              qkv_bias=qkv_bias, proj_bias=proj_bias, qk_scale=qk_scale,
        #                              drop=drop, attn_drop=attn_drop,
        #                              drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
        #                              norm_layer=norm_layer)
        #     )
        #     self.cross_attention.append(
        #         SwinTransformerBlock(dim=dim, input_resolution=input_resolution,
        #                              num_heads=num_heads, window_size=window_size,
        #                              shift_size=0 if (i % 2 == 0) else window_size // 2,
        #                              mlp_ratio=mlp_ratio,
        #                              qkv_bias=qkv_bias, proj_bias=proj_bias, qk_scale=qk_scale,
        #                              drop=drop, attn_drop=attn_drop,
        #                              drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
        #                              norm_layer=norm_layer)
        #     )
        #     self.self_attention_ys.append(
        #         SwinTransformerBlock(dim=dim, input_resolution=input_resolution,
        #                              num_heads=num_heads, window_size=window_size,
        #                              shift_size=0 if (i % 2 == 0) else window_size // 2,
        #                              mlp_ratio=mlp_ratio,
        #                              qkv_bias=qkv_bias, proj_bias=proj_bias, qk_scale=qk_scale,
        #                              drop=drop, attn_drop=attn_drop,
        #                              drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
        #                              norm_layer=norm_layer)
        #     )

        # patch merging layer
        if downsample is not None:
            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)
        else:
            self.downsample = None

    def forward(self, x, geo_mask=None):
        '''
        x.shape = [B, HW, C]
        y.shape = [B, HW, C]
        geo_mask.shape = [B, H, W, 1]
        '''

        # for i in range(self.depth):
        #     x, self_atten_matrix_x = self.self_attention_xs[i](x, x, geo_mask)
        #     x, cross_atten_matrix = self.cross_attention[i](x, y, geo_mask=None)
        #     y, self_atten_matrix_y = self.self_attention_ys[i](y, y, geo_mask=None)

        for blk in self.blocks:
            if self.use_checkpoint:
                x, _ = checkpoint.checkpoint(blk, x, geo_mask)
            else:
                x, _ = blk(x, geo_mask)
        if self.downsample is not None:
            x = self.downsample(x)

            geo_mask = PatchMergingMask(geo_mask)
        return x, geo_mask
        # x.shape = [B, H*W, C]
        # geo_mask.shape = [B, H, W, 1]
        # [nW*B, self.num_heads, window_size*window_size, window_size*window_size,]

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}"

    def flops(self):
        flops = 0
        for blk in self.blocks:
            flops += blk.flops()
        if self.downsample is not None:
            flops += self.downsample.flops()
        return flops


class PatchEmbed(nn.Module):
    r""" Image to Patch Embedding

    Args:
        img_size (int): Image size.  Default: 224.
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
        if self.norm is not None:
            x = self.norm(x)
        return x

    def flops(self):
        Ho, Wo = self.patches_resolution
        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])
        if self.norm is not None:
            flops += Ho * Wo * self.embed_dim
        return flops


class SwinTransformerSelf(nn.Module):
    r""" Swin Transformer
        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -
          https://arxiv.org/pdf/2103.14030

    Args:
        img_size (int | tuple(int)): Input image size. Default 224
        patch_size (int | tuple(int)): Patch size. Default: 4
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        embed_dim (int): Patch embedding dimension. Default: 96
        depths (tuple(int)): Depth of each Swin Transformer layer.
        num_heads (tuple(int)): Number of attention heads in different layers.
        window_size (int): Window size. Default: 7
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None
        drop_rate (float): Dropout rate. Default: 0
        attn_drop_rate (float): Attention dropout rate. Default: 0
        drop_path_rate (float): Stochastic depth rate. Default: 0.1
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
        patch_norm (bool): If True, add normalization after patch embedding. Default: True
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,
                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],
                 window_size=7, mlp_ratio=4., qkv_bias=True, proj_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False):
        super().__init__()

        self.in_chans = in_chans
        self.num_classes = num_classes
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.ape = ape
        self.patch_norm = patch_norm
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))
        self.mlp_ratio = mlp_ratio
        # self.pose_from = pose_from
        self.patch_size = patch_size

        # split image into non-overlapping patches
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)
        num_patches = self.patch_embed.num_patches
        patches_resolution = self.patch_embed.patches_resolution
        self.patches_resolution = patches_resolution

        # absolute position embedding
        if self.ape:
            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
            trunc_normal_(self.absolute_pos_embed, std=.02)

        self.pos_drop = nn.Dropout(p=drop_rate)

        # stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

        # build layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayerSelf(dim=int(embed_dim * 2 ** i_layer),
                               input_resolution=(patches_resolution[0] // (2 ** i_layer),
                                                 patches_resolution[1] // (2 ** i_layer)),
                               depth=depths[i_layer],
                               num_heads=num_heads[i_layer],
                               window_size=window_size,
                               mlp_ratio=self.mlp_ratio,
                               qkv_bias=qkv_bias, proj_bias=proj_bias, qk_scale=qk_scale,
                               drop=drop_rate, attn_drop=attn_drop_rate,
                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                               norm_layer=norm_layer,
                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                               use_checkpoint=use_checkpoint)
            self.layers.append(layer)

        self.head = nn.Linear(self.num_features, self.in_chans)

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'absolute_pos_embed'}

    @torch.jit.ignore
    def no_weight_decay_keywords(self):
        return {'relative_position_bias_table'}

    def forward(self, x, geo_mask):
        B, C, H, W = x.shape
        x = self.patch_embed(x)
        if geo_mask is not None:
            geo_mask = F.avg_pool2d(geo_mask.permute(0, 3, 1, 2).float(), (self.patch_size, self.patch_size),
                                    stride=(self.patch_size, self.patch_size), divisor_override=1).permute(0, 2, 3, 1)
        if self.ape:
            x = x + self.absolute_pos_embed

        x = self.pos_drop(x)

        for layer in self.layers:
            # import pdb; pdb.set_trace()
            x, geo_mask = layer(x, geo_mask)
            # x.shape = [B, HW, C]

        x = self.head(x)

        return x #.permute(0, 2, 1).reshape(B, -1, H, W)




    def flops(self):
        flops = 0
        flops += self.patch_embed.flops()
        for i, layer in enumerate(self.layers):
            flops += layer.flops()
        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)
        flops += self.num_features * self.num_classes
        return flops


class SwinTransformer(nn.Module):
    r""" Swin Transformer
        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -
          https://arxiv.org/pdf/2103.14030

    Args:
        img_size (int | tuple(int)): Input image size. Default 224
        patch_size (int | tuple(int)): Patch size. Default: 4
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        embed_dim (int): Patch embedding dimension. Default: 96
        depths (tuple(int)): Depth of each Swin Transformer layer.
        num_heads (tuple(int)): Number of attention heads in different layers.
        window_size (int): Window size. Default: 7
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None
        drop_rate (float): Dropout rate. Default: 0
        attn_drop_rate (float): Attention dropout rate. Default: 0
        drop_path_rate (float): Stochastic depth rate. Default: 0.1
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
        patch_norm (bool): If True, add normalization after patch embedding. Default: True
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,
                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],
                 window_size=7, mlp_ratio=4., qkv_bias=True, proj_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, pose_from='feature', channel=32, **kwargs):
        super().__init__()

        self.num_classes = num_classes
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.ape = ape
        self.patch_norm = patch_norm
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))
        self.mlp_ratio = mlp_ratio
        self.pose_from = pose_from
        self.patch_size = patch_size

        # split image into non-overlapping patches
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)
        num_patches = self.patch_embed.num_patches
        patches_resolution = self.patch_embed.patches_resolution
        self.patches_resolution = patches_resolution

        # absolute position embedding
        if self.ape:
            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
            trunc_normal_(self.absolute_pos_embed, std=.02)

        self.pos_drop = nn.Dropout(p=drop_rate)

        # stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

        # build layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),
                               input_resolution=(patches_resolution[0] // (2 ** i_layer),
                                                 patches_resolution[1] // (2 ** i_layer)),
                               depth=depths[i_layer],
                               num_heads=num_heads[i_layer],
                               window_size=window_size,
                               mlp_ratio=self.mlp_ratio,
                               qkv_bias=qkv_bias, proj_bias=proj_bias, qk_scale=qk_scale,
                               drop=drop_rate, attn_drop=attn_drop_rate,
                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                               norm_layer=norm_layer,
                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                               use_checkpoint=use_checkpoint)
            self.layers.append(layer)

        if self.pose_from == 'feature':
            self.norm = norm_layer(self.num_features)
            self.avgpool = nn.AdaptiveAvgPool1d(1)
            self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
        elif self.pose_from == 'attention':
            self.head = Attention2PoseUpdate(img_size, window_size, num_heads[-1], channel)

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'absolute_pos_embed'}

    @torch.jit.ignore
    def no_weight_decay_keywords(self):
        return {'relative_position_bias_table'}

    def forward(self, x, y, geo_mask):
        batch_size = x.shape[0]
        x = self.patch_embed(x)
        geo_mask = F.avg_pool2d(geo_mask.permute(0, 3, 1, 2).float(), (self.patch_size, self.patch_size), stride=(self.patch_size, self.patch_size), divisor_override=1).permute(0, 2, 3, 1)
        
        y = self.patch_embed(y)
        if self.ape:
            x = x + self.absolute_pos_embed
            y = y + self.absolute_pos_embed
        x = self.pos_drop(x)
        y = self.pos_drop(y)

        for layer in self.layers:
            x, geo_mask, cross_atten_matrix, y = layer(x, y, geo_mask)
            # x.shape = [B, HW, C]
            # cross_atten_matrix = [B * num_windows, num_heads, window_H * window_W, window_H * window_W]
        if self.pose_from == 'feature':
            x = self.norm(x)  # B L C
            x = self.avgpool(x.transpose(1, 2))  # B C 1
            x = torch.flatten(x, 1  )
            return self.head(x)
        else:
            return self.head(cross_atten_matrix, batch_size)

    # def forward(self, x):
    #     x = self.forward_features(x)
    #     x = self.head(x)
    #     return x

    def flops(self):
        flops = 0
        flops += self.patch_embed.flops()
        for i, layer in enumerate(self.layers):
            flops += layer.flops()
        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)
        flops += self.num_features * self.num_classes
        return flops

class Attention2PoseUpdate(nn.Module):
    
    def __init__(self, img_size, window_size, num_heads, channel):
        super(Attention2PoseUpdate, self).__init__()
        
        H, W = self.img_size = img_size
        self.window_size = window_size
        self.num_heads = num_heads
        self.channel = channel
        self.register_buffer('grid', self.get_grid_points(H, W))

        self.conv1 = nn.Sequential(
            nn.Conv2d(num_heads, num_heads, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)),
            nn.ReLU(inplace=True),
            nn.Conv2d(num_heads, 1, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)),
            nn.ReLU(inplace=True),
        )
        
        self.conv2_1 = nn.Conv2d(window_size*window_size, channel, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))
           
        self.conv2_2 = nn.Conv2d(2, channel, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))
        
        self.conv3 = nn.Sequential(
            nn.Conv2d(2*channel, channel, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(inplace=True),
        )
        
        self.mlp = nn.Sequential(nn.ReLU(inplace=True),
                                     nn.Linear(channel, channel//2),
                                     nn.ReLU(inplace=True),
                                     nn.Linear(channel//2, 3),
                                     nn.Tanh())
    
    def get_grid_points(self, H, W):
        ref_y, ref_x = torch.meshgrid(
            torch.arange(0, H, dtype=torch.float32)/H,
            torch.arange(0, W, dtype=torch.float32)/W
        )
        ref_2d = torch.stack((ref_x, ref_y), -1)[None]  # [1, H, W, 2]
        return ref_2d 
        
    def forward(self, attn, batch_size):
        '''
        atten.shape = [B * num_windows, num_heads, window_H * window_W, window_H * window_W]
        '''
        
        out = self.conv1(attn)
        _, C, M, N = out.shape
        assert M == N == self.window_size*self.window_size
        assert C == 1
        out_windows = out.reshape(-1, self.window_size, self.window_size, self.window_size * self.window_size)
        out_windows_reverse = window_reverse(out_windows, self.window_size, self.img_size[0], self.img_size[1])  # [batch_size, img_H, img_W, C (window_size * window_size)]
        
        feat_proj = self.conv2_1(out_windows_reverse.permute(0, 3, 1, 2))
        coor_proj = self.conv2_2(self.grid.permute(0, 3, 1, 2).repeat(batch_size, 1, 1, 1))
        
        comb_proj = self.conv3(torch.cat([feat_proj, coor_proj], dim=1)).reshape(batch_size, self.channel, self.img_size[0] * self.img_size[1])  # [B, C, H, W]
        comb_proj = torch.mean(comb_proj, dim=-1)

        return self.mlp(comb_proj)
        


class TransOptimizerG2SP(nn.Module):
    def __init__(self,  pose_from='feature', channel=32):
        super(TransOptimizerG2SP, self).__init__()

        self.level_1 = SwinTransformer(img_size=[256, 256], patch_size=4, in_chans=64, num_classes=3,
                 embed_dim=48, depths=[1], num_heads=[3],
                 window_size=8, mlp_ratio=4., qkv_bias=False, proj_bias=False, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, pose_from=pose_from, channel=channel)

        self.level_2 = SwinTransformer(img_size=[128, 128], patch_size=4, in_chans=128, num_classes=3,
                 embed_dim=48, depths=[1], num_heads=[3],
                 window_size=8, mlp_ratio=4., qkv_bias=False, proj_bias=False, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, pose_from=pose_from, channel=channel)

        self.level_4 = SwinTransformer(img_size=[64, 64], patch_size=4, in_chans=256, num_classes=3,
                 embed_dim=48, depths=[1], num_heads=[3],
                 window_size=8, mlp_ratio=4., qkv_bias=False, proj_bias=False, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, pose_from=pose_from, channel=channel)
        

    def forward(self, pred_feat, ref_feat, geo_mask):
        # r = pred_feat - ref_feat  # [B, C, H, W]
        C = pred_feat.shape[1]
        
        if C == 256:
            x = self.level_4(pred_feat, ref_feat, geo_mask)  
        elif C == 128:
            x = self.level_2(pred_feat, ref_feat, geo_mask)
        elif C == 64:
            x = self.level_1(pred_feat, ref_feat, geo_mask)
       
        return x # [B, 3]
        


class TransOptimizerG2SPV2(nn.Module):
    def __init__(self,  pose_from='feature', channel=32):
        super(TransOptimizerG2SPV2, self).__init__()

        self.level_1 = SwinTransformer(img_size=[256, 256], patch_size=4, in_chans=64, num_classes=3,
                 embed_dim=48, depths=[2, 4], num_heads=[3, 6],
                 window_size=8, mlp_ratio=4., qkv_bias=False, proj_bias=False, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, pose_from=pose_from, channel=channel)

        self.level_2 = SwinTransformer(img_size=[128, 128], patch_size=4, in_chans=128, num_classes=3,
                 embed_dim=48, depths=[2, 4], num_heads=[3, 6],
                 window_size=8, mlp_ratio=4., qkv_bias=False, proj_bias=False, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, pose_from=pose_from, channel=channel)

        self.level_4 = SwinTransformer(img_size=[64, 64], patch_size=4, in_chans=256, num_classes=3,
                 embed_dim=48, depths=[2, 4], num_heads=[3, 6],
                 window_size=8, mlp_ratio=4., qkv_bias=False, proj_bias=False, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, pose_from=pose_from, channel=channel)
        

    def forward(self, pred_feat, ref_feat, geo_mask):
        # r = pred_feat - ref_feat  # [B, C, H, W]
        C = pred_feat.shape[1]
        
        if C == 256:
            x = self.level_4(pred_feat, ref_feat, geo_mask)  
        elif C == 128:
            x = self.level_2(pred_feat, ref_feat, geo_mask)
        elif C == 64:
            x = self.level_1(pred_feat, ref_feat, geo_mask)
       
        return x # [B, 3]

================
File: train_ford_3DoF.py
================
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

import os

# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
# os.environ['CUDA_VISIBLE_DEVICES'] = '1'

import torch

import torch.optim as optim

from dataLoader.Ford_dataset import SatGrdDatasetFord, SatGrdDatasetFordTest, train_logs, train_logs_img_inds, test_logs, test_logs_img_inds

import scipy.io as scio

import ssl

ssl._create_default_https_context = ssl._create_unverified_context  # for downloading pretrained VGG weights

from models_ford import ModelFord

import numpy as np
import os
import argparse

from torch.utils.data import DataLoader
import time

torch.autograd.set_detect_anomaly(True)

def test1(net, args, save_path, test_log_ind=1, epoch=0, device=torch.device("cuda:0")):

    net.eval()
    mini_batch = args.batch_size

    np.random.seed(2022)
    torch.manual_seed(2022)

    test_set = SatGrdDatasetFordTest(logs=test_logs[test_log_ind:test_log_ind+1],
                                 logs_img_inds=test_logs_img_inds[test_log_ind:test_log_ind+1],
                                  shift_range_lat=args.shift_range_lat, shift_range_lon=args.shift_range_lon,
                                  rotation_range=args.rotation_range, whole=args.test_whole)
    testloader = DataLoader(test_set, batch_size=mini_batch, shuffle=False, pin_memory=True,
                             num_workers=2, drop_last=False)

    pred_shifts = []
    pred_headings = []
    gt_shifts = []
    gt_headings = []

    start_time = time.time()
    with torch.no_grad():
        for i, data in enumerate(testloader, 0):
            sat_map, grd_img, gt_shift_u, gt_shift_v, gt_heading, R_FL, T_FL = \
                [item.to(device) for item in data[:-1]]

            if args.proj == 'CrossAttn':
                shifts_u, shifts_v, theta = \
                    net.CrossAttn_rot_corr(sat_map, grd_img, R_FL, T_FL, gt_shift_u, gt_shift_v, gt_heading, mode='test')
            else:
                shifts_u, shifts_v, theta = \
                    net.rot_corr(sat_map, grd_img, R_FL, T_FL, gt_shift_u, gt_shift_v, gt_heading, mode='test')

            shifts = torch.stack([shifts_u, shifts_v], dim=-1)
            headings = theta.unsqueeze(dim=-1)

            gt_shift = torch.stack([gt_shift_u, gt_shift_v], dim=-1)  # [B, 2]

            pred_shifts.append(shifts.data.cpu().numpy())
            pred_headings.append(headings.data.cpu().numpy())
            gt_shifts.append(gt_shift.data.cpu().numpy())
            gt_headings.append(gt_heading.reshape(-1, 1).data.cpu().numpy())

            if i % 20 == 0:
                print(i)

    end_time = time.time()
    duration = (end_time - start_time)/len(testloader)

    pred_shifts = np.concatenate(pred_shifts, axis=0)
    pred_headings = np.concatenate(pred_headings, axis=0) * args.rotation_range
    gt_shifts = np.concatenate(gt_shifts, axis=0) * np.array([args.shift_range_lat, args.shift_range_lon]).reshape(1, 2)
    gt_headings = np.concatenate(gt_headings, axis=0) * args.rotation_range

    scio.savemat(os.path.join(save_path, str(test_log_ind) + '_result.mat'), {'gt_shifts': gt_shifts, 'gt_headings': gt_headings,
                                                         'pred_shifts': pred_shifts, 'pred_headings': pred_headings})

    distance = np.sqrt(np.sum((pred_shifts - gt_shifts) ** 2, axis=1))  # [N]
    angle_diff = np.remainder(np.abs(pred_headings - gt_headings), 360)
    idx0 = angle_diff > 180
    angle_diff[idx0] = 360 - angle_diff[idx0]

    init_dis = np.sqrt(np.sum(gt_shifts ** 2, axis=1))
    init_angle = np.abs(gt_headings)
    diff_shifts = np.abs(pred_shifts - gt_shifts)

    diff_lats = diff_shifts[:, 0]
    diff_lons = diff_shifts[:, 1]

    gt_lats = gt_shifts[:, 0]
    gt_lons = gt_shifts[:, 1]

    metrics = [1, 3, 5]
    angles = [1, 3, 5]

    f = open(os.path.join(save_path, str(test_log_ind) + '_results.txt'), 'a')
    f.write('====================================\n')
    f.write('       EPOCH: ' + str(epoch) + '\n')
    f.write('Time per image (second): ' + str(duration) + '\n')
    print('====================================')
    print('       EPOCH: ' + str(epoch))
    print(str(test_log_ind) + ' Validation results:')

    print('Distance average: (init, pred)', np.mean(init_dis), np.mean(distance))
    print('Distance median: (init, pred)', np.median(init_dis), np.median(distance))

    print('Lateral average: (init, pred)', np.mean(np.abs(gt_lats)), np.mean(diff_lats))
    print('Lateral median: (init, pred)', np.median(np.abs(gt_lats)), np.median(diff_lats))

    print('Longitudinal average: (init, pred)', np.mean(np.abs(gt_lons)), np.mean(diff_lons))
    print('Longitudinal median: (init, pred)', np.median(np.abs(gt_lons)), np.median(diff_lons))

    print('Angle average (init, pred): ', np.mean(np.abs(gt_headings)), np.mean(angle_diff))
    print('Angle median (init, pred): ', np.median(np.abs(gt_headings)), np.median(angle_diff))

    for idx in range(len(metrics)):
        pred = np.sum(distance < metrics[idx]) / distance.shape[0] * 100
        init = np.sum(init_dis < metrics[idx]) / init_dis.shape[0] * 100

        line = 'distance within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    print('-------------------------')
    f.write('------------------------\n')

    for idx in range(len(metrics)):
        pred = np.sum(diff_lats < metrics[idx]) / diff_lats.shape[0] * 100
        init = np.sum(np.abs(gt_lats) < metrics[idx]) / gt_lats.shape[0] * 100

        line = 'lateral      within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

        pred = np.sum(diff_lons < metrics[idx]) / diff_lons.shape[0] * 100
        init = np.sum(np.abs(gt_lons) < metrics[idx]) / gt_lons.shape[0] * 100

        line = 'longitudinal within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    print('-------------------------')
    f.write('------------------------\n')

    for idx in range(len(angles)):
        pred = np.sum(angle_diff < angles[idx]) / angle_diff.shape[0] * 100
        init = np.sum(init_angle < angles[idx]) / angle_diff.shape[0] * 100
        line = 'angle within ' + str(angles[idx]) + ' degrees (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    print('====================================')
    f.write('====================================\n')
    f.close()

    net.train()

    return


def train(args, save_path, train_log_start=1, train_log_end=2):

    for epoch in range(args.resume, args.epochs):
        net.train()

        base_lr = 1e-4
        if epoch >= 2:
            base_lr = 1e-5

        optimizer = optim.Adam(net.parameters(), lr=base_lr)

        optimizer.zero_grad()

        train_set = SatGrdDatasetFord(logs=train_logs[train_log_start:train_log_end],
                                      logs_img_inds=train_logs_img_inds[train_log_start:train_log_end],
                                      shift_range_lat=args.shift_range_lat,
                                      shift_range_lon=args.shift_range_lon,
                                      rotation_range=args.rotation_range, whole=args.train_whole)
        trainloader = DataLoader(train_set, batch_size=mini_batch, shuffle=(args.visualize==0), pin_memory=True,
                                 num_workers=1, drop_last=False)

        print('batch_size:', mini_batch, '\n num of batches:', len(trainloader))

        for Loop, Data in enumerate(trainloader, 0):

            sat_map, grd_img, gt_shift_u, gt_shift_v, theta, R_FL, T_FL = \
                [item.to(device) for item in Data[:-1]]

            optimizer.zero_grad()

            if args.proj == 'CrossAttn':
                opt_loss, loss_decrease, shift_lat_decrease, shift_lon_decrease, thetas_decrease, loss_last, \
                shift_lat_last, shift_lon_last, theta_last, \
                corr_loss = \
                    net.CrossAttn_rot_corr(sat_map, grd_img, R_FL, T_FL, gt_shift_u, gt_shift_v, theta, mode='train', epoch=epoch)
            else:
                opt_loss, loss_decrease, shift_lat_decrease, shift_lon_decrease, thetas_decrease, loss_last, \
                shift_lat_last, shift_lon_last, theta_last, \
                grd_conf_list, corr_loss = \
                    net.rot_corr(sat_map, grd_img, R_FL, T_FL, gt_shift_u, gt_shift_v, theta, mode='train', epoch=epoch)

            loss = opt_loss + corr_loss * torch.exp(-net.coe_T) + net.coe_T + net.coe_R

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            if Loop % 10 == 9:  #
                level = args.level - 1

                print('Epoch: ' + str(epoch) + ' Loop: ' + str(Loop) + ' Delta: Level-' + str(level) +
                      ' loss: ' + str(np.round(loss_decrease[level].item(), decimals=4)) +
                      ' lat: ' + str(np.round(shift_lat_decrease[level].item(), decimals=2)) +
                      ' lon: ' + str(np.round(shift_lon_decrease[level].item(), decimals=2)) +
                      ' rot: ' + str(np.round(thetas_decrease[level].item(), decimals=2)))


                print('Epoch: ' + str(epoch) + ' Loop: ' + str(Loop) + ' Last: Level-' + str(level) +
                      ' loss: ' + str(np.round(loss_last[level].item(), decimals=4)) +
                      ' lat: ' + str(np.round(shift_lat_last[level].item(), decimals=2)) +
                      ' lon: ' + str(np.round(shift_lon_last[level].item(), decimals=2)) +
                      ' rot: ' + str(np.round(theta_last[level].item(), decimals=2))
                      )


                print('Epoch: ' + str(epoch) + ' Loop: ' + str(Loop) +
                      ' triplet loss: ' + str(np.round(corr_loss.item(), decimals=4)) +
                      ' coe_R: ' + str(np.round(net.coe_R.item(), decimals=2)) +
                      ' coe_T: ' + str(np.round(net.coe_T.item(), decimals=2))
                      )

        print('Save Model ...')

        if not os.path.exists(save_path):
            os.makedirs(save_path)

        torch.save(net.state_dict(), os.path.join(save_path, 'model_' + str(epoch) + '.pth'))

        test1(net, args, save_path, test_log_ind=train_log_start, epoch=epoch)

    print('Finished Training')


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--resume', type=int, default=0, help='resume the trained model')
    parser.add_argument('--test', type=int, default=1, help='test with trained model')

    parser.add_argument('--epochs', type=int, default=2, help='number of training epochs')

    parser.add_argument('--lr', type=float, default=1e-4, help='learning rate')  # 1e-2

    parser.add_argument('--rotation_range', type=float, default=10., help='degree')
    parser.add_argument('--shift_range_lat', type=float, default=20., help='meters')
    parser.add_argument('--shift_range_lon', type=float, default=20., help='meters')

    parser.add_argument('--batch_size', type=int, default=1, help='batch size')

    parser.add_argument('--level', type=int, default=3, help='2, 3, 4, -1, -2, -3, -4')
    parser.add_argument('--N_iters', type=int, default=2, help='any integer')

    parser.add_argument('--train_log_start', type=int, default=0, help='')
    parser.add_argument('--train_log_end', type=int, default=1, help='')
    parser.add_argument('--test_log_ind', type=int, default=0, help='')

    parser.add_argument('--proj', type=str, default='CrossAttn', help='geo, polar, nn, CrossAttn')

    parser.add_argument('--train_whole', type=int, default=0, help='0 or 1')
    parser.add_argument('--test_whole', type=int, default=0, help='0 or 1')

    parser.add_argument('--use_uncertainty', type=int, default=1, help='0 or 1')

    args = parser.parse_args()

    return args


def getSavePath(args):
    save_path = './ModelsFord/3DoF/' \
                + 'Log_' + str(args.train_log_start+1) + 'lat' + str(args.shift_range_lat) + 'm_lon' + str(args.shift_range_lon) + 'm_rot' + str(
        args.rotation_range) \
                + '_Nit' + str(args.N_iters) + '_' + str(args.proj)

    if args.use_uncertainty:
        save_path = save_path + '_Uncertainty'

    if not os.path.exists(save_path):
        os.makedirs(save_path)

    print('save_path:', save_path)

    return save_path


if __name__ == '__main__':

    if torch.cuda.is_available():
        device = torch.device("cuda:0")
    else:
        device = torch.device("cpu")

    np.random.seed(2022)

    args = parse_args()

    mini_batch = args.batch_size

    save_path = getSavePath(args)

    net = ModelFord(args)
    net.to(device)

    if args.test:
        net.load_state_dict(torch.load(os.path.join(save_path, 'model_4.pth')), strict=False)
        test1(net, args, save_path, args.train_log_start)

    else:
        if args.resume:
            net.load_state_dict(torch.load(os.path.join(save_path, 'model_' + str(args.resume - 1) + '.pth')))
            print("resume from " + 'model_' + str(args.resume - 1) + '.pth')

        lr = args.lr

        train(args, save_path, train_log_start=args.train_log_start, train_log_end=args.train_log_end)

================
File: LICENSE
================
MIT License

Copyright (c) 2023 Yujiao Shi

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: sam.py
================
from __future__ import annotations

from pathlib import Path
from urllib.request import urlretrieve

import torch
from segment_anything import sam_model_registry
from torch import nn
import torch.nn.functional as F
from .dpt import DPT



class SAM(nn.Module):
    def __init__(self, arch="vit_b", output="dense", layer=-1, return_multilayer=False,
                 output_channels = 320):
        super().__init__()

        assert output in ["gap", "dense"], "Options: [gap, dense]"
        self.output = output
        self.checkpoint_name = f"sam_{arch}"
        ckpt_paths = {
            "vit_b": "sam_vit_b_01ec64.pth",
            "vit_l": "sam_vit_l_0b3195.pth",
            "vit_h": "sam_vit_h_4b8939.pth",
        }

        ckpt_file = ckpt_paths[arch]
        ckpt_path = Path(__file__).parent / "checkpoint_weights" / ckpt_file

        if not ckpt_path.exists():
            download_path = (
                f"https://dl.fbaipublicfiles.com/segment_anything/{ckpt_file}"
            )
            urlretrieve(download_path, ckpt_path)

        sam = sam_model_registry[arch](checkpoint=ckpt_path)
        vit = sam.image_encoder

        feat_dim = vit.neck[0].in_channels
        emb_h, emb_w = vit.pos_embed.shape[1:3]
        self.patch_size = vit.patch_embed.proj.kernel_size[0]
        self.image_size = (emb_h * self.patch_size, emb_w * self.patch_size)
        assert self.patch_size == 16

        vit.pos_embed = nn.Parameter(
            torch.zeros(1, 32, 32, 768)
        )

        self.vit = vit

        # frozen
        for param in self.vit.parameters():
            param.requires_grad = False

        num_layers = len(self.vit.blocks)
        multilayers = [
            num_layers // 4 - 1,
            num_layers // 2 - 1,
            num_layers // 4 * 3 - 1,
            num_layers - 1,
        ]

        if return_multilayer:
            self.feat_dim = [feat_dim, feat_dim, feat_dim, feat_dim]
            self.multilayers = multilayers
        else:
            self.feat_dim = feat_dim
            layer = multilayers[-1] if layer == -1 else layer
            self.multilayers = [layer]

        # define layer name (for logging)
        self.layer = "-".join(str(_x) for _x in self.multilayers)

        self.flatten = nn.Flatten(2)
        self.mlp = MLP(input_dim=768, hidden_dim=512, output_dim=320)
        self.unflatten = nn.Unflatten(2, (32, 32))
        self.conv_down = nn.Conv2d(in_channels=320, out_channels=320, kernel_size=3, stride=2, padding=1)
        self.relu = nn.ReLU()

        # self.dpt = DPT(self.feat_dim, output_channels)

    def resize_pos_embed(self, image_size):
        # get embed size
        h, w = image_size
        h = h // self.patch_size
        w = w // self.patch_size

        # resize embed
        pos_embed = self.vit.pos_embed.data.permute(0, 3, 1, 2)
        pos_embed = torch.nn.functional.interpolate(
            pos_embed, size=(h, w), mode="bicubic"
        )
        pos_embed = pos_embed.permute(0, 2, 3, 1)
        self.vit.pos_embed.data = pos_embed
        self.image_size = image_size

    def forward(self, x):
        # with torch.no_grad():
        _, _, h, w = x.shape
        assert h % self.patch_size == 0 and w % self.patch_size == 0, f"{h}, {w}"

        # if h != self.image_size[0] or w != self.image_size[1]:
        #     self.resize_pos_embed(image_size=(h, w))

        # run vit
        x = self.vit.patch_embed(x)
        if self.vit.pos_embed is not None:
            x = x + self.vit.pos_embed

        embeds = []
        for i, blk in enumerate(self.vit.blocks):
            x = blk(x)
            if i in self.multilayers:
                embeds.append(x)
                if len(embeds) == len(self.multilayers):
                    break

        # feat shape is batch x feat_dim x height x width
        embeds = [_emb.permute(0, 3, 1, 2).contiguous() for _emb in embeds]

        if self.output == "gap":
            embeds = [x.mean(dim=(-2, -1)) for x in embeds]

        # pure_sam = embeds[0]
        # embeds = self.dpt(embeds)
        # output_tensor = F.interpolate(embeds, size=(16, 16), mode='bilinear', align_corners=True)
        # return pure_sam, output_tensor

        x = embeds[0]
        fmap = x
        x = self.flatten(x)  #  [16, 768, 1024]
        x = x.permute(0, 2, 1)  #  [16, 1024, 768]  MLP
        x = self.mlp(x)  #  [16, 1024, 320]
        x = x.permute(0, 2, 1)  #  [16, 320, 1024]

        x = self.unflatten(x)  #  [16, 320, 16, 16]
        x = self.conv_down(x)
        x = self.relu(x)
        embeds[0] = x


        # return embeds[0] if len(embeds) == 1 else embeds
        return fmap, x


class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

================
File: dino.py
================
import time

import torch

from .utils.utils import center_padding, tokens_to_output
from torch.nn.functional import interpolate
import torch.nn as nn
import torch.nn.functional as F

from .dpt import DPT

class DINO(torch.nn.Module):
    def __init__(
        self,
        dino_name="dinov2",
        model_name="vitb14",
        output="dense",
        layer=-1,
        return_multilayer=True,
        output_channels=112,
        hidden_channels1=544,
        hidden_channels2=465,
        down_sample=False,
    ):
        super().__init__()
        feat_dims = {
            "vitb8": 768,
            "vitb16": 768,
            "vitb14": 768,
            "vitb14_reg": 768,
            "vitl14": 1024,
            "vitg14": 1536,
        }

        # get model
        self.model_name = dino_name
        self.down_sample = down_sample
        self.checkpoint_name = f"{dino_name}_{model_name}"
        torch.hub._validate_not_a_forked_repo = lambda a, b, c: True
        dino_vit = torch.hub.load(f"facebookresearch/{dino_name}", self.checkpoint_name)
        self.vit = dino_vit.eval().to(torch.float32)
        for param in dino_vit.parameters():
            param.requires_grad = False
        self.has_registers = "_reg" in model_name

        self.flatten = nn.Flatten(2)
        self.mlp = MLP(input_dim=768, hidden_dim=512, output_dim=320)
        self.unflatten = nn.Unflatten(2, (37, 37))
        self.conv_down = nn.Conv2d(in_channels=320, out_channels=320, kernel_size=3, stride=2, padding=1)
        self.relu = nn.ReLU()



        assert output in ["cls", "gap", "dense", "dense-cls"]
        self.output = output
        self.patch_size = self.vit.patch_embed.proj.kernel_size[0]

        feat_dim = feat_dims[model_name]
        feat_dim = feat_dim * 2 if output == "dense-cls" else feat_dim

        num_layers = len(self.vit.blocks)
        # print("num_layers: ", num_layers)
        # multilayers = [
        #     num_layers // 2 - 1,  # dinov2 vitb14 num_layers=12
        #     num_layers // 2 + 1,
        #     num_layers // 4 * 3,
        #     num_layers - 1,
        # ]
        multilayers = [
            num_layers // 4 - 1,
            num_layers // 2 - 1,
            num_layers // 4 * 3 - 1,
            num_layers - 1,
        ]

        if return_multilayer:
            self.feat_dim = [feat_dim, feat_dim, feat_dim, feat_dim]
            self.multilayers = multilayers
        else:
            self.feat_dim = feat_dim
            layer = multilayers[-1] if layer == -1 else layer
            self.multilayers = [layer]

        self.dpt = DPT(self.feat_dim, output_channels)

        # define layer name (for logging)
        self.layer = "-".join(str(_x) for _x in self.multilayers)

    def forward(self, images):

        images = center_padding(images, self.patch_size)
        h, w = images.shape[-2:]
        h, w = h // self.patch_size, w // self.patch_size

        if self.model_name == "dinov2":
            x = self.vit.prepare_tokens_with_masks(images, None)
        else:
            x = self.vit.prepare_tokens(images)

        embeds = []
        for i, blk in enumerate(self.vit.blocks):
            x = blk(x)
            if i in self.multilayers:
                embeds.append(x)
                if len(embeds) == len(self.multilayers):
                    break

        num_spatial = h * w
        outputs = []
        for i, x_i in enumerate(embeds):
            cls_tok = x_i[:, 0]
            # ignoring register tokens
            spatial = x_i[:, -1 * num_spatial :]
            x_i = tokens_to_output(self.output, spatial, cls_tok, (h, w))
            outputs.append(x_i)

        res = self.dpt(outputs)
        x = F.interpolate(res, size=(16,16), mode='bilinear', align_corners=True)

        # x = outputs[0] # shape (16, 768, 37,37)
        # x = self.flatten(x)  #  [16, 768, 1024]
        # x = x.permute(0, 2, 1)  #  [16, 1024, 768]  MLP
        # x = self.mlp(x)  #  [16, 1024, 320]
        # x = x.permute(0, 2, 1)  #  [16, 320, 1024]
        #
        # x = self.unflatten(x)  #  [16, 320, 16, 16]
        # x = self.conv_down(x)
        # x = self.relu(x)
        # x = self.conv_down(x)
        # x = self.relu(x)
        # x = F.interpolate(x, size=(16, 16), mode='bilinear', align_corners=True)
        # print(f"dino feat shape: {x.shape}")


        return outputs[0], x

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

================
File: train_oxford_2DoF.py
================
import os

import torchvision.utils

# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
# os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1, 2'

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms
from dataLoader.Oxford_dataset import load_train_data, load_val_data, load_test_data
from torch.utils.tensorboard import SummaryWriter
import torch.nn.functional as F
import scipy.io as scio

import ssl

ssl._create_default_https_context = ssl._create_unverified_context  # for downloading pretrained VGG weights

from model_oxford import ModelOxford

import numpy as np
import os
import argparse


class MultiGPU(nn.DataParallel):
    def __getattr__(self, item):
        try:
            return super().__getattr__(item)
        except:
            pass
        return getattr(self.module, item)


def val(net_test, args, save_path, best_rank_result, epoch):
    ### net evaluation state
    net_test.eval()

    dataloader = load_val_data(mini_batch, args.rotation_range, ori_sat_res=args.sat_ori_res)

    pred_us = []
    pred_vs = []
    # pred_oriens = []

    gt_us = []
    gt_vs = []


    for i, data in enumerate(dataloader, 0):

        sat_map, left_camera_k, grd_left_imgs, gt_shift_u, gt_shift_v, gt_heading = [item.to(device) for item in
                                                                                     data]
        if args.proj == 'CrossAttn':
            pred_u, pred_v = net_test(sat_map, grd_left_imgs, left_camera_k,
                                                                  gt_heading=gt_heading, mode='test')
        else:
            pred_u, pred_v = net_test.corr(sat_map, grd_left_imgs, left_camera_k, gt_heading=gt_heading,
                                                           mode='test')

        pred_us.append(pred_u.data.cpu().numpy())
        pred_vs.append(pred_v.data.cpu().numpy())

        gt_us.append(gt_shift_u[:, 0].data.cpu().numpy())
        gt_vs.append(gt_shift_v[:, 0].data.cpu().numpy())

        if i % 20 == 0:
            print(i)

    pred_us = np.concatenate(pred_us, axis=0)
    pred_vs = np.concatenate(pred_vs, axis=0)
    # pred_oriens = np.concatenate(pred_oriens, axis=0)

    gt_us = np.concatenate(gt_us, axis=0)
    gt_vs = np.concatenate(gt_vs, axis=0)
    # gt_oriens = np.concatenate(gt_oriens, axis=0)

    scio.savemat(os.path.join(save_path, 'result.mat'), {'gt_us': gt_us, 'gt_vs': gt_vs,
                                                         'pred_us': pred_us, 'pred_vs': pred_vs,
                                                         })
    meter_per_pixel = 0.0924 * args.sat_ori_res / 512
    distance = np.sqrt((pred_us - gt_us) ** 2 + (pred_vs - gt_vs) ** 2) * meter_per_pixel  # [N]
    init_dis = np.sqrt(gt_us ** 2 + gt_vs ** 2) * meter_per_pixel

    # angle_diff = np.remainder(np.abs(pred_oriens - gt_oriens), 360)
    # idx0 = angle_diff > 180
    # angle_diff[idx0] = 360 - angle_diff[idx0]
    #
    # init_angle = np.abs(gt_oriens)

    metrics = [1, 3, 5]
    angles = [1, 3, 5]

    f = open(os.path.join(save_path, 'results.txt'), 'a')
    f.write('====================================\n')
    f.write('       EPOCH: ' + str(epoch) + '\n')
    print('====================================')
    print('       EPOCH: ' + str(epoch))
    print('Test1 results:')

    line = 'Distance average: (init, pred)' + str(np.mean(init_dis)) + ' ' + str(np.mean(distance))
    print(line)
    f.write(line + '\n')
    
    line ='Distance median: (init, pred)' + str(np.median(init_dis)) + ' ' + str(np.median(distance))
    print(line)
    f.write(line + '\n')

    for idx in range(len(metrics)):
        pred = np.sum(distance < metrics[idx]) / distance.shape[0] * 100
        init = np.sum(init_dis < metrics[idx]) / init_dis.shape[0] * 100

        line = 'distance within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    # print('-------------------------')
    # f.write('------------------------\n')
    #
    # for idx in range(len(angles)):
    #     pred = np.sum(angle_diff < angles[idx]) / angle_diff.shape[0] * 100
    #     init = np.sum(init_angle < angles[idx]) / angle_diff.shape[0] * 100
    #     line = 'angle within ' + str(angles[idx]) + ' degrees (init, pred): ' + str(init) + ' ' + str(pred)
    #     print(line)
    #     f.write(line + '\n')

    print('====================================')
    f.write('====================================\n')
    f.close()
    result = np.mean(distance)

    net_test.train()

    ### save the best params
    if (result < best_rank_result):
        if not os.path.exists(save_path):
            os.makedirs(save_path)
        torch.save(net_test.state_dict(), os.path.join(save_path, 'Model_best.pth'))

    return result


def test(net_test, args, save_path, epoch, test_set):

    net_test.eval()

    dataloader = load_test_data(mini_batch, args.rotation_range, test_set, ori_sat_res=args.sat_ori_res)

    pred_us = []
    pred_vs = []

    gt_us = []
    gt_vs = []

    with torch.no_grad():
        for i, data in enumerate(dataloader, 0):
            sat_map, left_camera_k, grd_left_imgs, gt_shift_u, gt_shift_v, gt_heading = [item.to(device) for item in
                                                                                         data]
            if args.proj == 'CrossAttn':
                pred_u, pred_v = net_test(sat_map, grd_left_imgs, left_camera_k, gt_heading=gt_heading, mode='test')
            else:
                pred_u, pred_v = net_test.corr(sat_map, grd_left_imgs, left_camera_k, gt_heading=gt_heading,
                                                               mode='test')

            pred_us.append(pred_u.data.cpu().numpy())
            pred_vs.append(pred_v.data.cpu().numpy())

            gt_us.append(gt_shift_u[:, 0].data.cpu().numpy() )
            gt_vs.append(gt_shift_v[:, 0].data.cpu().numpy() )

            if i % 20 == 0:
                print(i)

    pred_us = np.concatenate(pred_us, axis=0)
    pred_vs = np.concatenate(pred_vs, axis=0)

    gt_us = np.concatenate(gt_us, axis=0)
    gt_vs = np.concatenate(gt_vs, axis=0)

    scio.savemat(os.path.join(save_path, 'Sat_ori_res' + str(args.sat_ori_res) + 'test' + str(test_set) + '_result.mat'), {'gt_us': gt_us, 'gt_vs': gt_vs,
                                                         'pred_us': pred_us, 'pred_vs': pred_vs,
                                                         })

    meter_per_pixel = 0.0924 * args.sat_ori_res / 512
    distance = np.sqrt((pred_us - gt_us) ** 2 + (pred_vs - gt_vs) ** 2) * meter_per_pixel  # [N]
    init_dis = np.sqrt(gt_us ** 2 + gt_vs ** 2) * meter_per_pixel

    diff_lats = np.abs((pred_us - gt_us)) * meter_per_pixel
    diff_lons = np.abs((pred_vs - gt_vs)) * meter_per_pixel

    metrics = [1, 3, 5]

    f = open(os.path.join(save_path, 'Sat_ori_res' + str(args.sat_ori_res) + 'test' + str(test_set) + '_results.txt'), 'a')
    f.write('====================================\n')
    f.write('       EPOCH: ' + str(epoch) + '\n')
    print('====================================')
    print('       EPOCH: ' + str(epoch))
    print('Test', test_set, ' results:')

    line = 'Distance average: (init, pred) ' + str(np.mean(init_dis)) + ' ' + str(np.mean(distance))
    print(line)
    f.write(line + '\n')

    line = 'Distance average: (init, pred) ' + str(np.median(init_dis)) + ' ' + str(np.median(distance))
    print(line)
    f.write(line + '\n')

    for idx in range(len(metrics)):
        pred = np.sum(distance < metrics[idx]) / distance.shape[0] * 100
        init = np.sum(init_dis < metrics[idx]) / init_dis.shape[0] * 100

        line = 'distance within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

    print('-------------------------')
    f.write('------------------------\n')

    for idx in range(len(metrics)):
        pred = np.sum(diff_lats < metrics[idx]) / diff_lats.shape[0] * 100
        init = np.sum(np.abs(gt_us) < metrics[idx]) / gt_us.shape[0] * 100

        line = 'lateral      within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')

        pred = np.sum(diff_lons < metrics[idx]) / diff_lons.shape[0] * 100
        init = np.sum(np.abs(gt_vs) < metrics[idx]) / gt_vs.shape[0] * 100

        line = 'longitudinal within ' + str(metrics[idx]) + ' meters (init, pred): ' + str(init) + ' ' + str(pred)
        print(line)
        f.write(line + '\n')


    print('====================================')
    f.write('====================================\n')
    f.close()

    net_test.train()

    return np.mean(distance), np.median(distance)


def triplet_loss(corr_maps, gt_shift_u, gt_shift_v, gt_heading):
    
   
    losses = []
    for level in range(len(corr_maps)):

        corr = corr_maps[level]
        B, corr_H, corr_W = corr.shape

        w = torch.round(corr_W / 2 - 0.5 + gt_shift_u / np.power(2, 3 - level)).reshape(-1)
        h = torch.round(corr_H / 2 - 0.5 + gt_shift_v / np.power(2, 3 - level)).reshape(-1)

        pos = corr[range(B), h.long(), w.long()]  # [B]

        pos_neg = pos.reshape(-1, 1, 1) - corr  # [B, H, W]
        loss = torch.sum(torch.log(1 + torch.exp(pos_neg * 10))) / (B * (corr_H * corr_W - 1))

        losses.append(loss)

    return torch.sum(torch.stack(losses, dim=0))


def train(net, args, save_path):
    bestRankResult = 0.0

    for epoch in range(args.resume, args.epochs):
        net.train()

        base_lr = 1e-4

        optimizer = optim.Adam(net.parameters(), lr=base_lr)
        optimizer.zero_grad()

        trainloader = load_train_data(mini_batch, args.rotation_range, ori_sat_res=args.sat_ori_res)

        loss_vec = []

        print('batch_size:', mini_batch, '\n num of batches:', len(trainloader))

        for Loop, Data in enumerate(trainloader, 0):
            # get the inputs

            sat_map, left_camera_k, grd_left_imgs, gt_shift_u, gt_shift_v, gt_heading = [item.to(device) for item in
                                                                                         Data]


            if args.proj == 'CrossAttn':

                corr_maps0, corr_maps1, corr_maps2 = net(sat_map, grd_left_imgs, left_camera_k, gt_shift_u, gt_shift_v,
                                                gt_heading, mode='train', epoch=epoch)
                loss = triplet_loss([corr_maps0, corr_maps1, corr_maps2], gt_shift_u, gt_shift_v, gt_heading)
            else:
                loss = \
                    net.corr(sat_map, grd_left_imgs, left_camera_k, gt_shift_u, gt_shift_v, gt_heading,
                                 mode='train', epoch=epoch)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()  # This step is responsible for updating weights


            loss_vec.append(loss.item())

            if Loop % 10 == 9:  #

                print('Epoch: ' + str(epoch) + ' Loop: ' + str(Loop) +
                      ' triplet loss: ' + str(np.round(loss.item(), decimals=4))
                      )

        print('Save Model ...')
        if not os.path.exists(save_path):
            os.makedirs(save_path)

        torch.save(net.state_dict(), os.path.join(save_path, 'model_' + str(epoch) + '.pth'))

        current = val(net, args, save_path, bestRankResult, epoch)
        if (current > bestRankResult):
            bestRankResult = current

    print('Finished Training')


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--resume', type=int, default=0, help='resume the trained model')
    parser.add_argument('--test', type=int, default=0, help='test with trained model')
    parser.add_argument('--debug', type=int, default=0, help='debug to dump middle processing images')

    parser.add_argument('--epochs', type=int, default=20, help='number of training epochs')

    parser.add_argument('--rotation_range', type=float, default=0., help='degree')

    parser.add_argument('--batch_size', type=int, default=14, help='batch size')

    parser.add_argument('--N_iters', type=int, default=2, help='any integer')

    parser.add_argument('--Optimizer', type=str, default='TransV1', help='LM or SGD')

    parser.add_argument('--proj', type=str, default='CrossAttn', help='geo, polar, nn, CrossAttn')

    parser.add_argument('--use_uncertainty', type=int, default=1, help='0 or 1')
    parser.add_argument('--multi_gpu', type=int, default=1, help='0 or 1')

    parser.add_argument('--sat_ori_res', type=int, default=800, help='original satellite image resolution, default is 800')
    parser.add_argument('--grd_rand_shift_pixels', type=int, default=200,
                        help='random shift pixel of the ground camera with respect to its satellite image, default is 200')
    parser.add_argument('--ori_meter_per_pixel', type=float, default=0.0924,
                        help='meter per pixel of the original satellite image provided by the dataset, fixed, plz do not change')

    parser.add_argument('--test_epoch', type=int, default=19, help='19')

    args = parser.parse_args()

    return args


def getSavePath(args):
    save_path = './ModelsOxford/2DoF/' + str(args.proj)

    if args.use_uncertainty:
        save_path = save_path + '_Uncertainty'

    if args.sat_ori_res != 800:
        save_path = save_path + '_SatOriRes' + str(args.sat_ori_res)

    if args.grd_rand_shift_pixels != 800:
        save_path = save_path + '_RandShift' + str(args.grd_rand_shift_pixels)

    if not os.path.exists(save_path):
        os.makedirs(save_path)

    print('save_path:', save_path)

    return save_path


if __name__ == '__main__':

    if torch.cuda.is_available():
        device = torch.device("cuda:0")
    else:
        device = torch.device("cpu")

    np.random.seed(2022)
    
    print(torch.cuda.device_count())

    args = parse_args()

    mini_batch = args.batch_size

    save_path = getSavePath(args)

    net = ModelOxford(args)
    
    if args.multi_gpu:
        net = nn.DataParallel(net, dim=0)
    net.to(device)

    if args.test:
        net.load_state_dict(torch.load(os.path.join(save_path, 'model_' + str(args.test_epoch) + '.pth')))

        # mean_dis1, median_dis1 = test(net, args, save_path, epoch=args.test_epoch, test_set=0)
        mean_dis1, median_dis1 = test(net, args, save_path, epoch=args.test_epoch, test_set=1)
        # mean_dis2, median_dis2 = test(net, args, save_path, epoch=args.test_epoch, test_set=2)
        # mean_dis3, median_dis3 = test(net, args, save_path, epoch=args.test_epoch, test_set=3)

    else:

        if args.resume:
            net.load_state_dict(torch.load(os.path.join(save_path, 'model_' + str(args.resume - 1) + '.pth')))
            print("resume from " + 'model_' + str(args.resume - 1) + '.pth')

        train(net, args, save_path)

================
File: dataLoader/KITTI_dataset.py
================
import random

import numpy as np
import os
from PIL import Image
from torch.utils.data import Dataset

import torch
import pandas as pd
import utils
import torchvision.transforms.functional as TF
from torchvision import transforms
import torch.nn.functional as F

from torch.utils.data import DataLoader
from torchvision import transforms

root_dir = '../../dataset/Kitti1' # '../../data/Kitti' # '../Data' #'..\\Data' #
# root_dir = '/media/yujiao/6TB/dataset/Kitti1'

test_csv_file_name = 'test.csv'
ignore_csv_file_name = 'ignore.csv'
satmap_dir = 'satmap'
grdimage_dir = 'raw_data'
left_color_camera_dir = 'image_02/data'  # 'image_02\\data' #
right_color_camera_dir = 'image_03/data'  # 'image_03\\data' #
oxts_dir = 'oxts/data'  # 'oxts\\data' #
# depth_dir = 'depth/data_depth_annotated/train/'

GrdImg_H = 256  # 256 # original: 375 #224, 256
GrdImg_W = 1024  # 1024 # original:1242 #1248, 1024
GrdOriImg_H = 375
GrdOriImg_W = 1242
num_thread_workers = 2

# train_file = './dataLoader/train_files.txt'
train_file = './dataLoader/train_files.txt'
test1_file = './dataLoader/test1_files.txt'
test2_file = './dataLoader/test2_files.txt'


# def depth_read(filename):
#     # loads depth map D from png file
#     # and returns it as a numpy array,
#     # for details see readme.txt

#     depth_png = np.array(Image.open(filename), dtype=int)
#     # make sure we have a proper 16bit depth map here.. not 8bit!
#     assert(np.max(depth_png) > 255)

#     depth = depth_png.astype(np.float) / 256.
#     depth[depth_png == 0] = -1.
#     return depth


class SatGrdDataset(Dataset):
    def __init__(self, root, file,
                 transform=None, shift_range_lat=20, shift_range_lon=20, rotation_range=10):
        self.root = root

        self.meter_per_pixel = utils.get_meter_per_pixel(scale=1)
        self.shift_range_meters_lat = shift_range_lat  # in terms of meters
        self.shift_range_meters_lon = shift_range_lon  # in terms of meters
        self.shift_range_pixels_lat = shift_range_lat / self.meter_per_pixel  # shift range is in terms of meters
        self.shift_range_pixels_lon = shift_range_lon / self.meter_per_pixel  # shift range is in terms of meters

        # self.shift_range_meters = shift_range  # in terms of meters

        self.rotation_range = rotation_range  # in terms of degree

        self.skip_in_seq = 2  # skip 2 in sequence: 6,3,1~
        if transform != None:
            self.satmap_transform = transform[0]
            self.grdimage_transform = transform[1]

        self.pro_grdimage_dir = 'raw_data'

        self.satmap_dir = satmap_dir

        with open(file, 'r') as f:
            file_name = f.readlines()

        # np.random.seed(2022)
        # num = len(file_name)//3
        # random.shuffle(file_name)
        # self.file_name = [file[:-1] for file in file_name[:num]]
        self.file_name = [file[:-1] for file in file_name]
        # self.file_name = []
        # count = 0
        # for file in file_name:
        #     left_depth_name = os.path.join(self.root, depth_dir, file.split('/')[1],
        #                                    'proj_depth/groundtruth/image_02', os.path.basename(file.strip()))
        #     if os.path.exists(left_depth_name):
        #         self.file_name.append(file.strip())
        #     else:
        #         count += 1
        #
        # print('number of files whose depth unavailable: ', count)


    def __len__(self):
        return len(self.file_name)

    def get_file_list(self):
        return self.file_name

    def __getitem__(self, idx):
        # read cemera k matrix from camera calibration files, day_dir is first 10 chat of file name

        file_name = self.file_name[idx]
        day_dir = file_name[:10]
        drive_dir = file_name[:38]
        image_no = file_name[38:]

        # =================== read camera intrinsice for left and right cameras ====================
        calib_file_name = os.path.join(self.root, grdimage_dir, day_dir, 'calib_cam_to_cam.txt')
        with open(calib_file_name, 'r') as f:
            lines = f.readlines()
            for line in lines:
                # left color camera k matrix
                if 'P_rect_02' in line:
                    # get 3*3 matrix from P_rect_**:
                    items = line.split(':')
                    valus = items[1].strip().split(' ')
                    fx = float(valus[0]) * GrdImg_W / GrdOriImg_W
                    cx = float(valus[2]) * GrdImg_W / GrdOriImg_W
                    fy = float(valus[5]) * GrdImg_H / GrdOriImg_H
                    cy = float(valus[6]) * GrdImg_H / GrdOriImg_H
                    left_camera_k = [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]
                    left_camera_k = torch.from_numpy(np.asarray(left_camera_k, dtype=np.float32))
                    # if not self.stereo:
                    break

        # =================== read satellite map ===================================
        SatMap_name = os.path.join(self.root, self.satmap_dir, file_name)
        with Image.open(SatMap_name, 'r') as SatMap:
            sat_map = SatMap.convert('RGB')

        # =================== initialize some required variables ============================
        grd_left_imgs = torch.tensor([])
        grd_left_depths = torch.tensor([])
        image_no = file_name[38:]

        # oxt: such as 0000000000.txt
        oxts_file_name = os.path.join(self.root, grdimage_dir, drive_dir, oxts_dir,
                                      image_no.lower().replace('.png', '.txt'))
        with open(oxts_file_name, 'r') as f:
                content = f.readline().split(' ')
                # get heading
                heading = float(content[5])
                heading = torch.from_numpy(np.asarray(heading))

                left_img_name = os.path.join(self.root, self.pro_grdimage_dir, drive_dir, left_color_camera_dir,
                                             image_no.lower())
                with Image.open(left_img_name, 'r') as GrdImg:
                    grd_img_left = GrdImg.convert('RGB')
                    if self.grdimage_transform is not None:
                        grd_img_left = self.grdimage_transform(grd_img_left)

                # left_depth_name = os.path.join(self.root, depth_dir, file_name.split('/')[1], 'proj_depth/groundtruth/image_02', image_no)

                # left_depth = torch.tensor(depth_read(left_depth_name), dtype=torch.float32)
                # left_depth = F.interpolate(left_depth[None, None, :, :], (GrdImg_H, GrdImg_W))
                # left_depth = left_depth[0, 0]

                grd_left_imgs = torch.cat([grd_left_imgs, grd_img_left.unsqueeze(0)], dim=0)
                # grd_left_depths = torch.cat([grd_left_depths, left_depth.unsqueeze(0)], dim=0)

        sat_rot = sat_map.rotate(-heading / np.pi * 180)
        sat_align_cam = sat_rot.transform(sat_rot.size, Image.AFFINE,
                                          (1, 0, utils.CameraGPS_shift_left[0] / self.meter_per_pixel,
                                           0, 1, utils.CameraGPS_shift_left[1] / self.meter_per_pixel),
                                          resample=Image.BILINEAR)
        # the homography is defined on: from target pixel to source pixel
        # now east direction is the real vehicle heading direction

        # randomly generate shift
        gt_shift_x = np.random.uniform(-1, 1)  # --> right as positive, parallel to the heading direction
        gt_shift_y = np.random.uniform(-1, 1)  # --> up as positive, vertical to the heading direction

        sat_rand_shift = \
            sat_align_cam.transform(
                sat_align_cam.size, Image.AFFINE,
                (1, 0, gt_shift_x * self.shift_range_pixels_lon,
                 0, 1, -gt_shift_y * self.shift_range_pixels_lat),
                resample=Image.BILINEAR)

        # randomly generate roation
        theta = np.random.uniform(-1, 1)
        sat_rand_shift_rand_rot = \
            sat_rand_shift.rotate(theta * self.rotation_range)

        sat_map =TF.center_crop(sat_rand_shift_rand_rot, utils.SatMap_process_sidelength)
        # sat_map = np.array(sat_map, dtype=np.float32)

        # transform
        if self.satmap_transform is not None:
            sat_map = self.satmap_transform(sat_map)
        
        # gt_corr_x, gt_corr_y = self.generate_correlation_GTXY(gt_shift_x, gt_shift_y, theta)

        return sat_map, left_camera_k, grd_left_imgs[0], \
               torch.tensor(-gt_shift_x, dtype=torch.float32).reshape(1), \
               torch.tensor(-gt_shift_y, dtype=torch.float32).reshape(1), \
               torch.tensor(theta, dtype=torch.float32).reshape(1), \
               file_name


    # def generate_correlation_GTXY(self, gt_shift_x, gt_shift_y, gt_heading):
        
    #     cos = np.cos(gt_heading * self.rotation_range / 180 * np.pi)
    #     sin = np.sin(gt_heading * self.rotation_range / 180 * np.pi)
        
    #     gt_corr_x = - gt_shift_x * cos + gt_shift_y * sin
    #     gt_corr_y = gt_shift_x * sin + gt_shift_y * cos
        
    #     return gt_corr_x, gt_corr_y
        
        
        



class SatGrdDatasetTest(Dataset):
    def __init__(self, root, file,
                 transform=None, shift_range_lat=20, shift_range_lon=20, rotation_range=10):
        self.root = root

        self.meter_per_pixel = utils.get_meter_per_pixel(scale=1)
        self.shift_range_meters_lat = shift_range_lat  # in terms of meters
        self.shift_range_meters_lon = shift_range_lon  # in terms of meters
        self.shift_range_pixels_lat = shift_range_lat / self.meter_per_pixel  # shift range is in terms of meters
        self.shift_range_pixels_lon = shift_range_lon / self.meter_per_pixel  # shift range is in terms of meters

        # self.shift_range_meters = shift_range  # in terms of meters

        self.rotation_range = rotation_range  # in terms of degree

        self.skip_in_seq = 2  # skip 2 in sequence: 6,3,1~
        if transform != None:
            self.satmap_transform = transform[0]
            self.grdimage_transform = transform[1]

        self.pro_grdimage_dir = 'raw_data'

        self.satmap_dir = satmap_dir

        with open(file, 'r') as f:
            file_name = f.readlines()

        # np.random.seed(2022)
        # num = len(file_name)//3
        # random.shuffle(file_name)
        # self.file_name = [file[:-1] for file in file_name[:num]]
        self.file_name = [file[:-1] for file in file_name]
        # self.file_name = []
        # count = 0
        # for line in file_name:
        #     file = line.split(' ')[0]
        #     left_depth_name = os.path.join(self.root, depth_dir, file.split('/')[1],
        #                                    'proj_depth/groundtruth/image_02', os.path.basename(file.strip()))
        #     if os.path.exists(left_depth_name):
        #         self.file_name.append(line.strip())
        #     else:
        #         count += 1
        #
        # print('number of files whose depth unavailable: ', count)


    def __len__(self):
        return len(self.file_name)

    def get_file_list(self):
        return self.file_name

    def __getitem__(self, idx):
        # read cemera k matrix from camera calibration files, day_dir is first 10 chat of file name

        line = self.file_name[idx]
        file_name, gt_shift_x, gt_shift_y, theta = line.split(' ')
        day_dir = file_name[:10]
        drive_dir = file_name[:38]
        image_no = file_name[38:]

        # =================== read camera intrinsice for left and right cameras ====================
        calib_file_name = os.path.join(self.root, grdimage_dir, day_dir, 'calib_cam_to_cam.txt')
        with open(calib_file_name, 'r') as f:
            lines = f.readlines()
            for line in lines:
                # left color camera k matrix
                if 'P_rect_02' in line:
                    # get 3*3 matrix from P_rect_**:
                    items = line.split(':')
                    valus = items[1].strip().split(' ')
                    fx = float(valus[0]) * GrdImg_W / GrdOriImg_W
                    cx = float(valus[2]) * GrdImg_W / GrdOriImg_W
                    fy = float(valus[5]) * GrdImg_H / GrdOriImg_H
                    cy = float(valus[6]) * GrdImg_H / GrdOriImg_H
                    left_camera_k = [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]
                    left_camera_k = torch.from_numpy(np.asarray(left_camera_k, dtype=np.float32))
                    # if not self.stereo:
                    break

        # =================== read satellite map ===================================
        SatMap_name = os.path.join(self.root, self.satmap_dir, file_name)
        with Image.open(SatMap_name, 'r') as SatMap:
            sat_map = SatMap.convert('RGB')

        # =================== initialize some required variables ============================
        grd_left_imgs = torch.tensor([])
        grd_left_depths = torch.tensor([])
        # image_no = file_name[38:]

        # oxt: such as 0000000000.txt
        oxts_file_name = os.path.join(self.root, grdimage_dir, drive_dir, oxts_dir,
                                      image_no.lower().replace('.png', '.txt'))
        with open(oxts_file_name, 'r') as f:
            content = f.readline().split(' ')
            # get heading
            heading = float(content[5])
            heading = torch.from_numpy(np.asarray(heading))

            left_img_name = os.path.join(self.root, self.pro_grdimage_dir, drive_dir, left_color_camera_dir,
                                         image_no.lower())
            with Image.open(left_img_name, 'r') as GrdImg:
                grd_img_left = GrdImg.convert('RGB')
                if self.grdimage_transform is not None:
                    grd_img_left = self.grdimage_transform(grd_img_left)

            # left_depth_name = os.path.join(self.root, depth_dir, file_name.split('/')[1],
            #                                'proj_depth/groundtruth/image_02', image_no)

            # left_depth = torch.tensor(depth_read(left_depth_name), dtype=torch.float32)
            # left_depth = F.interpolate(left_depth[None, None, :, :], (GrdImg_H, GrdImg_W))
            # left_depth = left_depth[0, 0]

            grd_left_imgs = torch.cat([grd_left_imgs, grd_img_left.unsqueeze(0)], dim=0)
            # grd_left_depths = torch.cat([grd_left_depths, left_depth.unsqueeze(0)], dim=0)

        sat_rot = sat_map.rotate(-heading / np.pi * 180)
        sat_align_cam = sat_rot.transform(sat_rot.size, Image.AFFINE,
                                          (1, 0, utils.CameraGPS_shift_left[0] / self.meter_per_pixel,
                                           0, 1, utils.CameraGPS_shift_left[1] / self.meter_per_pixel),
                                          resample=Image.BILINEAR)
        # the homography is defined on: from target pixel to source pixel
        # now east direction is the real vehicle heading direction

        # randomly generate shift
        # gt_shift_x = np.random.uniform(-1, 1)  # --> right as positive, parallel to the heading direction
        # gt_shift_y = np.random.uniform(-1, 1)  # --> up as positive, vertical to the heading direction
        gt_shift_x = -float(gt_shift_x)  # --> right as positive, parallel to the heading direction
        gt_shift_y = -float(gt_shift_y)  # --> up as positive, vertical to the heading direction

        sat_rand_shift = \
            sat_align_cam.transform(
                sat_align_cam.size, Image.AFFINE,
                (1, 0, gt_shift_x * self.shift_range_pixels_lon,
                 0, 1, -gt_shift_y * self.shift_range_pixels_lat),
                resample=Image.BILINEAR)

        # randomly generate roation
        # theta = np.random.uniform(-1, 1)
        theta = float(theta)
        sat_rand_shift_rand_rot = \
            sat_rand_shift.rotate(theta * self.rotation_range)

        sat_map = TF.center_crop(sat_rand_shift_rand_rot, utils.SatMap_process_sidelength)
        # sat_map = np.array(sat_map, dtype=np.float32)

        # transform
        if self.satmap_transform is not None:
            sat_map = self.satmap_transform(sat_map)

        # gt_corr_x, gt_corr_y = self.generate_correlation_GTXY(gt_shift_x, gt_shift_y, theta)

        return sat_map, left_camera_k, grd_left_imgs[0], \
               torch.tensor(-gt_shift_x, dtype=torch.float32).reshape(1), \
               torch.tensor(-gt_shift_y, dtype=torch.float32).reshape(1), \
               torch.tensor(theta, dtype=torch.float32).reshape(1), \
               file_name
    
    # def generate_correlation_GTXY(self, gt_shift_x, gt_shift_y, gt_heading):
        
    #     cos = np.cos(gt_heading * self.rotation_range / 180 * np.pi)
    #     sin = np.sin(gt_heading * self.rotation_range / 180 * np.pi)
        
    #     gt_corr_x = - gt_shift_x * cos + gt_shift_y * sin
    #     gt_corr_y = gt_shift_x * sin + gt_shift_y * cos
        
    #     return gt_corr_x, gt_corr_y
        


def load_train_data(batch_size, shift_range_lat=20, shift_range_lon=20, rotation_range=10):
    SatMap_process_sidelength = utils.get_process_satmap_sidelength()

    satmap_transform = transforms.Compose([
        transforms.Resize(size=[SatMap_process_sidelength, SatMap_process_sidelength]),
        transforms.ToTensor(),
    ])

    Grd_h = GrdImg_H
    Grd_w = GrdImg_W

    grdimage_transform = transforms.Compose([
        transforms.Resize(size=[Grd_h, Grd_w]),
        transforms.ToTensor(),
    ])

    train_set = SatGrdDataset(root=root_dir, file=train_file,
                              transform=(satmap_transform, grdimage_transform),
                              shift_range_lat=shift_range_lat,
                              shift_range_lon=shift_range_lon,
                              rotation_range=rotation_range)

    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True,
                              num_workers=num_thread_workers, drop_last=False)
    return train_loader


def load_test1_data(batch_size, shift_range_lat=20, shift_range_lon=20, rotation_range=10):
    SatMap_process_sidelength = utils.get_process_satmap_sidelength()

    satmap_transform = transforms.Compose([
        transforms.Resize(size=[SatMap_process_sidelength, SatMap_process_sidelength]),
        transforms.ToTensor(),
    ])

    Grd_h = GrdImg_H
    Grd_w = GrdImg_W

    grdimage_transform = transforms.Compose([
        transforms.Resize(size=[Grd_h, Grd_w]),
        transforms.ToTensor(),
    ])

    # # Plz keep the following two lines!!! These are for fair test comparison.
    # np.random.seed(2022)
    # torch.manual_seed(2022)

    test1_set = SatGrdDatasetTest(root=root_dir, file=test1_file,
                            transform=(satmap_transform, grdimage_transform),
                            shift_range_lat=shift_range_lat,
                            shift_range_lon=shift_range_lon,
                            rotation_range=rotation_range)

    test1_loader = DataLoader(test1_set, batch_size=batch_size, shuffle=False, pin_memory=True,
                            num_workers=num_thread_workers, drop_last=False)
    return test1_loader


def load_test2_data(batch_size, shift_range_lat=20, shift_range_lon=20, rotation_range=10):
    SatMap_process_sidelength = utils.get_process_satmap_sidelength()

    satmap_transform = transforms.Compose([
        transforms.Resize(size=[SatMap_process_sidelength, SatMap_process_sidelength]),
        transforms.ToTensor(),
    ])

    Grd_h = GrdImg_H
    Grd_w = GrdImg_W

    grdimage_transform = transforms.Compose([
        transforms.Resize(size=[Grd_h, Grd_w]),
        transforms.ToTensor(),
    ])

    # # Plz keep the following two lines!!! These are for fair test comparison.
    # np.random.seed(2022)
    # torch.manual_seed(2022)

    test2_set = SatGrdDatasetTest(root=root_dir, file=test2_file,
                              transform=(satmap_transform, grdimage_transform),
                              shift_range_lat=shift_range_lat,
                              shift_range_lon=shift_range_lon,
                              rotation_range=rotation_range)

    test2_loader = DataLoader(test2_set, batch_size=batch_size, shuffle=False, pin_memory=True,
                              num_workers=num_thread_workers, drop_last=False)
    return test2_loader

================
File: dataLoader/utils.py
================
import numpy as np
import torch

CameraGPS_shift = [1.08, 0.26]
Satmap_zoom = 18
Camera_height = 1.65 #meter
Camera_distance = 0.54 #meter

SatMap_original_sidelength = 512 # 0.2 m per pixel
SatMap_process_sidelength = 512 # 0.2 m per pixel
Default_lat = 49.015

CameraGPS_shift_left = [1.08, 0.26]
CameraGPS_shift_right = [1.08, 0.8]  # 0.26 + 0.54

EPS = 1e-7

def get_satmap_zoom():
    return Satmap_zoom

def get_camera_height():
    return Camera_height

def get_camera_distance():
    return Camera_distance

def get_original_satmap_sidelength():
    return SatMap_original_sidelength

def get_process_satmap_sidelength():
    return SatMap_process_sidelength

# x: east shift in meter, y: south shift in meter
# return lat and lon after shift
# Curvature formulas from https://en.wikipedia.org/wiki/Earth_radius#Meridional
def meter2latlon(lat, lon, x, y):
    r = 6378137 # equatorial radius
    flatten = 1/298257 # flattening
    E2 = flatten * (2- flatten)
    m = r * np.pi/180  
    coslat = np.cos(lat * np.pi/180)
    w2 = 1/(1-E2 *(1-coslat*coslat))
    w = np.sqrt(w2)
    kx = m * w * coslat
    ky = m * w * w2 * (1-E2)
    lon += x / kx 
    lat -= y / ky
    
    return lat, lon   

def gps2meters(lat_s, lon_s, lat_d, lon_d ):
    r = 6378137 # equatorial radius
    flatten = 1/298257 # flattening
    E2 = flatten * (2- flatten)
    m = r * np.pi/180  
    lat = (lat_s+lat_d)/2
    coslat = np.cos(lat * np.pi/180)
    w2 = 1/(1-E2 *(1-coslat*coslat))
    w = np.sqrt(w2)
    kx = m * w * coslat
    ky = m * w * w2 * (1-E2)
    x = (lon_d-lon_s)*kx
    y = (lat_s-lat_d)*ky # y: from top to bottom
    
    return [x,y]


def gps2utm(lat, lon, lat0=49.015):
    # from paper "Vision meets Robotics: The KITTI Dataset"

    r = 6378137.
    s = np.cos(lat0 * np.pi / 180)

    x = s * r * np.pi * lon / 180
    y = s * r * np.log(np.tan(np.pi * (90 + lat) / 360))

    return x, y

def gps2utm_torch(lat, lon, lat0=torch.tensor(49.015)):
    # from paper "Vision meets Robotics: The KITTI Dataset"

    r = 6378137.
    s = torch.cos(lat0 * np.pi / 180)

    x = s * r * np.pi * lon / 180
    y = s * r * torch.log(torch.tan(np.pi * (90 + lat) / 360))

    return x, y


def gps2meters_torch(lat_s, lon_s, lat_d=torch.tensor([49.015]), lon_d=torch.tensor([8.43])):
    # inputs: torch array: [n]
    r = 6378137 # equatorial radius
    flatten = 1/298257 # flattening
    E2 = flatten * (2- flatten)
    m = r * np.pi/180  
    lat = lat_d[0]
    coslat = np.cos(lat * np.pi/180)
    w2 = 1/(1-E2 *(1-coslat*coslat))
    w = np.sqrt(w2)
    kx = m * w * coslat
    ky = m * w * w2 * (1-E2)
    
    x = (lon_d-lon_s)*kx
    y = (lat_s-lat_d)*ky # y: from top to bottom
    
    return x,y


def gps2shiftmeters(latlon ):
    # torch array: [B,S,2]

    r = 6378137 # equatoristereoal radius
    flatten = 1/298257 # flattening
    E2 = flatten * (2- flatten)
    m = r * np.pi/180  
    lat = latlon[0,0,0]
    coslat = torch.cos(lat * np.pi/180)
    w2 = 1/(1-E2 *(1-coslat*coslat))
    w = torch.sqrt(w2)
    kx = m * w * coslat
    ky = m * w * w2 * (1-E2)

    shift_x = (latlon[:,:1,1]-latlon[:,:,1])*kx #B,S east
    shift_y = (latlon[:,:,0]-latlon[:,:1,0])*ky #B,S south
    shift = torch.cat([shift_x.unsqueeze(-1),shift_y.unsqueeze(-1)],dim=-1) #[B,S,2] #shift from 0
    
    # shift from privious
    S = latlon.size()[1]
    shift = shift[:,1:,:]-shift[:,:(S-1),:]
    
    return shift


def gps2distance(lat_s, lon_s, lat_d, lon_d ):
    x,y = gps2meters_torch(lat_s, lon_s, lat_d, lon_d )
    dis = torch.sqrt(torch.pow(x, 2)+torch.pow(y,2))
    return dis


def get_meter_per_pixel(lat=Default_lat, zoom=Satmap_zoom, scale=SatMap_process_sidelength/SatMap_original_sidelength):
    meter_per_pixel = 156543.03392 * np.cos(lat * np.pi/180.) / (2**zoom)	
    meter_per_pixel /= 2 # because use scale 2 to get satmap 
    meter_per_pixel /= scale
    return meter_per_pixel


def gps2shiftscale(latlon):
    # torch array: [B,S,2]
    
    shift = gps2shiftmeters(latlon)
    
    # turn meter to -1~1
    meter_per_pixel = get_meter_per_pixel(scale=1)
    win_range = meter_per_pixel*SatMap_original_sidelength
    shift /= win_range//2
    
    return shift

def get_camera_max_meter_shift():
    return np.linalg.norm(CameraGPS_shift)

def get_camera_gps_shift(heading):
    shift_x = CameraGPS_shift[0] * np.cos(heading%(2*np.pi)) + CameraGPS_shift[1] * np.sin(heading%(2*np.pi))
    shift_y = CameraGPS_shift[1] * np.cos(heading%(2*np.pi)) - CameraGPS_shift[0] * np.sin(heading%(2*np.pi))
    return shift_x, shift_y


def get_camera_gps_shift_left(heading):
    shift_x = CameraGPS_shift_left[0] * np.cos(heading%(2*np.pi)) + CameraGPS_shift_left[1] * np.sin(heading%(2*np.pi))
    shift_y = CameraGPS_shift_left[0] * np.sin(heading%(2*np.pi)) - CameraGPS_shift_left[1] * np.cos(heading%(2*np.pi))
    return shift_x, shift_y


def get_camera_gps_shift_right(heading):
    shift_x = CameraGPS_shift_right[0] * np.cos(heading%(2*np.pi)) + CameraGPS_shift_right[1] * np.sin(heading%(2*np.pi))
    shift_y = CameraGPS_shift_right[0] * np.sin(heading%(2*np.pi)) - CameraGPS_shift_right[1] * np.cos(heading%(2*np.pi))
    return shift_x, shift_y


def get_height_config():
    start = 0 #-15 -7 0
    end = 0 
    count = 1 #16 8 1
    return start, end, count

================
File: dataLoader/Ford_dataset.py
================
import numpy as np
import os
from PIL import Image
from torch.utils.data import Dataset

import torch
import pandas as pd
import utils
import torchvision.transforms.functional as TF
from torchvision import transforms
from cfgnode import CfgNode
import yaml

# Ford_root = '/media/yujiao/6TB/dataset/Ford/'
Ford_root = '../../dataset/Ford/'
satmap_dir = 'SatelliteMaps_18'
data_file = 'grd_sat_quaternion_latlon.txt'
data_file_test = 'grd_sat_quaternion_latlon_test.txt'
pose_file_dir = 'Calibration-V2/V2/'
FL_ex = 'cameraFrontLeft_body.yaml'
FL_in = 'cameraFrontLeftIntrinsics.yaml'

train_logs = [
              '2017-10-26/V2/Log1',
              '2017-10-26/V2/Log2',
              '2017-08-04/V2/Log3',
              '2017-10-26/V2/Log4',
              '2017-08-04/V2/Log5',
              '2017-08-04/V2/Log6',
              ]

train_logs_img_inds = [
    list(range(4500, 8500)),
    list(range(3150)) + list(range(6000, 9200)) + list(range(11000, 15000)),
    list(range(1500)),
    list(range(7466)),
    list(range(3200)) + list(range(5300, 9900)) + list(range(10500, 11130)),
    list(range(1000, 3500)) + list(range(4500, 5000)) + list(range(7000, 7857)),
                       ]

test_logs = [
             '2017-08-04/V2/Log1',
             '2017-08-04/V2/Log2',
             '2017-08-04/V2/Log3',
             '2017-08-04/V2/Log4',
             '2017-10-26/V2/Log5',
             '2017-10-26/V2/Log6',
]
test_logs_img_inds = [
    list(range(100, 200)) + list(range(5000, 5500)) + list(range(7000, 8500)),
    list(range(2500, 3000)) + list(range(8500, 10500)) + list(range(12500, 13727)),
    list(range(3500, 5000)),
    list(range(1500, 2500)) + list(range(4000, 4500)) + list(range(7000, 9011)),
    list(range(3500)),
    list(range(2000, 2500)) + list(range(3500, 4000)),
]

# For the Ford dataset coordinates:
# x--> North, y --> east, z --> down
# North direction as 0-degree, clockwise as positive.

def qvec2rotmat(qvec):
    return np.array([
        [1 - 2 * qvec[2]**2 - 2 * qvec[3]**2,
         2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],
         2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2]],
        [2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],
         1 - 2 * qvec[1]**2 - 2 * qvec[3]**2,
         2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1]],
        [2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2],
         2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1],
         1 - 2 * qvec[1]**2 - 2 * qvec[2]**2]])

def qvec2angle(q0, q1, q2, q3):
    roll  = np.arctan2(2.0 * (q3 * q2 + q0 * q1) , 1.0 - 2.0 * (q1 * q1 + q2 * q2)) / np.pi * 180
    pitch = np.arcsin(2.0 * (q2 * q0 - q3 * q1)) / np.pi * 180
    yaw   = np.arctan2(2.0 * (q3 * q0 + q1 * q2) , - 1.0 + 2.0 * (q0 * q0 + q1 * q1)) / np.pi * 180
    return roll, pitch, yaw


class SatGrdDatasetFord(Dataset):
    def __init__(self, root=Ford_root, logs=train_logs, logs_img_inds=train_logs_img_inds,
                 shift_range_lat=20, shift_range_lon=20, rotation_range=10, whole=False):
        self.root = root

        self.shift_range_meters_lat = shift_range_lat  # in terms of meters
        self.shift_range_meters_lon = shift_range_lon  # in terms of meters
        self.meters_per_pixel = 0.22
        self.shift_range_pixels_lat = shift_range_lat / self.meters_per_pixel  # in terms of pixels
        self.shift_range_pixels_lon = shift_range_lon / self.meters_per_pixel  # in terms of pixels

        self.rotation_range = rotation_range # in terms of degree

        self.satmap_dir = satmap_dir

        file_name = []
        for idx in range(len(logs)):
            log = logs[idx]
            img_inds = logs_img_inds[idx]
            FL_dir = os.path.join(root, log, log.replace('/', '-') + '-FL')

            with open(os.path.join(root, log, data_file), 'r') as f:
                lines = f.readlines()
                if whole == 0:
                    lines = [lines[ind] for ind in img_inds]
                # lines = f.readlines()[img_inds]
                for line in lines:
                    grd_name, q0, q1, q2, q3, g_lat, g_lon, s_lat, s_lon = line.strip().split(' ')
                    grd_file_FL = os.path.join(FL_dir, grd_name.replace('.txt', '.png'))
                    sat_file = os.path.join(root, log, satmap_dir, s_lat + '_' + s_lon + '.png')
                    file_name.append([grd_file_FL, float(q0), float(q1), float(q2), float(q3), float(g_lat), float(g_lon),
                                  float(s_lat), float(s_lon), sat_file])

        self.file_name = file_name

        self.lat0 = 42.29424422604817  # 08-04-Log0-img0

        with open(os.path.join(root, pose_file_dir, FL_ex), "r") as f:
            cfg_dict = yaml.load(f, Loader=yaml.FullLoader)
            cfg_FL_ex = CfgNode(cfg_dict)

        qx = cfg_FL_ex.transform.rotation.x
        qy = cfg_FL_ex.transform.rotation.y
        qz = cfg_FL_ex.transform.rotation.z
        qw = cfg_FL_ex.transform.rotation.w

        FLx, FLy, FLz = cfg_FL_ex.transform.translation.x, cfg_FL_ex.transform.translation.y, cfg_FL_ex.transform.translation.z
        self.T_FL = np.array([FLx, FLy, FLz]).reshape(3).astype(np.float32)
        self.R_FL = qvec2rotmat([qw, qx, qy, qz]).astype(np.float32)
        # from camera coordinates to body coordinates
        # Xb = R_FL @ Xc + T_FL

        with open(os.path.join(root, pose_file_dir, FL_in), "r") as f:
            cfg_dict = yaml.load(f, Loader=yaml.FullLoader)
            cfg_FL_in = CfgNode(cfg_dict)

        self.K_FL = np.array(cfg_FL_in.K, dtype=np.float32).reshape([3, 3])
        self.H_FL = 860
        self.W_FL = 1656

        self.H = 256
        self.W = 1024

        self.K_FL[0] = self.K_FL[0] / self.W_FL * self.W
        self.K_FL[1] = self.K_FL[1] / self.H_FL * self.H

        self.sidelength = 512
        self.satmap_sidelength_meters = self.sidelength * self.meters_per_pixel
        self.satmap_transform = transforms.Compose([
            transforms.ToTensor(),
        ])
        self.grdimage_transform = transforms.Compose([
            transforms.Resize(size=[self.H, self.W]),
            transforms.ToTensor(),
        ])

    def __len__(self):
        return len(self.file_name)

    def get_file_list(self):
        return self.file_name

    def __getitem__(self, idx):
        # read cemera k matrix from camera calibration files, day_dir is first 10 chat of file name

        grd_name, q0, q1, q2, q3, g_lat, g_lon, s_lat, s_lon, sat_name = self.file_name[idx]

        grd_img = Image.open(grd_name).convert('RGB')
        grd_img = self.grdimage_transform(grd_img)

        # Xc = np.array([0, 0, 0]).reshape(3)
        # Rw = qvec2rotmat([float(q0), float(q1), float(q2), float(q3)])
        # # body frame to world frame: Xw = Rw @ Xb + Tw  (Tw are all zeros)
        # Xw = Rw @ (self.R_FL @ Xc + self.T_FL)  # North (up) --> X, East (right) --> Y
        # # camera location represented in world coordinates,
        # # world coordinates is centered at the body coordinates, but with X pointing north, Y pointing east, Z pointing down

        g_x, g_y = utils.gps2utm(float(g_lat), float(g_lon), float(s_lat))
        s_x, s_y = utils.gps2utm(float(s_lat), float(s_lon), float(s_lat))
        # x, y here are the x, y under gps/utm coordinates, x pointing right and y pointing up

        b_delta_u = (g_x - s_x) / self.meters_per_pixel # relative u shift of body frame with respect to satellite image center
        b_delta_v = - (g_y - s_y) / self.meters_per_pixel # relative v shift of body frame with respect to satellite image center

        sat_map = Image.open(sat_name).convert('RGB')
        sat_align_body_loc = sat_map.transform(sat_map.size, Image.AFFINE,
                                          (1, 0, b_delta_u,
                                           0, 1, b_delta_v),
                                          resample=Image.BILINEAR)
        # Homography is defined on from target pixel to source pixel
        roll, pitch, yaw = qvec2angle(q0, q1, q2, q3)  # in terms of degree
        sat_align_body_loc_orien = sat_align_body_loc.rotate(yaw)

        # random shift
        gt_shift_u = np.random.uniform(-1, 1)  # --> right (east) as positive, vertical to the heading, lateral
        gt_shift_v = np.random.uniform(-1, 1)  # --> down (south) as positive, parallel to the heading, longitudinal

        sat_rand_shift = \
            sat_align_body_loc_orien.transform(
                sat_align_body_loc_orien.size, Image.AFFINE,
                (1, 0, gt_shift_u * self.shift_range_pixels_lat,
                 0, 1, gt_shift_v * self.shift_range_pixels_lon),
                resample=Image.BILINEAR)

        theta = np.random.uniform(-1, 1)
        sat_rand_shift_rot = sat_rand_shift.rotate(theta * self.rotation_range)

        sat_img = TF.center_crop(sat_rand_shift_rot, self.sidelength)
        sat_img = self.satmap_transform(sat_img)

        return sat_img, grd_img, \
               torch.tensor(gt_shift_u, dtype=torch.float32), \
               torch.tensor(gt_shift_v, dtype=torch.float32), \
               torch.tensor(theta, dtype=torch.float32), self.R_FL, self.T_FL, grd_name


class SatGrdDatasetFordTest(Dataset):
    def __init__(self, root=Ford_root, logs=test_logs, logs_img_inds=test_logs_img_inds,
                 shift_range_lat=20, shift_range_lon=20, rotation_range=10, whole=False):
        self.root = root

        self.shift_range_meters_lat = shift_range_lat  # in terms of meters
        self.shift_range_meters_lon = shift_range_lon  # in terms of meters
        self.meters_per_pixel = 0.22
        self.shift_range_pixels_lat = shift_range_lat / self.meters_per_pixel  # in terms of pixels
        self.shift_range_pixels_lon = shift_range_lon / self.meters_per_pixel  # in terms of pixels

        self.rotation_range = rotation_range  # in terms of degree

        self.satmap_dir = satmap_dir

        file_name = []
        for idx in range(len(logs)):
            log = logs[idx]
            img_inds = logs_img_inds[idx]
            FL_dir = os.path.join(root, log, log.replace('/', '-') + '-FL')

            with open(os.path.join(root, log, data_file_test), 'r') as f:
                lines = f.readlines()
                # if whole == 0:
                #     lines = [lines[ind] for ind in img_inds]
                # lines = f.readlines()[img_inds]
                for line in lines:
                    grd_name, q0, q1, q2, q3, g_lat, g_lon, s_lat, s_lon, gt_shift_u, gt_shift_v, theta = line.strip().split(' ')
                    grd_file_FL = os.path.join(FL_dir, grd_name.replace('.txt', '.png'))
                    sat_file = os.path.join(root, log, satmap_dir, s_lat + '_' + s_lon + '.png')
                    file_name.append(
                        [grd_file_FL, float(q0), float(q1), float(q2), float(q3), float(g_lat), float(g_lon),
                         float(s_lat), float(s_lon), sat_file, float(gt_shift_u), float(gt_shift_v),
                         float(theta)])

        self.file_name = file_name

        self.lat0 = 42.29424422604817  # 08-04-Log0-img0

        with open(os.path.join(root, pose_file_dir, FL_ex), "r") as f:
            cfg_dict = yaml.load(f, Loader=yaml.FullLoader)
            cfg_FL_ex = CfgNode(cfg_dict)

        qx = cfg_FL_ex.transform.rotation.x
        qy = cfg_FL_ex.transform.rotation.y
        qz = cfg_FL_ex.transform.rotation.z
        qw = cfg_FL_ex.transform.rotation.w

        FLx, FLy, FLz = cfg_FL_ex.transform.translation.x, cfg_FL_ex.transform.translation.y, cfg_FL_ex.transform.translation.z
        self.T_FL = np.array([FLx, FLy, FLz]).reshape(3).astype(np.float32)
        self.R_FL = qvec2rotmat([qw, qx, qy, qz]).astype(np.float32)
        # from camera coordinates to body coordinates
        # Xb = R_FL @ Xc + T_FL

        with open(os.path.join(root, pose_file_dir, FL_in), "r") as f:
            cfg_dict = yaml.load(f, Loader=yaml.FullLoader)
            cfg_FL_in = CfgNode(cfg_dict)

        self.K_FL = np.array(cfg_FL_in.K, dtype=np.float32).reshape([3, 3])
        self.H_FL = 860
        self.W_FL = 1656

        self.H = 256
        self.W = 1024

        self.K_FL[0] = self.K_FL[0] / self.W_FL * self.W
        self.K_FL[1] = self.K_FL[1] / self.H_FL * self.H

        self.sidelength = 512
        self.satmap_sidelength_meters = self.sidelength * self.meters_per_pixel
        self.satmap_transform = transforms.Compose([
            transforms.ToTensor(),
        ])
        self.grdimage_transform = transforms.Compose([
            transforms.Resize(size=[self.H, self.W]),
            transforms.ToTensor(),
        ])

    def __len__(self):
        return len(self.file_name)

    def get_file_list(self):
        return self.file_name

    def __getitem__(self, idx):
        # read cemera k matrix from camera calibration files, day_dir is first 10 chat of file name

        grd_name, q0, q1, q2, q3, g_lat, g_lon, s_lat, s_lon, sat_name, gt_shift_u, gt_shift_v, theta = self.file_name[idx]

        grd_img = Image.open(grd_name).convert('RGB')
        grd_img = self.grdimage_transform(grd_img)

        # Xc = np.array([0, 0, 0]).reshape(3)
        # Rw = qvec2rotmat([float(q0), float(q1), float(q2), float(q3)])
        # # body frame to world frame: Xw = Rw @ Xb + Tw  (Tw are all zeros)
        # Xw = Rw @ (self.R_FL @ Xc + self.T_FL)  # North (up) --> X, East (right) --> Y
        # # camera location represented in world coordinates,
        # # world coordinates is centered at the body coordinates, but with X pointing north, Y pointing east, Z pointing down

        g_x, g_y = utils.gps2utm(float(g_lat), float(g_lon), float(s_lat))
        s_x, s_y = utils.gps2utm(float(s_lat), float(s_lon), float(s_lat))
        # x, y here are the x, y under gps/utm coordinates, x pointing right and y pointing up

        b_delta_u = (
                                g_x - s_x) / self.meters_per_pixel  # relative u shift of body frame with respect to satellite image center
        b_delta_v = - (
                    g_y - s_y) / self.meters_per_pixel  # relative v shift of body frame with respect to satellite image center

        sat_map = Image.open(sat_name).convert('RGB')
        sat_align_body_loc = sat_map.transform(sat_map.size, Image.AFFINE,
                                               (1, 0, b_delta_u,
                                                0, 1, b_delta_v),
                                               resample=Image.BILINEAR)
        # Homography is defined on from target pixel to source pixel
        roll, pitch, yaw = qvec2angle(q0, q1, q2, q3)  # in terms of degree
        sat_align_body_loc_orien = sat_align_body_loc.rotate(yaw)

        # random shift
        # gt_shift_u = np.random.uniform(-1, 1)  # --> right (east) as positive, vertical to the heading, lateral
        # gt_shift_v = np.random.uniform(-1, 1)  # --> down (south) as positive, parallel to the heading, longitudinal

        sat_rand_shift = \
            sat_align_body_loc_orien.transform(
                sat_align_body_loc_orien.size, Image.AFFINE,
                (1, 0, gt_shift_u * self.shift_range_pixels_lat,
                 0, 1, gt_shift_v * self.shift_range_pixels_lon),
                resample=Image.BILINEAR)

        # theta = np.random.uniform(-1, 1)
        sat_rand_shift_rot = sat_rand_shift.rotate(theta * self.rotation_range)

        sat_img = TF.center_crop(sat_rand_shift_rot, self.sidelength)
        sat_img = self.satmap_transform(sat_img)

        return sat_img, grd_img, \
               torch.tensor(gt_shift_u, dtype=torch.float32), \
               torch.tensor(gt_shift_v, dtype=torch.float32), \
               torch.tensor(theta, dtype=torch.float32), self.R_FL, self.T_FL, grd_name

================
File: dataLoader/Oxford_dataset.py
================
import random

import numpy as np
import os
from PIL import Image
from torch.utils.data import Dataset

import torch
import pandas as pd
import utils
import torchvision.transforms.functional as TF
from torchvision import transforms
import torch.nn.functional as F

from torch.utils.data import DataLoader
from torchvision import transforms
import cv2
import math

# root_dir = '/media/yujiao/6TB/FeiWu/Oxford_dataset/Oxford_ground/' # '../../data/Kitti' # '../Data' #'..\\Data' #
root_dir = '/home/users/u6293587/Oxford_dataset/Oxford_ground/'
# root_dir = '/home/yujiao/dataset/Oxford_dataset/Oxford_ground/'

GrdImg_H = 154  # 256 # original: 375 #224, 256
GrdImg_W = 231  # 1024 # original:1242 #1248, 1024
GrdOriImg_H = 800
GrdOriImg_W = 1200
num_thread_workers = 2

# train_file = './dataLoader/train_files.txt'
train_file = './dataLoader/oxford/training.txt'
test1_file = './dataLoader/oxford/test1_j.txt'
test2_file = './dataLoader/oxford/test2_j.txt'
test3_file = './dataLoader/oxford/test3_j.txt'
val_file = "./oxford/validation.txt"


class SatGrdDataset(Dataset):
    """
    output:
    sat img
    left_camera_k
    grd img
    gt_shift_x       pixel
    gt_shift_y       pixel
    theta            (-1,1)
    0.1235 meter_per_pixel
    sat_map shape:  torch.Size([1, 3, 512, 512])
    grd image shape:  torch.Size([1, 3, 154, 231])
    """
    def __init__(self, root, file, transform=None, rotation_range=0, ori_sat_res=800):

        self.root = root
        self.ori_sat_res = ori_sat_res

        if transform != None:
            self.satmap_transform = transform[0]
            self.grdimage_transform = transform[1]
        self.rotation_range=rotation_range

        #broken ground image idx
        self.yaws = np.load("./dataLoader/oxford/train_yaw.npy")

        broken = [937, 9050, 11811, 12388, 16584]
        self.train_yaw = []
        for i in range(len(self.yaws)):
            if i not in broken:
                self.train_yaw.append(self.yaws[i])  #loading yaws

        primary = np.array([[619400., 5736195.],
                            [619400., 5734600.],
                            [620795., 5736195.],
                            [620795., 5734600.],
                            [620100., 5735400.]])
        secondary = np.array([[900., 900.],  # tl
                              [492., 18168.],  # bl
                              [15966., 1260.],  # tr
                              [15553., 18528.],  # br
                              [8255., 9688.]])  # c
        n = primary.shape[0]
        pad = lambda x: np.hstack([x, np.ones((x.shape[0], 1))])
        unpad = lambda x: x[:, :-1]
        X = pad(primary)
        Y = pad(secondary)
        A, res, rank, s = np.linalg.lstsq(X, Y)

        self.transform = lambda x: unpad(np.dot(pad(x), A))
        self.sat_map = cv2.imread(
            "./dataLoader/oxford/satellite_map_new.png")  # read whole over-view map

        print(self.sat_map.shape)

        with open(file, 'r') as f:
            self.file_name = f.readlines()    # read training file

        trainlist = []
        with open("./dataLoader/oxford/"+'training.txt', 'r') as filehandle:
            filecontents = filehandle.readlines()
            for line in filecontents:
                content = line[:-1]
                trainlist.append(content.split(" "))
        self.trainList = trainlist
        self.trainNum = len(trainlist)
        trainarray = np.array(trainlist)
        self.trainUTM = np.transpose(trainarray[:, 2:].astype(np.float64))

    def __len__(self):
        return len(self.file_name)

    def get_file_list(self):
        return self.file_name

    def __getitem__(self, idx):
        img_idx=idx
        # =================== read camera intrinsice for left and right cameras ====================

        fx = float(964.828979) * GrdImg_W / GrdOriImg_W
        cx = float(643.788025) * GrdImg_W / GrdOriImg_W
        fy = float(964.828979) * GrdImg_H / GrdOriImg_H
        cy = float(484.407990) * GrdImg_H / GrdOriImg_H

        left_camera_k = [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]
        left_camera_k = torch.from_numpy(np.asarray(left_camera_k, dtype=np.float32))
        # if not self.stereo:
        # =================== read ground img ===================================
        line = self.file_name[idx]
        grdimg=line.split(" ")[0]
        grdimg=root_dir+grdimg

        left_img_name = os.path.join(grdimg)
        grd_img = cv2.imread(left_img_name)

        grd_img = Image.fromarray(cv2.cvtColor(grd_img, cv2.COLOR_BGR2RGB))
        grd_img= self.grdimage_transform(grd_img)

        # =================== position in satellite map ===================================
        image_coord = np.round(self.transform(np.array([[self.trainUTM[0, img_idx], self.trainUTM[1, img_idx]]]))[0])

        # =================== set random offset ===================================
        alpha = 2 * math.pi * random.random()
        r = 200 * np.sqrt(2) * random.random()
        row_offset = int(r * math.cos(alpha))
        col_offset = int(r * math.sin(alpha))

        sat_coord_row = int(image_coord[1] + row_offset)  # sat center location
        sat_coord_col = int(image_coord[0] + col_offset)

        # print(sat_coord_row, sat_coord_col)
        # =================== crop satellite map ===================================
        img = self.sat_map[sat_coord_row - int(self.ori_sat_res//2) - int(self.ori_sat_res//4):sat_coord_row + int(self.ori_sat_res//2) + int(self.ori_sat_res//4),
              sat_coord_col - int(self.ori_sat_res//2) - int(self.ori_sat_res//4):sat_coord_col + int(self.ori_sat_res//2) + int(self.ori_sat_res//4),
              :]  # load at each side extra 200 pixels to avoid blank after rotation

        #=================== set rotation random ===================================
        theta = np.random.uniform(-1, 1)
        # rdm= theta * self.rotation_range/ np.pi * 180        # radian  ground truth
        rdm = theta * self.rotation_range                      #degree
        #======================================================================

        # rotate_angle = self.train_yaw[img_idx] / np.pi * 180-90 +rdm*180/np.pi   # degree
        rotate_angle = self.train_yaw[img_idx] / np.pi * 180 - 90 + rdm            # degree
        H, W = img.shape[:2]
        assert H == W == self.ori_sat_res // 2 * 3
        rot_matrix = cv2.getRotationMatrix2D((int(H // 2), int(W // 2)), rotate_angle, 1)  # rotate satellite image
        # NOT SURE THE ORDER OF H * W IN ABOVE AND BELOW IS CORRECT OR NOT. HERE IT DOES NOT MATTER, BECAUSE H==W.
        img = cv2.warpAffine(img, rot_matrix, (H, W))
        # rot_matrix = cv2.getRotationMatrix2D((600, 600), rotate_angle, 1)
        # img = cv2.warpAffine(img, rot_matrix, (1200, 1200))
        img = img[int(self.ori_sat_res//4):-int(self.ori_sat_res//4), int(self.ori_sat_res//4):-int(self.ori_sat_res//4), :]
        sat_img = cv2.resize(img, (512, 512), interpolation=cv2.INTER_AREA)# 0.1235
        # img[:, :, 0] -= 103.939  # Blue
        # img[:, :, 1] -= 116.779  # Green
        # img[:, :, 2] -= 123.6  # Red
        sat_img = Image.fromarray(cv2.cvtColor(sat_img, cv2.COLOR_BGR2RGB))
        sat_img = self.satmap_transform(sat_img)
        row_offset_resized = int(np.round((int(self.ori_sat_res//2) + row_offset) / self.ori_sat_res * 512 - 256))
        col_offset_resized = int(np.round((int(self.ori_sat_res//2) + col_offset) / self.ori_sat_res * 512 - 256))

        #=================== set ground truth ===================================
        x, y = np.meshgrid(np.linspace(-256 + col_offset_resized, 256 + col_offset_resized, 512),
                           np.linspace(-256 + row_offset_resized, 256 + row_offset_resized, 512))
        d = np.sqrt(x * x + y * y)
        sigma, mu = 4, 0.0
        img0 = np.exp(-((d - mu) ** 2 / (2.0 * sigma ** 2)))
        rot_matrix = cv2.getRotationMatrix2D((256, 256), rotate_angle, 1)
        img0 = cv2.warpAffine(img0, rot_matrix, (512, 512))
        a = np.where(img0 == img0.max())
        y = a[0]
        x = a[1]

        gt_shift_x = (x[0]-256)   # pixel  right positive parallel heading
        gt_shift_y = (y[0]-256)   # pixel  down positive vertical heading         0.1235 meter_per_pixel

        return sat_img, left_camera_k, grd_img, \
               torch.tensor(gt_shift_x, dtype=torch.float32).reshape(1), \
               torch.tensor(gt_shift_y, dtype=torch.float32).reshape(1), \
               torch.tensor(theta, dtype=torch.float32).reshape(1) \


class SatGrdDatasetVal(Dataset):

    def __init__(self, root, transform=None, rotation_range=0, ori_sat_res=800):

        self.root = root
        self.ori_sat_res = ori_sat_res

        if transform is not None:
            self.satmap_transform = transform[0]
            self.grdimage_transform = transform[1]
        self.rotation_range=rotation_range

        #broken ground image idx
        self.yaws = np.load("./dataLoader/oxford/val_yaw.npy") # for debug

        primary = np.array([[619400., 5736195.],
                            [619400., 5734600.],
                            [620795., 5736195.],
                            [620795., 5734600.],
                            [620100., 5735400.]])
        secondary = np.array([[900., 900.],  # tl
                              [492., 18168.],  # bl
                              [15966., 1260.],  # tr
                              [15553., 18528.],  # br
                              [8255., 9688.]])  # c
        n = primary.shape[0]
        pad = lambda x: np.hstack([x, np.ones((x.shape[0], 1))])
        unpad = lambda x: x[:, :-1]
        X = pad(primary)
        Y = pad(secondary)
        A, res, rank, s = np.linalg.lstsq(X, Y)

        self.transform = lambda x: unpad(np.dot(pad(x), A))
        self.sat_map = cv2.imread("./dataLoader/oxford/satellite_map_new.png")  # read whole over-view map
        vallist = []
        with open("./dataLoader/oxford/"+'validation.txt', 'r') as filehandle:
            filecontents = filehandle.readlines()
            for line in filecontents:
                content = line[:-1]
                vallist.append(content.split(" "))
        self.valList = vallist
        self.valNum = len(vallist)
        valarray = np.array(vallist)
        self.valUTM = np.transpose(valarray[:, 2:].astype(np.float64))

    def __len__(self):
        return self.valNum

    def get_file_list(self):
        return self.valList

    def __getitem__(self, idx):
        img_idx=idx
        # =================== read camera intrinsice for left and right cameras ====================

        fx = float(964.828979) * GrdImg_W / GrdOriImg_W
        cx = float(643.788025) * GrdImg_W / GrdOriImg_W
        fy = float(964.828979) * GrdImg_H / GrdOriImg_H
        cy = float(484.407990) * GrdImg_H / GrdOriImg_H

        left_camera_k = [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]
        left_camera_k = torch.from_numpy(np.asarray(left_camera_k, dtype=np.float32))

        # =================== read ground img ===================================
        line = self.valList[idx]
        grdimg=root_dir+line[0]

        left_img_name = os.path.join(grdimg)
        grd_img = cv2.imread(left_img_name)

        grd_img = Image.fromarray(cv2.cvtColor(grd_img, cv2.COLOR_BGR2RGB))
        grd_img= self.grdimage_transform(grd_img)

        # =================== position in satellite map ===================================
        image_coord = np.round(self.transform(np.array([[self.valUTM[0, img_idx], self.valUTM[1, img_idx]]]))[0])
        col_split = int((image_coord[0]) // (self.ori_sat_res//2))
        if np.round(image_coord[0] - (self.ori_sat_res//2) * col_split) < (self.ori_sat_res//4):
            col_split -= 1
        col_pixel = int(np.round(image_coord[0] - (self.ori_sat_res//2) * col_split))

        row_split = int((image_coord[1]) // (self.ori_sat_res//2))
        if np.round(image_coord[1] - (self.ori_sat_res//2) * row_split) < (self.ori_sat_res//4):
            row_split -= 1
        row_pixel = int(np.round(image_coord[1] - (self.ori_sat_res//2) * row_split))

        img = self.sat_map[row_split * int(self.ori_sat_res//2) - int(self.ori_sat_res//4):row_split * int(self.ori_sat_res//2) + self.ori_sat_res + int(self.ori_sat_res//4),
              col_split * int(self.ori_sat_res//2) - int(self.ori_sat_res//4):col_split * int(self.ori_sat_res//2) + self.ori_sat_res + int(self.ori_sat_res//4),
              :]  # read extra 200 pixels at each side to avoid blank after rotation

        # =================== set rotation random ===================================
        theta = np.random.uniform(-1, 1)
        # rdm= theta * self.rotation_range/ np.pi * 180        # radian  ground truth
        rdm = theta * self.rotation_range  # degree
        # ======================================================================
        rotate_angle = self.yaws[img_idx] / np.pi * 180-90 +rdm  # degree
        H, W = img.shape[:2]
        assert H == W == (self.ori_sat_res//2) * 3
        rot_matrix = cv2.getRotationMatrix2D((int(H//2), int(W//2)), rotate_angle, 1)  # rotate satellite image
        # NOT SURE THE ORDER OF H * W IN ABOVE AND BELOW IS CORRECT OR NOT. HERE IT DOES NOT MATTER, BECAUSE H==W.
        img = cv2.warpAffine(img, rot_matrix, (H, W))
        img = img[int(self.ori_sat_res//4):-int(self.ori_sat_res//4), int(self.ori_sat_res//4):-int(self.ori_sat_res//4), :]
        img = cv2.resize(img, (512, 512), interpolation=cv2.INTER_AREA)
        # img[:, :, 0] -= 103.939  # Blue
        # img[:, :, 1] -= 116.779  # Green
        # img[:, :, 2] -= 123.6  # Red
        sat_img=Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))
        sat_img=self.satmap_transform(sat_img)

        row_offset_resized = int(-(row_pixel / self.ori_sat_res * 512 - 256))
        col_offset_resized = int(-(col_pixel / self.ori_sat_res * 512 - 256))
    ################################################
        x, y = np.meshgrid(np.linspace(-256 + col_offset_resized, 256 + col_offset_resized, 512),
                           np.linspace(-256 + row_offset_resized, 256 + row_offset_resized, 512))
        d = np.sqrt(x * x + y * y)
        sigma, mu = 4, 0.0
        img0 = np.exp(-((d - mu) ** 2 / (2.0 * sigma ** 2)))
        rot_matrix = cv2.getRotationMatrix2D((256, 256), rotate_angle, 1)
        img0 = cv2.warpAffine(img0, rot_matrix, (512, 512))
        # print("********** pos: ",img0.shape,np.where(img0==img0.max()))
        a=np.where(img0==img0.max())
        y=a[0]
        x=a[1]
    ####################################################

        gt_shift_x=(x[0]-256)   # right positive parallel heading
        gt_shift_y=(y[0]-256)   #down positive vertical heading

        return sat_img, left_camera_k, grd_img, \
               torch.tensor(gt_shift_x, dtype=torch.float32).reshape(1), \
               torch.tensor(gt_shift_y, dtype=torch.float32).reshape(1), \
               torch.tensor(theta, dtype=torch.float32).reshape(1) \


class SatGrdDatasetTest(Dataset):

    def __init__(self, root, transform=None, rotation_range=0, test=0, ori_sat_res=800):
        self.root = root
        self.ori_sat_res = ori_sat_res
        if transform != None:
            self.satmap_transform = transform[0]
            self.grdimage_transform = transform[1]
        self.rotation_range = rotation_range

        # broken ground image idx

        # self.yaws = np.load("./dataLoader/oxford/train_yaw.npy")

        with open('./dataLoader/oxford/test_yaw.npy', 'rb') as f:
            self.val_yaw = np.load(f)
        if test == 2:
                self.val_yaw=self.val_yaw[1672+1:1672+1708+1]
        elif test == 3:
                self.val_yaw=self.val_yaw[1672+1708+1:1672+1708+1708+1]

        primary = np.array([[619400., 5736195.],
                            [619400., 5734600.],
                            [620795., 5736195.],
                            [620795., 5734600.],
                            [620100., 5735400.]])
        secondary = np.array([[900., 900.],  # tl
                              [492., 18168.],  # bl
                              [15966., 1260.],  # tr
                              [15553., 18528.],  # br
                              [8255., 9688.]])  # c
        n = primary.shape[0]
        pad = lambda x: np.hstack([x, np.ones((x.shape[0], 1))])
        unpad = lambda x: x[:, :-1]
        X = pad(primary)
        Y = pad(secondary)
        A, res, rank, s = np.linalg.lstsq(X, Y)

        self.transform = lambda x: unpad(np.dot(pad(x), A))
        # self.sat_map = cv2.imread(
        #     "./dataLoader/oxford/satellite_map_new.png")  # read whole over-view map

        self.sat_map = cv2.imread(
            "./dataLoader/oxford/satellite_map_new.png")  # for debug

        test_2015_08_14_14_54_57 = []
        with open('./dataLoader/oxford/test1_j.txt', 'r') as filehandle:
            filecontents = filehandle.readlines()
            for line in filecontents:
                content = line[:-1]
                test_2015_08_14_14_54_57.append(content.split(" "))
        test_2015_08_12_15_04_18 = []
        with open('./dataLoader/oxford/test2_j.txt', 'r') as filehandle:
            filecontents = filehandle.readlines()
            for line in filecontents:
                content = line[:-1]
                test_2015_08_12_15_04_18.append(content.split(" "))
        test_2015_02_10_11_58_05 = []
        with open('./dataLoader/oxford/test3_j.txt', 'r') as filehandle:
            filecontents = filehandle.readlines()
            for line in filecontents:
                content = line[:-1]
                test_2015_02_10_11_58_05.append(content.split(" "))

        if test==0:
            testlist = test_2015_08_14_14_54_57 + test_2015_08_12_15_04_18 + test_2015_02_10_11_58_05
        elif test==1:
            testlist = test_2015_08_14_14_54_57
        elif test ==2:
            testlist = test_2015_08_12_15_04_18
        else:
            testlist = test_2015_02_10_11_58_05
        self.valList = testlist
        self.valNum = len(testlist)
        valarray = np.array(testlist)
        self.valUTM = np.transpose(valarray[:,2:].astype(np.float64))
        print("len.........: ",self.valNum )

    def __len__(self):
        return self.valNum

    def get_file_list(self):
        return self.valList

    def __getitem__(self, idx):
        img_idx = idx
        # =================== read camera intrinsice for left and right cameras ====================
        fx = float(964.828979) * GrdImg_W / GrdOriImg_W
        cx = float(643.788025) * GrdImg_W / GrdOriImg_W
        fy = float(964.828979) * GrdImg_H / GrdOriImg_H
        cy = float(484.407990) * GrdImg_H / GrdOriImg_H

        left_camera_k = [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]
        left_camera_k = torch.from_numpy(np.asarray(left_camera_k, dtype=np.float32))
        # =================== read ground img ===================================
        line = self.valList[idx]
        grdimg = root_dir + line[0]

        left_img_name = os.path.join(grdimg)
        grd_img = cv2.imread(left_img_name)

        grd_img = Image.fromarray(cv2.cvtColor(grd_img, cv2.COLOR_BGR2RGB))
        grd_img = self.grdimage_transform(grd_img)

        # grd_img= grd_img.astype(np.float32)
        # grd_img[:, :, 0] -= 103.939  # Blue
        # grd_img[:, :, 1] -= 116.779  # Green
        # grd_img[:, :, 2] -= 123.6  # Red

        # =================== position in satellite map ===================================
        image_coord = np.round(self.transform(np.array([[self.valUTM[0, img_idx], self.valUTM[1, img_idx]]]))[0])
        col_split = int((image_coord[0]) // (self.ori_sat_res//2))
        if np.round(image_coord[0] - (self.ori_sat_res//2) * col_split) < (self.ori_sat_res//4):
            col_split -= 1
        col_pixel = int(np.round(image_coord[0] - (self.ori_sat_res//2) * col_split))

        row_split = int((image_coord[1]) // (self.ori_sat_res//2))
        if np.round(image_coord[1] - (self.ori_sat_res//2) * row_split) < (self.ori_sat_res//4):
            row_split -= 1
        row_pixel = int(np.round(image_coord[1] - (self.ori_sat_res//2) * row_split))

        img = self.sat_map[int(row_split * (self.ori_sat_res//2) - (self.ori_sat_res//4)):int(row_split * (self.ori_sat_res//2) + self.ori_sat_res + (self.ori_sat_res//4)),
              col_split * (self.ori_sat_res//2) - (self.ori_sat_res//4):col_split * (self.ori_sat_res//2) + self.ori_sat_res + (self.ori_sat_res//4),
              :]  # read extra 200 pixels at each side to avoid blank after rotation

        # =================== set rotation random ===================================
        theta = np.random.uniform(-1, 1)
        rdm = theta * self.rotation_range  # degree
        # ======================================================================

        rotate_angle = self.val_yaw[img_idx] / np.pi * 180 - 90+rdm  # degree
        H, W = img.shape[:2]
        assert H == W == self.ori_sat_res//2*3
        rot_matrix = cv2.getRotationMatrix2D((int(H//2), int(W//2)), rotate_angle, 1)  # rotate satellite image
        img = cv2.warpAffine(img, rot_matrix, (H, W))
        # CANNOT gaurantee the order of H & W in above two lines are correct, but its fine here, because H==W
        img = img[int(self.ori_sat_res//4):-int(self.ori_sat_res//4), int(self.ori_sat_res//4):-int(self.ori_sat_res//4), :]
        img = cv2.resize(img, (512, 512), interpolation=cv2.INTER_AREA)
        # img[:, :, 0] -= 103.939  # Blue
        # img[:, :, 1] -= 116.779  # Green
        # img[:, :, 2] -= 123.6  # Red
        sat_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        sat_img = self.satmap_transform(sat_img)

        row_offset_resized = int(-(row_pixel / self.ori_sat_res * 512 - 256))
        col_offset_resized = int(-(col_pixel / self.ori_sat_res * 512 - 256))

        ################################################
        x, y = np.meshgrid(np.linspace(-256 + col_offset_resized, 256 + col_offset_resized, 512),
                           np.linspace(-256 + row_offset_resized, 256 + row_offset_resized, 512))
        d = np.sqrt(x * x + y * y)
        sigma, mu = 4, 0.0
        img0 = np.exp(-((d - mu) ** 2 / (2.0 * sigma ** 2)))
        rot_matrix = cv2.getRotationMatrix2D((256, 256), rotate_angle, 1)
        img0 = cv2.warpAffine(img0, rot_matrix, (512, 512))
        # print("********** pos: ",img0.shape,np.where(img0==img0.max()))
        a = np.where(img0 == img0.max())
        y = a[0]
        x = a[1]
        ####################################################
        gt_shift_x = (x[0] - 256)  # right positive parallel heading
        gt_shift_y = (y[0] - 256)  # down positive vertical heading
        # print("************* yx: ",gt_shift_x,gt_shift_y)
        # cv2.imwrite("./GT.png", img0)

        return sat_img, left_camera_k, grd_img, \
               torch.tensor(gt_shift_x, dtype=torch.float32).reshape(1), \
               torch.tensor(gt_shift_y, dtype=torch.float32).reshape(1), \
               torch.tensor(theta, dtype=torch.float32).reshape(1) \




"""
load dataset, shuffle=False, for load_test_data, testNum=0, test all test datasets
"""



def load_val_data(batch_size, rotation_range=0, ori_sat_res=800):

    print("loding validation dataset..............")
    satmap_transform = transforms.Compose([
        transforms.ToTensor()
    ])

    Grd_h = GrdImg_H
    Grd_w = GrdImg_W

    grdimage_transform = transforms.Compose([
        transforms.Resize(size=[Grd_h, Grd_w]),
        transforms.ToTensor(),
    ])

    val_set = SatGrdDatasetVal(root=root_dir,
                               transform=(satmap_transform, grdimage_transform),
                               rotation_range=rotation_range,
                               ori_sat_res=ori_sat_res)
    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, pin_memory=True,
                            num_workers=num_thread_workers, drop_last=False)
    return val_loader


def load_test_data(batch_size, rotation_range=0, testNum=0, ori_sat_res=800):
    print("loading test dataset..............")
    SatMap_process_sidelength = utils.get_process_satmap_sidelength()

    satmap_transform = transforms.Compose([
        transforms.Resize(size=[SatMap_process_sidelength, SatMap_process_sidelength]),
        transforms.ToTensor(),
    ])

    Grd_h = GrdImg_H
    Grd_w = GrdImg_W

    grdimage_transform = transforms.Compose([
        transforms.Resize(size=[Grd_h, Grd_w]),
        transforms.ToTensor(),
    ])

    test_set = SatGrdDatasetTest(root=root_dir,
                                 transform=(satmap_transform, grdimage_transform),
                                 rotation_range=rotation_range, test=testNum,
                                 ori_sat_res=ori_sat_res)

    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, pin_memory=True,
                            num_workers=num_thread_workers, drop_last=False)
    return test_loader


def load_train_data(batch_size, rotation_range=0, ori_sat_res=800):
    print("loding train dataset..............")
    satmap_transform = transforms.Compose([
        transforms.ToTensor()
    ])

    Grd_h = GrdImg_H
    Grd_w = GrdImg_W

    grdimage_transform = transforms.Compose([
        transforms.Resize(size=[Grd_h, Grd_w]),
        transforms.ToTensor(),
    ])

    train_set = SatGrdDataset(root=root_dir, file=train_file,
                              transform=(satmap_transform, grdimage_transform),
                              rotation_range=rotation_range,
                              ori_sat_res=ori_sat_res)

    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True,
                              num_workers=num_thread_workers, drop_last=False)
    return train_loader

================
File: dataLoader/Vigor_dataset.py
================
import random

import numpy as np
import os
from PIL import Image
from torch.utils.data import Dataset

import torch
# import pandas as pd
# import utils
# import torchvision.transforms.functional as TF
# from torchvision import transforms
# import torch.nn.functional as F
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
from torchvision import transforms
import cv2
import math

import PIL
from PIL import Image
from torch.utils.data import Dataset, Subset

num_thread_workers = 2
# root = '/backup/dataset/VIGOR'
root = '/data/dataset/VIGOR'


class VIGORDataset(Dataset):
    def __init__(self, root, rotation_range, label_root='splits_new', split='same', train=True, transform=None, pos_only=True):
        self.root = root
        self.rotation_range = rotation_range
        self.label_root = label_root
        self.split = split
        self.train = train
        self.pos_only = pos_only

        if transform != None:
            self.grdimage_transform = transform[0]
            self.satimage_transform = transform[1]

        if self.split == 'same':
            self.city_list = ['NewYork', 'Seattle', 'SanFrancisco', 'Chicago']
        elif self.split == 'cross':
            if self.train:
                self.city_list = ['NewYork', 'Seattle']
            else:
                self.city_list = ['SanFrancisco', 'Chicago']

        self.meter_per_pixel_dict = {'NewYork': 0.113248 * 640 / 512,
                                     'Seattle': 0.100817 * 640 / 512,
                                     'SanFrancisco': 0.118141 * 640 / 512,
                                     'Chicago': 0.111262 * 640 / 512}

        # load sat list
        self.sat_list = []
        self.sat_index_dict = {}

        idx = 0
        for city in self.city_list:
            sat_list_fname = os.path.join(self.root, label_root, city, 'satellite_list.txt')
            with open(sat_list_fname, 'r') as file:
                for line in file.readlines():
                    self.sat_list.append(os.path.join(self.root, city, 'satellite', line.replace('\n', '')))
                    self.sat_index_dict[line.replace('\n', '')] = idx
                    idx += 1
            print('InputData::__init__: load', sat_list_fname, idx)
        self.sat_list = np.array(self.sat_list)
        self.sat_data_size = len(self.sat_list)
        print('Sat loaded, data size:{}'.format(self.sat_data_size))

        # load grd list
        self.grd_list = []
        self.label = []
        self.sat_cover_dict = {}
        self.delta = []
        idx = 0
        for city in self.city_list:
            # load grd panorama list
            if self.split == 'same':
                if self.train:
                    label_fname = os.path.join(self.root, self.label_root, city, 'same_area_balanced_train__corrected.txt')
                else:
                    label_fname = os.path.join(self.root, label_root, city, 'same_area_balanced_test__corrected.txt')
            elif self.split == 'cross':
                label_fname = os.path.join(self.root, self.label_root, city, 'pano_label_balanced__corrected.txt')

            with open(label_fname, 'r') as file:
                for line in file.readlines():
                    data = np.array(line.split(' '))
                    label = []
                    for i in [1, 4, 7, 10]:
                        label.append(self.sat_index_dict[data[i]])
                    label = np.array(label).astype(int)
                    delta = np.array([data[2:4], data[5:7], data[8:10], data[11:13]]).astype(float)
                    self.grd_list.append(os.path.join(self.root, city, 'panorama', data[0]))
                    self.label.append(label)
                    self.delta.append(delta)
                    if not label[0] in self.sat_cover_dict:
                        self.sat_cover_dict[label[0]] = [idx]
                    else:
                        self.sat_cover_dict[label[0]].append(idx)
                    idx += 1
            print('InputData::__init__: load ', label_fname, idx)

        from sklearn.utils import shuffle
        for rand_state in range(20):
            self.grd_list, self.label, self.delta = shuffle(self.grd_list, self.label, self.delta, random_state=rand_state)

        self.data_size = int(len(self.grd_list))
        self.grd_list = self.grd_list[: self.data_size]
        self.label = self.label[: self.data_size]
        self.delta = self.delta[: self.data_size]
        print('Grd loaded, data size:{}'.format(self.data_size))
        self.label = np.array(self.label)
        self.delta = np.array(self.delta)

    def __len__(self):
        return self.data_size

    def get_grd_sat_img_pair(self, idx):

        # full ground panorama
        try:
            grd = PIL.Image.open(os.path.join(self.grd_list[idx]))
            grd = grd.convert('RGB')
        except:
            print('unreadable image')
            grd = PIL.Image.new('RGB', (320, 640))  # if the image is unreadable, use a blank image
        grd = self.grdimage_transform(grd)

        # generate a random rotation
        rotation = np.random.uniform(low=-1.0, high=1.0)  #
        rotation_angle = rotation * self.rotation_range
        grd = torch.roll(grd, (torch.round(torch.as_tensor(rotation_angle / 180) * grd.size()[2] / 2).int()).item(),
                         dims=2)

        # satellite
        if self.pos_only:  # load positives only
            pos_index = 0
            sat = PIL.Image.open(os.path.join(self.sat_list[self.label[idx][pos_index]]))
            [row_offset, col_offset] = self.delta[idx, pos_index]  # delta = [delta_lat, delta_lon]
        else:  # load positives and semi-positives
            col_offset = 320
            row_offset = 320
            while (np.abs(col_offset) >= 320 or np.abs(
                    row_offset) >= 320):  # do not use the semi-positives where GT location is outside the image
                pos_index = random.randint(0, 3)
                sat = PIL.Image.open(os.path.join(self.sat_list[self.label[idx][pos_index]]))
                [row_offset, col_offset] = self.delta[idx, pos_index]  # delta = [delta_lat, delta_lon]

        sat = sat.convert('RGB')
        width_raw, height_raw = sat.size

        sat = self.satimage_transform(sat)
        _, height, width = sat.size()
        row_offset = np.round(row_offset / height_raw * height)
        col_offset = np.round(col_offset / width_raw * width)

        # groundtruth location on the aerial image
        gt_shift_y = row_offset / height * 4  # -L/4 ~ L/4  -1 ~ 1
        gt_shift_x = -col_offset / width * 4  #

        if 'NewYork' in self.grd_list[idx]:
            city = 'NewYork'
        elif 'Seattle' in self.grd_list[idx]:
            city = 'Seattle'
        elif 'SanFrancisco' in self.grd_list[idx]:
            city = 'SanFrancisco'
        elif 'Chicago' in self.grd_list[idx]:
            city = 'Chicago'

        return grd, sat, \
            torch.tensor(gt_shift_x, dtype=torch.float32), \
            torch.tensor(gt_shift_y, dtype=torch.float32), \
            torch.tensor(rotation, dtype=torch.float32), \
            torch.tensor(self.meter_per_pixel_dict[city], dtype=torch.float32)

    def __getitem__(self, idx):

        return self.get_grd_sat_img_pair(idx)


def load_vigor_data(batch_size, area="same", rotation_range=10, train=True):
    """

    Args:
        batch_size: B
        area: same | cross
    """

    transform_grd = transforms.Compose([
        transforms.Resize([320, 640]),
        transforms.ToTensor(),
        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

    ])

    transform_sat = transforms.Compose([
        # resize
        transforms.Resize([512, 512]),
        transforms.ToTensor(),
        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

    ])

    # torch.manual_seed(202)
    # np.random.seed(202)
    vigor = VIGORDataset(root, rotation_range, split=area, train=train, transform=(transform_grd, transform_sat))

    if train is True:
        index_list = np.arange(vigor.__len__())
        # np.random.shuffle(index_list)
        train_indices = index_list[0: int(len(index_list) * 0.8)]
        val_indices = index_list[int(len(index_list) * 0.8):]
        training_set = Subset(vigor, train_indices)
        val_set = Subset(vigor, val_indices)

        train_dataloader = DataLoader(training_set, batch_size=batch_size, shuffle=True)

        val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=False)

        return train_dataloader, val_dataloader

    else:
        test_dataloader = DataLoader(vigor, batch_size=batch_size, shuffle=False)

        return test_dataloader



#
# class SatGrdDataset(Dataset):
#     def __init__(self, area, train_test,transform=None,mode="val"):
#         self.root= '../../dataset/VIGOR'
#         self.mode=mode
#         if transform != None:
#             self.satmap_transform = transform[0]
#             self.grdimage_transform = transform[1]
#         self.area = area
#         self.train_test = train_test
#         self.sat_size = [512, 512]  # [320, 320] or [512, 512]
#         self.grd_size = [320, 640]  # [320, 640]  # [224, 1232]
#         label_root = 'splits'
#
#         if self.area == 'same':
#             self.train_city_list = ['NewYork', 'Seattle', 'SanFrancisco', 'Chicago']
#             self.test_city_list = ['NewYork', 'Seattle', 'SanFrancisco', 'Chicago']
#         elif self.area == 'cross':
#             self.train_city_list = ['NewYork', 'Seattle']
#             if self.train_test == 'train':
#                 self.test_city_list = ['NewYork', 'Seattle']
#             elif self.train_test == 'test':
#                 self.test_city_list = ['SanFrancisco', 'Chicago']
#
#                 # load sat list, the training and test set both contain all satellite images
#         self.train_sat_list = []
#         self.train_sat_index_dict = {}
#         idx = 0
#         for city in self.train_city_list:
#             train_sat_list_fname = os.path.join(self.root, label_root, city, 'satellite_list.txt')
#             with open(train_sat_list_fname, 'r') as file:
#                 for line in file.readlines():
#                     self.train_sat_list.append(os.path.join(self.root, city, 'satellite', line.replace('\n', '')))
#                     self.train_sat_index_dict[line.replace('\n', '')] = idx
#                     idx += 1
#             print('InputData::__init__: load', train_sat_list_fname, idx)
#         self.train_sat_list = np.array(self.train_sat_list)
#         self.train_sat_data_size = len(self.train_sat_list)
#         print('Train sat loaded, data size:{}'.format(self.train_sat_data_size))
#
#         self.test_sat_list = []
#         self.test_sat_index_dict = {}
#         self.__cur_sat_id = 0  # for test
#         idx = 0
#         for city in self.test_city_list:
#             test_sat_list_fname = os.path.join(self.root, label_root, city, 'satellite_list.txt')
#             with open(test_sat_list_fname, 'r') as file:
#                 for line in file.readlines():
#                     self.test_sat_list.append(os.path.join(self.root, city, 'satellite', line.replace('\n', '')))
#                     self.test_sat_index_dict[line.replace('\n', '')] = idx
#                     idx += 1
#             print('InputData::__init__: load', test_sat_list_fname, idx)
#         self.test_sat_list = np.array(self.test_sat_list)
#         self.test_sat_data_size = len(self.test_sat_list)
#         print('Test sat loaded, data size:{}'.format(self.test_sat_data_size))
#
#         # load grd training list and test list.
#         self.train_list = []
#         self.train_label = []
#         self.train_sat_cover_dict = {}
#         self.train_delta = []
#         idx = 0
#         for city in self.train_city_list:
#             # load train panorama list
#             if self.area == 'same':
#                 train_label_fname = os.path.join(self.root, label_root, city, 'same_area_balanced_train.txt')
#             if self.area == 'cross':
#                 train_label_fname = os.path.join(self.root, label_root, city, 'pano_label_balanced.txt')
#             with open(train_label_fname, 'r') as file:
#                 for line in file.readlines():
#                     data = np.array(line.split(' '))
#                     label = []
#                     for i in [1, 4, 7, 10]:
#                         label.append(self.train_sat_index_dict[data[i]])
#                     label = np.array(label).astype(np.int)
#                     delta = np.array([data[2:4], data[5:7], data[8:10], data[11:13]]).astype(float)
#                     self.train_list.append(os.path.join(self.root, city, 'panorama', data[0]))
#                     self.train_label.append(label)
#                     self.train_delta.append(delta)
#                     if not label[0] in self.train_sat_cover_dict:
#                         self.train_sat_cover_dict[label[0]] = [idx]
#                     else:
#                         self.train_sat_cover_dict[label[0]].append(idx)
#                     idx += 1
#             print('InputData::__init__: load ', train_label_fname, idx)
#
#         # split the original training set into training and validation sets
#         self.train_list, self.val_list, self.train_label, self.val_label, self.train_delta, self.val_delta = train_test_split(
#                 self.train_list, self.train_label, self.train_delta, test_size=0.2, random_state=42)
#
#         self.train_label = np.array(self.train_label)
#         self.train_delta = np.array(self.train_delta)
#         self.val_label = np.array(self.val_label)
#         self.val_delta = np.array(self.val_delta)
#         self.train_data_size = len(self.train_list)
#         self.val_data_size = len(self.val_list)
#         self.trainIdList = [*range(0, self.train_data_size, 1)]
#         self.valIdList = [*range(0, self.val_data_size, 1)]
#
#
#     def __len__(self):
#         if self.mode=="train":
#             return len(self.train_list)
#         else:
#             return len(self.val_list)
#
#     def get_file_list(self):
#         if self.mode=="train":
#             return self.train_list
#         else:
#             return self.val_list
#
#     def __getitem__(self, idx):
#         if self.mode=="train":
#             img_idx=idx
#             # print("processing train set")
#             # img = cv2.imread(self.train_list[img_idx])
#             # img = img.astype(np.float32)
#             # grd_img = cv2.resize(img, (self.grd_size[1], self.grd_size[0]), interpolation=cv2.INTER_AREA)
#             # grd_img = (Image.fromarray(cv2.cvtColor((grd_img).astype(np.uint8), cv2.COLOR_BGR2RGB)))
#
#             grd_img = Image.open(self.train_list[img_idx]).resize((self.grd_size[1], self.grd_size[0]))
#             grd_img = self.grdimage_transform(grd_img)
#
#             pos_index = random.randint(0, 3)
#             img = cv2.imread(self.train_sat_list[self.train_label[img_idx][pos_index]])
#             sat_img1 = cv2.resize(img, (self.sat_size[1], self.sat_size[0]), interpolation=cv2.INTER_AREA)
#             sat_img = (Image.fromarray(cv2.cvtColor((sat_img1).astype(np.uint8), cv2.COLOR_BGR2RGB)))
#
#             # sat_img = Image.open(self.train_sat_list[self.train_label[img_idx][pos_index]]).crop((20, 20, 620, 620)).resize((self.sat_size[1], self.sat_size[0]))
#             sat_img = self.satmap_transform(sat_img)
#
#             [col_offset, row_offset] = self.train_delta[img_idx, pos_index]  # delta = [delta_lat, delta_lon]
#             row_offset_resized = (row_offset / 640 * self.sat_size[0]).astype(np.int32)
#             col_offset_resized = (col_offset / 640 * self.sat_size[0]).astype(np.int32)
#             x, y = np.meshgrid(
#                 np.linspace(-self.sat_size[0] / 2 + row_offset_resized, self.sat_size[0] / 2 + row_offset_resized,
#                             self.sat_size[0]),
#                 np.linspace(-self.sat_size[0] / 2 - col_offset_resized, self.sat_size[0] / 2 - col_offset_resized,
#                             self.sat_size[0]))
#             d = np.sqrt(x * x + y * y)
#             sigma, mu = 4, 0.0
#             img0 = 1000*np.exp(-( (d-mu)**2 / ( 2.0 * sigma**2 ) ) )
#             # cv2.imwrite("./GT_vigor.png ",img0)
#             # print("********** pos: ",img0.shape,np.where(img0==img0.max()))
#             a=np.where(img0==img0.max())
#             y=a[1]
#             x=a[0]
#         ####################################################
#
#             gt_shift_y = (x[0] - 256)
#             gt_shift_x = (y[0] - 256)
#
#             # print("************* yx: ",gt_shift_x,gt_shift_y)
#             # cv2.imwrite("./GT.png",img0)
#             #
#             # ###debug
#             # img1=cv2.resize(img, (512, 512), interpolation=cv2.INTER_AREA)
#             # cv2.circle(sat_img1, ( 256, 256), 1, (0, 255, 255), 8)
#             # cv2.circle(sat_img1, (y[0], x[0]), 1, (255, 255, 255), 8)
#             # cv2.imwrite("./offset_vigor.png",sat_img1)
#             return sat_img, grd_img, \
#                torch.tensor(gt_shift_x, dtype=torch.float32).reshape(1), \
#                torch.tensor(gt_shift_y, dtype=torch.float32).reshape(1)
#         else:
#             img_idx = idx
#             # img = cv2.imread(self.val_list[img_idx])
#             # img = img.astype(np.float32)
#             # grd_img = cv2.resize(img, (self.grd_size[1], self.grd_size[0]), interpolation=cv2.INTER_AREA)
#             # grd_img = (Image.fromarray(cv2.cvtColor((grd_img).astype(np.uint8), cv2.COLOR_BGR2RGB)))
#
#             grd_img = Image.open(self.val_list[img_idx]).resize((self.grd_size[1], self.grd_size[0]))
#             grd_img = self.grdimage_transform(grd_img)
#
#             # satellite
#             pos_index = 0  # we use the positive (no semi-positive) satellite images during testing
#             # img = cv2.imread(self.test_sat_list[self.val_label[img_idx][pos_index]])
#             # img = img.astype(np.float32)
#             # sat_img1 = cv2.resize(img, (self.sat_size[1], self.sat_size[0]), interpolation=cv2.INTER_AREA)
#             # sat_img = (Image.fromarray(cv2.cvtColor((sat_img1).astype(np.uint8), cv2.COLOR_BGR2RGB)))
#
#             sat_img = Image.open(self.test_sat_list[self.val_label[img_idx][pos_index]]).resize((self.sat_size[1], self.sat_size[0]))
#             sat_img= self.satmap_transform(sat_img)
#
#
#             # get groundtruth location on the satellite map
#             [col_offset, row_offset] = self.val_delta[img_idx, pos_index]  # delta = [delta_lat, delta_lon]
#             row_offset_resized = (row_offset / 640 * self.sat_size[0]).astype(np.int32)
#             col_offset_resized = (col_offset / 640 * self.sat_size[0]).astype(np.int32)
#             # Gaussian GT
#             x, y = np.meshgrid(
#                 np.linspace(-self.sat_size[0] / 2 + row_offset_resized, self.sat_size[0] / 2 + row_offset_resized,
#                             self.sat_size[0]),
#                 np.linspace(-self.sat_size[0] / 2 - col_offset_resized, self.sat_size[0] / 2 - col_offset_resized,
#                             self.sat_size[0]))
#             d = np.sqrt(x * x + y * y)
#             sigma, mu = 4, 0.0
#             img0 = 1000*np.exp(-( (d-mu)**2 / ( 2.0 * sigma**2 ) ) )
#             # cv2.imwrite("./GT_vigor.png ", img0)
#             # print("********** pos: ",img0.shape,np.where(img0==img0.max()))
#             a = np.where(img0 == img0.max())
#             y = a[1]
#             x = a[0]
#             ####################################################
#
#             gt_shift_x = (x[0] - 256)
#             gt_shift_y = (y[0] - 256)
#
#             # print("************* yx: ",gt_shift_x,gt_shift_y)
#             # cv2.imwrite("./GT.png",img0)
#             #
#             # ###debug
#             # img1=cv2.resize(img, (512, 512), interpolation=cv2.INTER_AREA)
#             # cv2.circle(sat_img1, (256, 256), 1, (0, 255, 255), 8)
#             # cv2.circle(sat_img1, (y[0], x[0]), 1, (255, 255, 0), 8)
#             # cv2.imwrite("./offset_vigor.png", sat_img1)
#             # print("debug")
#             # print("debug")
#
#
#             return sat_img, grd_img, \
#                torch.tensor(gt_shift_x, dtype=torch.float32).reshape(1), \
#                torch.tensor(gt_shift_y, dtype=torch.float32).reshape(1),\
#
# class SatGrdDatasetTest(Dataset):
#     def __init__(self, area, train_test,transform=None):
#         self.root = '../../dataset/VIGOR'
#         if transform != None:
#             self.satmap_transform = transform[0]
#             self.grdimage_transform = transform[1]
#         self.train_test=train_test
#         self.area=area
#         self.sat_size = [512, 512]  # [320, 320] or [512, 512]
#         self.grd_size = [320, 640]  # [320, 640]  # [224, 1232]
#         label_root = 'splits'
#
#         if self.area == 'same':
#             self.train_city_list = ['NewYork', 'Seattle', 'SanFrancisco', 'Chicago']
#             self.test_city_list = ['NewYork', 'Seattle', 'SanFrancisco', 'Chicago']
#         elif self.area == 'cross':
#             self.train_city_list = ['NewYork', 'Seattle']
#             if self.train_test == 'train':
#                 self.test_city_list = ['NewYork', 'Seattle']
#             elif self.train_test == 'test':
#                 self.test_city_list = ['SanFrancisco', 'Chicago']
#
#                 # load sat list, the training and test set both contain all satellite images
#         idx = 0
#
#         self.test_sat_list = []
#         self.test_sat_index_dict = {}
#         for city in self.test_city_list:
#             test_sat_list_fname = os.path.join(self.root, label_root, city, 'satellite_list.txt')
#             with open(test_sat_list_fname, 'r') as file:
#                 for line in file.readlines():
#                     self.test_sat_list.append(os.path.join(self.root, city, 'satellite', line.replace('\n', '')))
#                     self.test_sat_index_dict[line.replace('\n', '')] = idx
#                     idx += 1
#             print('InputData::__init__: load', test_sat_list_fname, idx)
#         self.test_sat_list = np.array(self.test_sat_list)
#         self.test_sat_data_size = len(self.test_sat_list)
#         print('Test sat loaded, data size:{}'.format(self.test_sat_data_size))
#
#         if self.train_test == 'test':
#             self.val_list = []
#             self.val_label = []
#             self.test_sat_cover_dict = {}
#             self.val_delta = []
#             idx = 0
#             for city in self.test_city_list:
#                 # load test panorama list
#                 if self.area == 'same':
#                     test_label_fname = os.path.join(self.root, label_root, city, 'same_area_balanced_test.txt')
#                 if self.area == 'cross':
#                     test_label_fname = os.path.join(self.root, label_root, city, 'pano_label_balanced.txt')
#                 with open(test_label_fname, 'r') as file:
#                     for line in file.readlines():
#                         data = np.array(line.split(' '))
#                         label = []
#                         for i in [1, 4, 7, 10]:
#                             label.append(self.test_sat_index_dict[data[i]])
#                         label = np.array(label).astype(np.int)
#                         delta = np.array([data[2:4], data[5:7], data[8:10], data[11:13]]).astype(float)
#                         self.val_list.append(os.path.join(self.root, city, 'panorama', data[0]))
#                         self.val_label.append(label)
#                         self.val_delta.append(delta)
#                         if not label[0] in self.test_sat_cover_dict:
#                             self.test_sat_cover_dict[label[0]] = [idx]
#                         else:
#                             self.test_sat_cover_dict[label[0]].append(idx)
#                         idx += 1
#                 print('InputData::__init__: load ', test_label_fname, idx)
#
#         self.val_label = np.array(self.val_label)
#         self.val_delta = np.array(self.val_delta)
#         self.val_data_size = len(self.val_list)
#         self.valIdList = [*range(0, self.val_data_size, 1)]
#
#     def __len__(self):
#             return len(self.val_list)
#
#     def get_file_list(self):
#             return self.val_list
#
#     def __getitem__(self, idx):
#         img_idx = idx
#         # print("processing validation set")
#         # img = cv2.imread(self.val_list[img_idx])
#         # img = img.astype(np.float32)
#         # grd_img = cv2.resize(img, (self.grd_size[1], self.grd_size[0]), interpolation=cv2.INTER_AREA)
#         # grd_img = (Image.fromarray(cv2.cvtColor((grd_img * 255).astype(np.uint8), cv2.COLOR_BGR2RGB)))
#         grd_img = Image.open(self.val_list[img_idx]).resize((self.grd_size[1], self.grd_size[0]))
#         grd_img = self.grdimage_transform(grd_img)
#
#         # satellite
#         pos_index = 0  # we use the positive (no semi-positive) satellite images during testing
#         img = cv2.imread(self.test_sat_list[self.val_label[img_idx][pos_index]])
#         img = img.astype(np.float32)
#         sat_img1 = cv2.resize(img, (self.sat_size[1], self.sat_size[0]), interpolation=cv2.INTER_AREA)
#         sat_img = (Image.fromarray(cv2.cvtColor((sat_img1 * 255).astype(np.uint8), cv2.COLOR_BGR2RGB)))
#         # sat_img = Image.open(self.test_sat_list[self.val_label[img_idx][pos_index]]).resize((self.sat_size[1], self.sat_size[0]))
#
#         sat_img = self.satmap_transform(sat_img)
#
#         # get groundtruth location on the satellite map
#         [col_offset, row_offset] = self.val_delta[img_idx, pos_index]  # delta = [delta_lat, delta_lon]
#         row_offset_resized = (row_offset / 640 * self.sat_size[0]).astype(np.int32)
#         col_offset_resized = (col_offset / 640 * self.sat_size[0]).astype(np.int32)
#         # Gaussian GT
#         x, y = np.meshgrid(
#             np.linspace(-self.sat_size[0] / 2 + row_offset_resized, self.sat_size[0] / 2 + row_offset_resized,
#                         self.sat_size[0]),
#             np.linspace(-self.sat_size[0] / 2 - col_offset_resized, self.sat_size[0] / 2 - col_offset_resized,
#                         self.sat_size[0]))
#         d = np.sqrt(x * x + y * y)
#         sigma, mu = 4, 0.0
#         img0 = 1000 * np.exp(-((d - mu) ** 2 / (2.0 * sigma ** 2)))
#
#         a = np.where(img0 == img0.max())
#         y = a[1]
#         x = a[0]
#         ####################################################
#
#         gt_shift_y = (x[0] - 256)
#         gt_shift_x = (y[0] - 256)
#
#
#
#         return sat_img, grd_img, \
#                torch.tensor(gt_shift_x, dtype=torch.float32).reshape(1), \
#                torch.tensor(gt_shift_y, dtype=torch.float32).reshape(1)
#
#
# def load_test_data(batch_size, area="same"):
#     """
#
#     Args:
#         batch_size: B
#         area: same | cross
#
#     Returns: sat_img, grd_img, gt_x ,gt_y  (pixel)
#
#     """
#
#     satmap_transform = transforms.Compose([
#         transforms.ToTensor()
#     ])
#
#     grdimage_transform = transforms.Compose([
#         transforms.ToTensor()
#     ])
#
#
#     test_set = SatGrdDatasetTest(area,"test",transform=(satmap_transform, grdimage_transform))
#
#     test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, pin_memory=True,
#                             num_workers=num_thread_workers, drop_last=False)
#     return test_loader
#
#
# def load_train_data(batch_size, area="cross",mode="train"):
#     """
#     Args:
#         batch_size:
#         area: cross | same
#         mode: train | val
#
#     Returns: sat_img, grd_img, gt_x ,gt_y  (pixel)
#
#     """
#     satmap_transform = transforms.Compose([
#         transforms.ToTensor()
#     ])
#
#     grdimage_transform = transforms.Compose([
#         transforms.ToTensor()
#     ])
#
#     train_set = SatGrdDataset(area,"train",transform=(satmap_transform, grdimage_transform),mode=mode)
#
#     train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True,
#                               num_workers=num_thread_workers, drop_last=False)
#     return train_loader

# device = torch.device("cuda:0")
# mini_batch=1
# trainloader =load_train_data(mini_batch,area="cross",mode="train")
# # trainloader =load_test_data(mini_batch,area="cross")
# for Loop, Data in enumerate(trainloader, 0):
#     # get the inputs
#     sat_map, grd_left_imgs, gt_shift_u, gt_shift_v= [item.to(device) for item in Data]
#     break
#
# print("sat_map shape: ",sat_map.shape)
# print("grd image shape: ",grd_left_imgs.shape)
# sat_map shape:  torch.Size([2, 3, 512, 512])
# grd image shape:  torch.Size([2, 3, 154, 231])
